{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate line execution\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "# # plotly\n",
    "# import plotly.express as px  # (version 4.7.0 or higher)\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import custom libraries\n",
    "import sys\n",
    "# sys.path.append(\"C:\\\\DATA\\\\Tasks\\\\lib\\\\hk\")\n",
    "# import hk_utils\n",
    "\n",
    "# folder paths\n",
    "ADD_DATA = \"C:\\\\DATA\\\\data\\\\raw\\\\mimic4\\\\lookup\\\\\"\n",
    "ADD_DATA_proc = \"C:/DATA/data/processed/\"\n",
    "\n",
    "\n",
    "PATH_PAPER = \"C:\\\\DATA\\\\Tasks\\\\220704\\\\Alternate-Transactions-Articles-LaTeX-template\\\\images\\\\\"\n",
    "\n",
    "\n",
    "PATH_SYS=\"/mlodata1/hokarami/tedam/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for THP\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import transformer.Constants as Constants\n",
    "import Utils\n",
    "\n",
    "# from preprocess.Dataset import get_dataloader, get_dataloader2\n",
    "# from transformer.Models import Transformer\n",
    "# from transformer.hk_transformer import Transformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_allocated()\n",
    "# torch.cuda.memory_reserved()\n",
    "\n",
    "# from sklearn import metrics\n",
    "# from hk_pytorch import save_checkpoint,load_checkpoint\n",
    "# import hk_pytorch\n",
    "\n",
    "\n",
    "# from custom2 import myparser\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb -qqq\n",
    "import wandb\n",
    "# wandb.login()\n",
    "api = wandb.Api()\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"0f780ac8a470afe6cb7fc474ff3794772c660465\"\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"jup_res\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "sudo conda install -c conda-forge transformers datasets tokenizers --name paper2022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bun', 'creatinine', 'glucose', 'hco3', 'na', 'k', 'mg', 'hct', 'platelets', 'wbc', 'fio2', 'paco2', 'pao2', 'ph', 'sao2', 'alp', 'alt', 'ast', 'albumin', 'bilirubin', 'lactate', 'cholesterol', 'troponini', 'troponint', 'nothing', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('vocab_list.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    vocab_list = content.split()\n",
    "    print(vocab_list)\n",
    "\n",
    "len(vocab_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57dbdeb2ed544b208d351566d0e48a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef23ca51f5e445f98748969dec7bd56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# configuration = BertConfig(vocab_size = len(tokenizer.get_vocab()), hidden_size=32, num_hidden_layers=4, num_attention_heads=4, intermediate_size=128,  )\n",
    "# # configuration = BertConfig()\n",
    "\n",
    "# model = AutoModelForMaskedLM.from_config(configuration)\n",
    "\n",
    "# configuration = model.config\n",
    "\n",
    "model_checkpoint = \"prajjwal1/bert-tiny\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30539, 128)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "30539"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer \n",
    "\n",
    "\n",
    "model_checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "len(tokenizer.get_vocab())\n",
    "\n",
    "tokenizer.add_tokens(vocab_list)\n",
    "\n",
    "# IMPORTANT: resize the model for the new vocab size\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "len(tokenizer.get_vocab())\n",
    "\n",
    "# str(tokenizer.tokenize(raw_datasets['train'][0]['text']))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/hokarami/.cache/huggingface/datasets/json/default-182a209067c9dc75/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d719f893328c4d81a53f4691e0699a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['mat', 'text'],\n",
       "        num_rows: 11136\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['mat', 'text'],\n",
       "        num_rows: 768\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['mat', 'text'],\n",
       "        num_rows: 768\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\hokarami\\.cache\\huggingface\\datasets\\json\\default-182a209067c9dc75\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-54387868a32678fc.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> text: nothing. nothing. fio2. nothing. k mg hct platelets wbc paco2 pao2 ph. nothing. fio2. nothing. wbc. wbc paco2 pao2 ph. bun creatinine glucose hco3 na k mg hct platelets wbc fio2 albumin. fio2. nothing. nothing. fio2. fio2 paco2 pao2 ph. nothing. nothing. fio2. paco2 pao2 ph. bun creatinine glucose hco3 na k mg hct platelets wbc. nothing. fio2. nothing. nothing. nothing. fio2. nothing. paco2 pao2 ph sao2. fio2. fio2 paco2 pao2 ph sao2. nothing. nothing. nothing. nothing. fio2. nothing. nothing. nothing. nothing. fio2 paco2 pao2 ph. nothing. paco2 pao2 ph. bun creatinine glucose hco3 na k mg hct platelets wbc fio2 paco2 pao2 ph albumin. nothing. nothing. fio2.'\n",
      "\n",
      "'>>> text: platelets fio2 paco2 pao2 ph. fio2 paco2 pao2 ph sao2. platelets paco2 pao2 ph sao2. bun creatinine na k hct platelets wbc alp albumin bilirubin lactate. fio2 paco2 pao2 ph. nothing. paco2 pao2 ph lactate. mg platelets fio2 paco2 pao2 ph sao2 alp alt ast bilirubin. nothing. nothing. fio2 paco2 pao2 ph sao2 lactate. nothing. paco2 pao2 ph sao2 lactate. fio2. bun creatinine glucose hco3 na k mg hct platelets wbc alp alt ast. fio2 paco2 pao2 ph sao2. paco2 pao2 ph lactate. nothing. nothing. fio2 paco2 pao2 ph sao2. nothing. nothing. paco2 pao2 ph sao2. nothing. nothing. hct platelets. paco2 pao2 ph sao2. nothing. nothing. nothing. paco2 pao2 ph sao2. fio2. nothing. nothing. nothing. nothing. fio2. nothing. bun creatinine glucose hco3 na k mg hct platelets wbc paco2 pao2 ph sao2. nothing. fio2. nothing. fio2 paco2 pao2 ph sao2. paco2 pao2 ph sao2. paco2 pao2 ph sao2. nothing. fio2.'\n",
      "\n",
      "'>>> text: nothing. bun creatinine glucose hco3 na k mg hct platelets wbc alp alt ast bilirubin cholesterol. nothing. nothing. nothing. nothing. nothing. hct. nothing. nothing. nothing. nothing. nothing. nothing. bun creatinine glucose hco3 na k mg hct platelets wbc. nothing. nothing. nothing. nothing. nothing. hct. nothing. nothing. nothing. nothing. nothing. nothing. nothing. nothing. nothing. hct. nothing. nothing. nothing. bun creatinine glucose hco3 na k mg hct platelets wbc albumin. nothing. nothing. nothing. nothing. hct platelets wbc. nothing. nothing. nothing. nothing. nothing. hct platelets wbc.'\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "data_files={\n",
    "            \"train\": \"ehr_bert_data/trainloader.json\",\n",
    "            \"validation\": \"ehr_bert_data/validloader.json\",\n",
    "            \"test\": \"ehr_bert_data/testloader.json\",\n",
    "        }\n",
    "\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "raw_datasets\n",
    "\n",
    "sample = raw_datasets[\"train\"].shuffle(seed=42).select(range(3))\n",
    "# sample = raw_datasets.shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> text: {row['text']}'\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0101d1c01c0b447b8874fd8f4fdfee71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11136 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d34033c0af44ebfb66e088fb9579f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd51dfdfd9c34dfdbc43fc875ac84116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function, batched=True, remove_columns=['text', \"mat\"]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Example 0 length: 284'\n",
      "'>>> Example 1 length: 204'\n",
      "'>>> Example 2 length: 127'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Slicing produces a list of lists for each feature\n",
    "# tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "tokenized_samples = tokenized_datasets['train'][:3]\n",
    "\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Example {idx} length: {len(sample)}'\")\n",
    "\n",
    "tokenized_samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated reviews length: 615'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 103'\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 128\n",
    "\n",
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")\n",
    "\n",
    "\n",
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b1c9c4d2554f2898d6d48eced5239b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11136 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cddf841e364ad4bb0208993192bbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8889baecfa79476cad4963127072b92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 12852\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 843\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 843\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glucose hco3 na k mg hct platelets wbc fio2 lactate troponint. nothing. nothing. paco2 pao2 ph sao2. bun creatinine mg hct platelets wbc fio2 alp alt ast bilirubin lactate. nothing. nothing. fio2. bun creatinine hco3 na mg hct platelets wbc paco2 pao2 ph sao2. lactate. paco2 pao2 ph. lactate. fio2. paco2 pao2 ph sao2. bun creatinine hco3 na mg hct platelets alp ast albumin bilirubin lactate. nothing. fio2. paco2 pao2 ph. lactate. nothing. fio2. paco2 pao2 ph sao2. bun creatinine hco3 na mg hct platelets wbc alp alt ast bilirubin lactate. nothing. fio2 paco2 pao2 ph sao2. bun creatinine hco3 na mg hct'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets['train'][1][\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] bun creatinine glucose hco3 na [MASK] [MASK] hct platelets [MASK] [MASK] pao2 ph alp [MASK] [MASK] albumin bilirubin [MASK]. paco2 pao2 ph lactate. lactate. paco2 pao2 ph sao2. [MASK] [MASK] ph [MASK] lactate peanuts fio2 paco2 [MASK] ph sao2 lactate. bun creatinine hco3 mg hct [MASK] wbc paco2 pao2 ph sao2 alp alt [MASK] albumin bilirubin lactate. [MASK] troponint. paco2 pao2 ph sao2 [MASK] [MASK] pao2 ph sao2 lactate [MASK] bun creatinine hco3 na mg [MASK] platelets wbc fio2 alp alt ast albumin bilirubin lactate. nothing. nothing. paco2 pao2 ph sao2. bun rourke mg [MASK] platelets wbc fio2 alp alt ast bilirubin lactate troponint [MASK] nothing. nothing. paco2 pao2 ph sao2. bun creatinine'\n",
      "\n",
      "'>>> [MASK] hco3 na [MASK] mg hct platelets wbc [MASK] lactate troponint. [MASK]. nothing. paco2 pao2 ph sao2. bun creatinine mg hct platelets wbc fio2 [MASK] alt ast bilirubin lactate. nothing. nothing. fio2. bun creatinine hco3 na [MASK] hct platelets wbc paco2 pao2 ph sao2 [MASK] lactate. [MASK] pao2 [MASK]. [MASK]. fio2. paco2 pao2 ph sao2. bun creatinine hco3 na mg hct platelets alp ast albumin bilirubin lactate. nothing. fio2. paco2 pao2 ph. lactate [MASK] nothing. [MASK]. [MASK] pao2 ph sao2. bun creatinine [MASK] na mg [MASK] platelets wbc alp alt ast bilirubin lactate. nothing. fio2 paco2 pao2 ph sao2. bun creatinine hco3 na [MASK] hct'\n"
     ]
    }
   ],
   "source": [
    "# samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "\n",
    "for sample in samples:\n",
    "    \n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] bun creatinine glucose hco3 [MASK] k mg hct platelets wbc paco2 pao2 ph alp alt ast albumin [MASK] lactate. paco2 [MASK] ph lactate. lactate. paco2 pao2 [MASK] [MASK]. paco2 pao2 ph [MASK] lactate. fio2 paco2 pao2 ph sao2 [MASK]. bun [MASK] hco3 mg hct [MASK] wbc paco2 pao2 ph sao2 alp [MASK] ast albumin bilirubin lactate. lactate troponint. paco2 pao2 ph sao2. paco2 pao2 [MASK] sao2 lactate. [MASK] creatinine hco3 [MASK] mg hct platelets wbc fio2 alp alt ast albumin bilirubin lactate [MASK] [MASK]. nothing. paco2 pao2 ph sao2. bun [MASK] mg hct platelets wbc fio2 alp alt ast bilirubin lactate troponint. nothing. nothing. paco2 [MASK] ph sao2. bun creatinine'\n",
      "\n",
      "'>>> [MASK] hco3 na k mg hct platelets wbc fio2 lactate troponint. nothing. nothing. paco2 pao2 ph sao2. bun [MASK] [MASK] hct platelets wbc fio2 alp alt ast bilirubin [MASK]. nothing. nothing [MASK] fio2. bun creatinine hco3 na mg hct platelets wbc paco2 pao2 ph sao2. lactate. paco2 [MASK] ph. [MASK] [MASK] fio2. paco2 pao2 [MASK] sao2 [MASK] bun creatinine hco3 na [MASK] hct platelets [MASK] ast [MASK] bilirubin lactate. nothing. fio2. paco2 [MASK] ph. [MASK]. nothing. fio2. paco2 pao2 ph sao2 [MASK] [MASK] [MASK] [MASK] na mg [MASK] [MASK] wbc alp alt ast bilirubin lactate. nothing. fio2 paco2 pao2 ph [MASK]. [MASK] creatinine hco3 na mg [MASK]'\n"
     ]
    }
   ],
   "source": [
    "# samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = 0.8\n",
    "# test_size = 0.2\n",
    "# # test_size = int(0.1 * train_size)\n",
    "\n",
    "# downsampled_dataset = lm_datasets.train_test_split(\n",
    "#     train_size=train_size, test_size=test_size, seed=42\n",
    "# )\n",
    "# downsampled_dataset\n",
    "\n",
    "downsampled_dataset=lm_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = 'EHRpattern'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-3,\n",
    "    weight_decay=0.1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "\n",
    "    dataloader_num_workers=0,\n",
    "    # debug='underflow_overflow',\n",
    "    \n",
    "    # use_multiprocessing=False,\n",
    "    # use_multiprocessing_for_evaluation=False,\n",
    ")\n",
    "\n",
    "training_args = training_args.set_optimizer(name=\"adamw_torch\", beta1=0.8)\n",
    "\n",
    "# torch.optim.AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "device\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\hokarami\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "# curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "# sudo apt-get install git-lfs\n",
    "\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DATA\\Tasks\\tedam2\\EHRpattern-finetuned-imdb is already a clone of https://huggingface.co/Hojjat/EHRpattern-finetuned-imdb. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7c0becb95848a3bcf9b0d52f6fdc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find jup_res.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mg-hojatkarami\u001b[0m (\u001b[33mhokarami\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\DATA\\Tasks\\tedam2\\wandb\\run-20230405_173123-ej9dky6o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hokarami/huggingface/runs/ej9dky6o' target=\"_blank\">trill-spot-27</a></strong> to <a href='https://wandb.ai/hokarami/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hokarami/huggingface' target=\"_blank\">https://wandb.ai/hokarami/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hokarami/huggingface/runs/ej9dky6o' target=\"_blank\">https://wandb.ai/hokarami/huggingface/runs/ej9dky6o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 1032.65\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4418891"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa73974417ba4fcc8b4dc90eb88b0bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (18) will be pushed upstream.\n",
      "Several commits (19) will be pushed upstream.\n",
      "Several commits (20) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6792, 'learning_rate': 9.334163036714376e-08, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616137a02ec54ba7a284dde59f5cbe4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30205413699150085, 'eval_runtime': 4.428, 'eval_samples_per_second': 190.378, 'eval_steps_per_second': 23.938, 'epoch': 1.0}\n",
      "{'train_runtime': 217.1243, 'train_samples_per_second': 59.192, 'train_steps_per_second': 7.401, 'train_loss': 0.6790788889153818, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1607, training_loss=0.6790788889153818, metrics={'train_runtime': 217.1243, 'train_samples_per_second': 59.192, 'train_steps_per_second': 7.401, 'train_loss': 0.6790788889153818, 'epoch': 1.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./saved_models/ehr-bert-tiny\"\n",
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", model=model, tokenizer=tokenizer,device=model.device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bun creatinine glucose hco3 na k mg hct platelets wbc paco2 pao2 ph alp alt ast albumin bilirubin lactate. paco2 pao2 ph lactate. lactate. paco2 pao2 ph sao2. paco2 pao2 ph sao2 lactate. fio2 paco2 pao2 ph sao2 lactate. bun creatinine hco3 mg hct platelets wbc paco2 pao2 ph sao2 alp alt ast albumin bilirubin lactate. lactate troponint. paco2 pao2 ph sao2. paco2 pao2 ph sao2 lactate. bun creatinine hco3 na mg hct platelets wbc fio2 alp alt ast albumin bilirubin lactate. nothing. nothing. paco2 pao2 ph sao2. bun creatinine mg hct platelets wbc fio2 alp alt ast bilirubin lactate troponint. nothing. nothing. paco2 pao2 ph sao2. bun creatinine glucose hco3 na k mg hct platelets wbc fio2 lactate troponint. nothing. nothing. paco2 pao2 ph sao2. bun creatinine mg hct platelets wbc fio2 alp alt ast bilirubin lactate. nothing. nothing. fio2. bun creatinine hco3 na mg hct platelets wbc paco2 pao2 ph sao2. lactate. paco2 pao2 ph. lactate. fio2. paco2 pao2 ph sao2. bun creatinine hco3 na mg hct platelets alp ast albumin bilirubin lactate. nothing. fio2. paco2 pao2 ph. lactate. nothing. fio2. paco2 pao2 ph sao2. bun creatinine hco3 na mg hct platelets wbc alp alt ast bilirubin lactate. nothing. fio2 paco2 pao2 ph sao2. bun creatinine hco3 na mg hct platelets wbc lactate. nothing. nothing. fio2. bun creatinine hco3 na mg hct platelets wbc paco2 pao2 ph sao2 alp alt ast bilirubin.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['nothing',\n",
       " '.',\n",
       " 'nothing',\n",
       " '.',\n",
       " 'bun',\n",
       " 'creatinine',\n",
       " 'glucose',\n",
       " 'hco3',\n",
       " 'na',\n",
       " 'k',\n",
       " 'mg',\n",
       " '[MASK]',\n",
       " 'platelets',\n",
       " 'wbc',\n",
       " '.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = raw_datasets['train'][0]['text']\n",
    "text\n",
    "text = 'nothing. nothing. bun creatinine glucose hco3 na k mg [MASK] platelets wbc.'\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> nothing. nothing. bun creatinine glucose hco3 na k mg hct platelets wbc.\t\t\t0.9987102746963501\n",
      ">>> nothing. nothing. bun creatinine glucose hco3 na k mg. platelets wbc.\t\t\t0.0005835213814862072\n",
      ">>> nothing. nothing. bun creatinine glucose hco3 na k mg platelets platelets wbc.\t\t\t0.00018369774625170976\n",
      ">>> nothing. nothing. bun creatinine glucose hco3 na k mg wbc platelets wbc.\t\t\t0.00010300029680365697\n",
      ">>> nothing. nothing. bun creatinine glucose hco3 na k mg k platelets wbc.\t\t\t7.401059701805934e-05\n"
     ]
    }
   ],
   "source": [
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\\t\\t\\t{pred['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bun creatinine glucose hco3 na k mg hct platelets wbc paco2 pao2 ph alp alt ast albumin bilirubin lactate. paco2 pao2 ph lactate. lactate. paco2 pao2 ph sao2. paco2 pao2 ph sao2 lactate. fio2 paco2 pao2 ph sao2 lactate. bun creatinine hco3 mg hct platelets wbc paco2 pao2 ph sao2 alp alt ast albumin bilirubin lactate. lactate troponint. paco2 pao2 ph sao2. paco2 pao2 ph sao2 lactate. bun creatinine hco3 na mg hct platelets wbc fio2 alp alt ast albumin bilirubin lactate. nothing. nothing. paco2 pao2 ph sao2. bun creatinine mg hct platelets wbc fio2 alp alt ast bilirubin lactate troponint. nothing. nothing. paco2 pao2 ph sao2. bun creatinine glucose hco3 na k mg hct platelets wbc fio2 lactate troponint. nothing. nothing. paco2 pao2 ph sao2. bun creatinine mg hct platelets wbc fio2 alp alt ast bilirubin lactate. nothing. nothing. fio2. bun creatinine hco3 na mg hct platelets wbc paco2 pao2 ph sao2. lactate. paco2 pao2 ph. lactate. fio2. paco2 pao2 ph sao2. bun creatinine hco3 na mg hct platelets alp ast albumin bilirubin lactate. nothing. fio2. paco2 pao2 ph. lactate. nothing. fio2. paco2 pao2 ph sao2. bun creatinine hco3 na mg hct platelets wbc alp alt ast bilirubin lactate. nothing. fio2 paco2 pao2 ph sao2. bun creatinine hco3 na mg hct platelets wbc lactate. nothing. nothing. fio2. bun creatinine hco3 na mg hct platelets wbc paco2 pao2 ph sao2 alp alt ast bilirubin.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creatinine  ###  creatinine ### 0.9522609710693359\n",
      "fio2  ###  glucose ### 0.9617787003517151\n",
      "hco3  ###  hco3 ### 0.6788309812545776\n",
      "k  ###  na ### 0.5549286007881165\n",
      ".  ###  k ### 0.5728877782821655\n",
      "mg  ###  mg ### 0.9144839644432068\n",
      "fio2  ###  hct ### 0.9942979216575623\n",
      "platelets  ###  platelets ### 0.9490697383880615\n",
      "wbc  ###  wbc ### 0.9309682846069336\n",
      "fio2  ###  paco2 ### 0.9964274764060974\n",
      "ph  ###  pao2 ### 0.9656757712364197\n",
      ".  ###  ph ### 0.8465187549591064\n",
      "lactate  ###  alp ### 0.9983721375465393\n",
      "fio2  ###  alt ### 0.9951778650283813\n",
      "ast  ###  ast ### 0.9526471495628357\n",
      "bilirubin  ###  albumin ### 0.9533405900001526\n",
      "bilirubin  ###  bilirubin ### 0.9767340421676636\n",
      "lactate  ###  lactate ### 0.9986942410469055\n",
      "fio2  ###  paco2 ### 0.8867586255073547\n",
      "ph  ###  pao2 ### 0.9418728947639465\n",
      ".  ###  ph ### 0.9561371207237244\n",
      "sao2  ###  lactate ### 0.9973249435424805\n",
      "fio2  ###  lactate ### 0.8113406896591187\n",
      "fio2  ###  paco2 ### 0.7835914492607117\n",
      "ph  ###  pao2 ### 0.8770282864570618\n",
      ".  ###  ph ### 0.9630685448646545\n",
      "sao2  ###  sao2 ### 0.9936536550521851\n",
      "fio2  ###  paco2 ### 0.6994732022285461\n",
      "ph  ###  pao2 ### 0.7666816115379333\n",
      ".  ###  ph ### 0.9518593549728394\n",
      "sao2  ###  sao2 ### 0.9830701351165771\n",
      "lactate  ###  lactate ### 0.9991986155509949\n",
      ".  ###  fio2 ### 0.4706360697746277\n",
      "lactate  ###  paco2 ### 0.998936116695404\n",
      "ph  ###  pao2 ### 0.6239050626754761\n",
      ".  ###  ph ### 0.9706963300704956\n",
      "sao2  ###  sao2 ### 0.9832344055175781\n",
      "lactate  ###  lactate ### 0.9990993738174438\n",
      ".  ###  bun ### 0.4930155873298645\n",
      "creatinine  ###  creatinine ### 0.9800753593444824\n",
      "fio2  ###  hco3 ### 0.997171938419342\n",
      "wbc  ###  mg ### 0.9653249382972717\n",
      "fio2  ###  hct ### 0.9859110116958618\n",
      "fio2  ###  platelets ### 0.9649256467819214\n",
      "wbc  ###  wbc ### 0.8066273927688599\n",
      "fio2  ###  paco2 ### 0.975904107093811\n",
      "ph  ###  pao2 ### 0.7728267908096313\n",
      ".  ###  ph ### 0.9694930911064148\n",
      "sao2  ###  sao2 ### 0.9925952553749084\n",
      "lactate  ###  alp ### 0.9992578625679016\n",
      "alt  ###  alt ### 0.9582849740982056\n",
      "ast  ###  ast ### 0.6427797675132751\n",
      "bilirubin  ###  albumin ### 0.9210928082466125\n",
      "bilirubin  ###  bilirubin ### 0.9783808588981628\n",
      "lactate  ###  lactate ### 0.9982725381851196\n",
      "fio2  ###  lactate ### 0.7099607586860657\n",
      "fio2  ###  troponint ### 0.999642014503479\n",
      "fio2  ###  paco2 ### 0.7876417636871338\n",
      "ph  ###  pao2 ### 0.8237283229827881\n",
      ".  ###  ph ### 0.9818029403686523\n",
      "sao2  ###  sao2 ### 0.9787623286247253\n",
      "fio2  ###  paco2 ### 0.7149807214736938\n",
      "ph  ###  pao2 ### 0.8055871725082397\n",
      ".  ###  ph ### 0.9508777260780334\n",
      "sao2  ###  sao2 ### 0.96043461561203\n",
      "lactate  ###  lactate ### 0.99866783618927\n",
      "fio2  ###  bun ### 0.5244832634925842\n",
      "creatinine  ###  creatinine ### 0.970092236995697\n",
      "fio2  ###  hco3 ### 0.9937816262245178\n",
      "wbc  ###  na ### 0.9036407470703125\n",
      "k  ###  mg ### 0.6975274085998535\n",
      "fio2  ###  hct ### 0.95503169298172\n",
      "wbc  ###  platelets ### 0.9004697799682617\n",
      "wbc  ###  wbc ### 0.690013587474823\n",
      "fio2  ###  fio2 ### 0.97397780418396\n",
      "lactate  ###  alp ### 0.9989627599716187\n",
      "alt  ###  alt ### 0.9093157649040222\n",
      "ast  ###  ast ### 0.7580519318580627\n",
      "bilirubin  ###  albumin ### 0.8772287964820862\n",
      "bilirubin  ###  bilirubin ### 0.9427828192710876\n",
      "lactate  ###  lactate ### 0.9988822340965271\n",
      "fio2  ###  nothing ### 0.4867331385612488\n",
      "fio2  ###  nothing ### 0.7696352005004883\n",
      "fio2  ###  paco2 ### 0.8016277551651001\n",
      "ph  ###  pao2 ### 0.8451480865478516\n",
      ".  ###  ph ### 0.972005307674408\n",
      "sao2  ###  sao2 ### 0.9823111295700073\n",
      "fio2  ###  bun ### 0.7307829260826111\n",
      "creatinine  ###  creatinine ### 0.9535138010978699\n",
      "sao2  ###  mg ### 0.9949507713317871\n",
      "fio2  ###  hct ### 0.9928784966468811\n",
      "fio2  ###  platelets ### 0.9271224141120911\n",
      "wbc  ###  wbc ### 0.8240365386009216\n",
      "fio2  ###  fio2 ### 0.9811267256736755\n",
      "lactate  ###  alp ### 0.9991348385810852\n",
      "alt  ###  alt ### 0.9032666683197021\n",
      "ast  ###  ast ### 0.5403990745544434\n",
      "bilirubin  ###  bilirubin ### 0.7699323892593384\n",
      "lactate  ###  lactate ### 0.9975516200065613\n",
      "troponint  ###  troponint ### 0.9995149374008179\n",
      ".  ###  nothing ### 0.4482356309890747\n",
      "fio2  ###  nothing ### 0.5202252268791199\n",
      ".  ###  paco2 ### 0.5402005910873413\n",
      "ph  ###  pao2 ### 0.49717941880226135\n",
      ".  ###  ph ### 0.9906530380249023\n",
      "sao2  ###  sao2 ### 0.9723253846168518\n",
      "fio2  ###  bun ### 0.7510919570922852\n",
      "creatinine  ###  creatinine ### 0.9021651744842529\n",
      "bilirubin  ###  glucose ### 0.9803544878959656\n",
      "hco3  ###  hco3 ### 0.8853924870491028\n",
      "wbc  ###  na ### 0.7001001238822937\n",
      "k  ###  k ### 0.45885634422302246\n",
      "mg  ###  mg ### 0.8531439900398254\n",
      "fio2  ###  hct ### 0.9820644855499268\n",
      "platelets  ###  platelets ### 0.8479875922203064\n",
      ".  ###  wbc ### 0.5591146349906921\n",
      "fio2  ###  fio2 ### 0.9667975902557373\n",
      "lactate  ###  lactate ### 0.9960676431655884\n",
      "troponint  ###  troponint ### 0.9992709755897522\n",
      ".  ###  nothing ### 0.4973087012767792\n",
      ".  ###  nothing ### 0.7875192165374756\n",
      ".  ###  paco2 ### 0.7044934034347534\n",
      ".  ###  pao2 ### 0.6369767189025879\n",
      ".  ###  ph ### 0.9905809760093689\n",
      "sao2  ###  sao2 ### 0.9823054671287537\n",
      ".  ###  bun ### 0.46658143401145935\n",
      "creatinine  ###  creatinine ### 0.9223164319992065\n",
      "ph  ###  mg ### 0.978215754032135\n",
      "fio2  ###  hct ### 0.9732787609100342\n",
      "fio2  ###  platelets ### 0.8900198340415955\n",
      "wbc  ###  wbc ### 0.8852689266204834\n",
      "fio2  ###  fio2 ### 0.9690943956375122\n",
      "lactate  ###  alp ### 0.995930016040802\n",
      "alt  ###  alt ### 0.8153826594352722\n",
      ".  ###  ast ### 0.46101799607276917\n",
      ".  ###  bilirubin ### 0.6854968667030334\n",
      "bilirubin  ###  lactate ### 0.9860397577285767\n",
      ".  ###  nothing ### 0.6034293174743652\n",
      ".  ###  nothing ### 0.7253345251083374\n",
      ".  ###  fio2 ### 0.6004015803337097\n",
      ".  ###  bun ### 0.4870368540287018\n",
      "creatinine  ###  creatinine ### 0.8051730990409851\n",
      "bilirubin  ###  hco3 ### 0.9581873416900635\n",
      "k  ###  na ### 0.7200896739959717\n",
      ".  ###  mg ### 0.4181053936481476\n",
      "fio2  ###  hct ### 0.8658396005630493\n",
      "platelets  ###  platelets ### 0.6237199902534485\n",
      ".  ###  wbc ### 0.7485030293464661\n",
      "fio2  ###  paco2 ### 0.9468978643417358\n",
      ".  ###  pao2 ### 0.6332618594169617\n",
      ".  ###  ph ### 0.9893914461135864\n",
      "sao2  ###  sao2 ### 0.9869972467422485\n",
      ".  ###  lactate ### 0.4784136414527893\n",
      "fio2  ###  paco2 ### 0.4935431480407715\n",
      ".  ###  pao2 ### 0.7895355820655823\n",
      ".  ###  ph ### 0.9859317541122437\n",
      ".  ###  lactate ### 0.5019844174385071\n",
      ".  ###  fio2 ### 0.6428470015525818\n",
      ".  ###  paco2 ### 0.6645451784133911\n",
      ".  ###  pao2 ### 0.632424533367157\n",
      ".  ###  ph ### 0.9913709163665771\n",
      "sao2  ###  sao2 ### 0.955435574054718\n",
      ".  ###  bun ### 0.5141018629074097\n",
      "creatinine  ###  creatinine ### 0.9213138818740845\n",
      "ph  ###  hco3 ### 0.9817830920219421\n",
      "k  ###  na ### 0.8444665670394897\n",
      "k  ###  mg ### 0.34800592064857483\n",
      "fio2  ###  hct ### 0.8938515186309814\n",
      "platelets  ###  platelets ### 0.8939123153686523\n",
      "wbc  ###  alp ### 0.7036808133125305\n",
      "alt  ###  ast ### 0.8831896781921387\n",
      "bilirubin  ###  albumin ### 0.9675910472869873\n",
      "bilirubin  ###  bilirubin ### 0.9578315019607544\n",
      "lactate  ###  lactate ### 0.9979010820388794\n",
      ".  ###  nothing ### 0.4754698872566223\n",
      ".  ###  fio2 ### 0.5622072219848633\n",
      ".  ###  paco2 ### 0.6296811699867249\n",
      ".  ###  pao2 ### 0.6362660527229309\n",
      ".  ###  ph ### 0.9945387244224548\n",
      ".  ###  lactate ### 0.5992793440818787\n",
      ".  ###  nothing ### 0.5806910991668701\n",
      ".  ###  fio2 ### 0.7639752626419067\n",
      ".  ###  paco2 ### 0.6440567374229431\n",
      ".  ###  pao2 ### 0.7564075589179993\n",
      ".  ###  ph ### 0.9889146685600281\n",
      "sao2  ###  sao2 ### 0.9565091133117676\n",
      "fio2  ###  bun ### 0.49360617995262146\n",
      "creatinine  ###  creatinine ### 0.7529797554016113\n",
      "ph  ###  hco3 ### 0.9459490180015564\n",
      "fio2  ###  na ### 0.7374187707901001\n",
      ".  ###  mg ### 0.40405839681625366\n",
      "fio2  ###  hct ### 0.7890039086341858\n",
      "platelets  ###  platelets ### 0.6646237373352051\n",
      ".  ###  wbc ### 0.541925311088562\n",
      "fio2  ###  alp ### 0.9156269431114197\n",
      "alt  ###  alt ### 0.41218501329421997\n",
      "bilirubin  ###  ast ### 0.7027757167816162\n",
      "bilirubin  ###  bilirubin ### 0.48647159337997437\n",
      "lactate  ###  lactate ### 0.9895423650741577\n",
      ".  ###  nothing ### 0.6772859692573547\n",
      ".  ###  fio2 ### 0.7450780272483826\n",
      "lactate  ###  paco2 ### 0.992682158946991\n",
      ".  ###  pao2 ### 0.690903902053833\n",
      ".  ###  ph ### 0.9908971190452576\n",
      "sao2  ###  sao2 ### 0.9749739766120911\n",
      ".  ###  bun ### 0.5655393004417419\n",
      "creatinine  ###  creatinine ### 0.8640872836112976\n",
      "fio2  ###  hco3 ### 0.9709787964820862\n",
      "k  ###  na ### 0.791496753692627\n",
      ".  ###  mg ### 0.508127748966217\n",
      "fio2  ###  hct ### 0.8205815553665161\n",
      "platelets  ###  platelets ### 0.6604822874069214\n",
      ".  ###  wbc ### 0.5441305637359619\n",
      "fio2  ###  lactate ### 0.9444583058357239\n",
      ".  ###  nothing ### 0.7924302220344543\n",
      "hct  ###  nothing ### 0.8346607089042664\n",
      ".  ###  fio2 ### 0.7667409777641296\n",
      "fio2  ###  bun ### 0.5811108350753784\n",
      "creatinine  ###  creatinine ### 0.7593045234680176\n",
      "fio2  ###  hco3 ### 0.8980510830879211\n",
      "k  ###  na ### 0.5259431600570679\n",
      "fio2  ###  mg ### 0.4713093936443329\n",
      "fio2  ###  hct ### 0.8168475031852722\n",
      "platelets  ###  platelets ### 0.5964788198471069\n",
      ".  ###  wbc ### 0.6679285168647766\n",
      "fio2  ###  paco2 ### 0.8663213849067688\n",
      ".  ###  pao2 ### 0.8797826766967773\n",
      ".  ###  ph ### 0.9958892464637756\n",
      "sao2  ###  sao2 ### 0.9513116478919983\n",
      "lactate  ###  alp ### 0.9828855991363525\n",
      "ph  ###  alt ### 0.5400533676147461\n",
      "bilirubin  ###  ast ### 0.5939027667045593\n",
      ".  ###  bilirubin ### 0.8589106202125549\n"
     ]
    }
   ],
   "source": [
    "text = raw_datasets['train'][0]['text']\n",
    "text\n",
    "seq = text.replace(\".\", \" .\").split(\" \")\n",
    "\n",
    "for i in range(1,len(seq)):\n",
    "    if seq[i]=='.':\n",
    "        continue\n",
    "    temp = seq[:i+1].copy()\n",
    "    temp[i]= '[MASK]'\n",
    "    temp = ' '.join(temp).replace(\" .\", \".\")\n",
    "\n",
    "    # temp\n",
    "    # mask_filler(temp)\n",
    "    print(mask_filler(temp)[1]['token_str'], ' ### ', seq[i], '###', mask_filler(temp)[0]['score']) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "* https://huggingface.co/course/chapter7/6?fw=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocab_list.txt', 'r') as file:\n",
    "#     content = file.read()\n",
    "#     vocab_list = content.split()\n",
    "#     print(vocab_list)\n",
    "\n",
    "# len(vocab_list)\n",
    "\n",
    "\n",
    "vocab_list = [ f\"eso{i}\" for i in range(22)] + ['Ä ']\n",
    "\n",
    "\n",
    "with open('vocab_list_so.txt', 'w') as file:\n",
    "    content = file.write(\" \".join(vocab_list))\n",
    "    # vocab_list = content.split()\n",
    "    # print(vocab_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/hokarami/.cache/huggingface/datasets/Hojjat___parquet/Hojjat--so-38585404d8f1c126/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50163a9199584b1787805fdf64aa28f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load so dataset from hub\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"Hojjat/so\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eso5 eso13 eso3 eso3 eso2 eso3 eso3 eso5 eso13 eso7 eso7 eso17 eso17 eso7 eso17 eso11 eso5 eso3 eso5 eso13 eso5 eso5 eso5 eso5 eso3 eso5 eso3 eso8 eso1 eso1 eso7 eso4 eso5 eso8 eso0 eso11 eso5 eso1 eso3 eso8 eso5 eso1 eso14 eso0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my own tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(raw_datasets['train']), 100):\n",
    "        yield raw_datasets['train'][i : i + 100][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'s\", (3, 5)),\n",
       " ('Ä test', (5, 10)),\n",
       " ('Ä pre', (10, 14)),\n",
       " ('-', (14, 15)),\n",
       " ('tokenization', (15, 27)),\n",
       " ('!', (27, 28))]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'eso17': 17,\n",
       " 'Ä eso': 17,\n",
       " '4': 5,\n",
       " '9': 10,\n",
       " 'eso20': 20,\n",
       " '0': 1,\n",
       " 'eso10': 10,\n",
       " 's': 13,\n",
       " '1': 2,\n",
       " '18': 22,\n",
       " 'eso9': 9,\n",
       " '5': 6,\n",
       " '8': 9,\n",
       " 'eso8': 8,\n",
       " '2': 3,\n",
       " 'eso3': 3,\n",
       " 'eso16': 16,\n",
       " '3': 4,\n",
       " '12': 20,\n",
       " '6': 7,\n",
       " 'eso1': 1,\n",
       " 'eso13': 13,\n",
       " '7': 8,\n",
       " '<|endoftext|>': 0,\n",
       " 'eso': 16,\n",
       " 'eso6': 6,\n",
       " 'eso19': 19,\n",
       " 'eso11': 11,\n",
       " 'es': 15,\n",
       " '13': 19,\n",
       " 'o': 12,\n",
       " '17': 21,\n",
       " 'eso14': 14,\n",
       " '11': 18,\n",
       " 'eso21': 21,\n",
       " 'eso2': 2,\n",
       " 'eso4': 4,\n",
       " 'eso0': 0,\n",
       " 'eso5': 5,\n",
       " 'e': 11,\n",
       " 'eso18': 18,\n",
       " 'eso15': 15,\n",
       " 'Ä ': 22,\n",
       " 'eso12': 12,\n",
       " 'eso7': 7}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")\n",
    "tokenizer.add_tokens(vocab_list)\n",
    "\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=23, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eso1', 'Ä ', 'eso2']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"eso1 eso2\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 8\n",
      "Input chunk lengths: [87, 128, 27, 107, 128, 45, 128, 13]\n",
      "Chunk mapping: [0, 1, 1, 2, 3, 3, 4, 4]\n",
      "len(tokenizer.get_vocab()): 44\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "model_checkpoint = \"prajjwal1/bert-tiny\"\n",
    "model_checkpoint = \"gpt2\"\n",
    "model_checkpoint = \"distilgpt2\"\n",
    "# model_checkpoint = \"Hojjat/ehr_gpt2\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# tokenizer.add_tokens(vocab_list)\n",
    "\n",
    "tokenizer = wrapped_tokenizer\n",
    "\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:5][\"text\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")\n",
    "print(f\"len(tokenizer.get_vocab()): {len(tokenizer.get_vocab())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9478006123943fab68181bd3d0f04fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4777 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3275cb6c762a4605902999859d61be79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2915115c954032868f8d2ed0d73950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 2493\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 708\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 274\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokenize dataset\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(44, 768)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 43.3M parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, AutoModelForCausalLM\n",
    "from transformers import GPT2Config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# config = GPT2Config(vocab_size=len(tokenizer),n_positions=256,n_layer=4,n_head=4,n_embed=256, max_length=100)\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "device\n",
    "_ = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 128])\n",
      "attention_mask shape: torch.Size([5, 128])\n",
      "labels shape: torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]='true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\hokarami\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DATA\\Tasks\\tedam2\\codeparrot-ds is already a clone of https://huggingface.co/Hojjat/codeparrot-ds. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"codeparrot-ds\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"dev\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f9ddc97b0d4fdd869132438259c92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 233.2592, 'train_samples_per_second': 10.688, 'train_steps_per_second': 0.167, 'train_loss': 1.6204472077198517, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=1.6204472077198517, metrics={'train_runtime': 233.2592, 'train_samples_per_second': 10.688, 'train_steps_per_second': 0.167, 'train_loss': 1.6204472077198517, 'epoch': 1.0})"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869c5560c4c04800bb5af0d897c73bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/115M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9268978d6f64f1abff1cf19589d276d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Hojjat/so_gpt2/commit/0e66078857d04101aff18e4e725014d6f00ec7ce', commit_message='Upload model', commit_description='', oid='0e66078857d04101aff18e4e725014d6f00ec7ce', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"Hojjat/so_gpt2\")\n",
    "model.push_to_hub(\"Hojjat/so_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Hojjat/so_gpt2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Hojjat/so_gpt2\",never_split=vocab_list)\n",
    "# tokenizer.add_tokens(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer,device=model.device,  pad_token_id=50256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [04:55<00:00,  2.95s/it]\n"
     ]
    }
   ],
   "source": [
    "list_C = []\n",
    "list_I = []\n",
    "list_I_b = []\n",
    "\n",
    "selected_dataset = raw_datasets['test']\n",
    "\n",
    "N_patients = len(selected_dataset)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "bad_i=[]\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(100)):\n",
    "        text = selected_dataset[i]['text']\n",
    "        seq = text.split(\" \")\n",
    "\n",
    "        C,I,I_b=0,0,0\n",
    "        list_temp = []\n",
    "\n",
    "        \n",
    "        for L in range(1,len(seq)):\n",
    "\n",
    "            temp = ' '.join(seq[:L])\n",
    "            list_temp.append(temp)\n",
    "            # sss = tokenizer(temp, return_tensors='pt')\n",
    "            try:    \n",
    "                preds = generator(temp,max_new_tokens=2, num_return_sequences=1)\n",
    "            except:\n",
    "                bad_i.append([i,L])\n",
    "                continue\n",
    "            text_pred = preds[0]['generated_text']\n",
    "            seq_pred = text_pred.split(\" \")\n",
    "\n",
    "            if len(seq_pred)==(L+2):\n",
    "                y_true.append(seq[L])\n",
    "                y_pred.append(seq_pred[L+1])\n",
    "\n",
    "                if seq[L]==seq_pred[L+1]:\n",
    "                    C+=1\n",
    "                else:\n",
    "                    I+=1\n",
    "            else:\n",
    "                I_b+=1\n",
    "\n",
    "        list_C.append(C)\n",
    "        list_I.append(I)\n",
    "        list_I_b.append(I_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([], dtype=float64), array([], dtype=int64))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([], dtype=float64), array([], dtype=int64))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true1[:10]\n",
    "y_pred1[:10]\n",
    "\n",
    "np.unique(np.array(y_true1),return_counts=True)\n",
    "np.unique(np.array(y_pred1),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.43478260869565216"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.2934294043131835"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,  386,    0,   37,    0,    0],\n",
       "       [   0,    0,    0,   53,    0,  112,    0,    0],\n",
       "       [   0,    0,    0,   11,    0,   44,    0,    0],\n",
       "       [   0,    0,    0, 2650,    0,  147,    0,    0],\n",
       "       [   0,    0,    0,  332,    0,   36,    0,    0],\n",
       "       [   0,    0,    0,  180,    0,  271,    0,    0],\n",
       "       [   0,    0,    0,  109,    0,   17,    0,    0],\n",
       "       [   0,    0,    0,  105,    0,   45,    0,    0]], dtype=int64)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq_pred)\n",
    "\n",
    "y_true1 = [int(i[3:]) for i in y_true]\n",
    "y_pred1 = [int(i[3:]) for i in y_pred]\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "accuracy_score(y_true1, y_pred1)\n",
    "f1_score(y_true1, y_pred1, average='weighted')\n",
    "confusion_matrix(y_true1, y_pred1)[:8,:8]\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_C\n",
    "list_I\n",
    "list_I_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bun': 0,\n",
       " 'creatinine': 1,\n",
       " 'glucose': 2,\n",
       " 'hco3': 3,\n",
       " 'na': 4,\n",
       " 'k': 5,\n",
       " 'mg': 6,\n",
       " 'hct': 7,\n",
       " 'platelets': 8,\n",
       " 'wbc': 9,\n",
       " 'fio2': 10,\n",
       " 'paco2': 11,\n",
       " 'pao2': 12,\n",
       " 'ph': 13,\n",
       " 'sao2': 14,\n",
       " 'alp': 15,\n",
       " 'alt': 16,\n",
       " 'ast': 17,\n",
       " 'albumin': 18,\n",
       " 'bilirubin': 19,\n",
       " 'lactate': 20,\n",
       " 'cholesterol': 21,\n",
       " 'troponini': 22,\n",
       " 'troponint': 23}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_vocab = {word:idx for idx,word in enumerate(vocab_list[:-2])}\n",
    "dict_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nothing. nothing. bun creatinine glucose hco3 na k mg hct platelets wbc. nothing. nothing. nothing. nothing. nothing. nothing. nothing. nothing. lactate. nothing. nothing. nothing. nothing. nothing. bun creatinine glucose hco3 na k mg hct platelets wbc lactate. nothing. nothing. nothing. nothing. nothing. nothing. lactate. nothing. nothing. nothing. nothing. nothing. nothing. nothing. nothing. nothing. nothing. nothing. nothing. nothing. nothing. nothing. bun creatinine glucose hco3 na k mg hct platelets wbc.'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(70, 24)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(24, 41)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert text to matrix of one-hot vectors\n",
    "\n",
    "n_words = len(dict_vocab)\n",
    "\n",
    "text = raw_datasets['test'][0]['text']\n",
    "text\n",
    "temp = text.replace(\".\", \" .\").split(\".\")\n",
    "# temp\n",
    "temp = [x.split(\" \") for x in temp]\n",
    "# temp\n",
    "temp = [ [y for y in x if (y!='' and y!='nothing')] for x in temp]\n",
    "\n",
    "temp = [[dict_vocab[y] for y in x] for x in temp]\n",
    "\n",
    "\n",
    "def one_hot(x_list, n_words):\n",
    "\n",
    "\n",
    "    \n",
    "    if x_list:\n",
    "        x_one_hot = np.zeros((len(x_list), n_words),dtype=int)\n",
    "\n",
    "        for i in range(len(x_list)):\n",
    "            x_one_hot[i,x_list[i]] = 1\n",
    "    else:\n",
    "        x_one_hot = np.zeros((1, n_words),dtype=int)\n",
    "\n",
    "    return x_one_hot\n",
    "\n",
    "temp = [one_hot(x, n_words) for x in temp]\n",
    "temp = np.concatenate(temp, axis=0)\n",
    "temp.shape\n",
    "\n",
    "np.array(raw_datasets['test'][0]['mat']).shape\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (194465 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer(\"\\n\\n\".join(raw_datasets[\"test\"][\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50260,   220, 50260,  ..., 50265,   220, 50265]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(\"\\n\\n\".join(raw_datasets[\"test\"][\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 194465])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 756/760 [03:00<00:00,  4.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 256\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over input tokens.\n",
    "        # Multiply it with trg_len to get the summation instead of average.\n",
    "        # We will take average over all the tokens to get the true average\n",
    "        # in the last step of this example.\n",
    "        neg_log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.8157, device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m L \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(seq)):\n\u001b[0;32m     15\u001b[0m     temp \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(seq[:L])\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m .\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     preds \u001b[39m=\u001b[39m generator(temp,max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,bos_token_id\u001b[39m=\u001b[39;49m\u001b[39m50256\u001b[39;49m, num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     17\u001b[0m     text_pred \u001b[39m=\u001b[39m preds[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     18\u001b[0m     seq_pred \u001b[39m=\u001b[39m text_pred\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m .\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:209\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    169\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\transformers\\pipelines\\base.py:1109\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[0;32m   1102\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[0;32m   1103\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1106\u001b[0m         )\n\u001b[0;32m   1107\u001b[0m     )\n\u001b[0;32m   1108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1109\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\transformers\\pipelines\\base.py:1116\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1115\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1116\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1117\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1118\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\transformers\\pipelines\\base.py:1015\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1014\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1015\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1016\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   1017\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:251\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m prompt_text \u001b[39m=\u001b[39m model_inputs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mprompt_text\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    250\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(input_ids\u001b[39m=\u001b[39minput_ids, attention_mask\u001b[39m=\u001b[39mattention_mask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    252\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    253\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\transformers\\generation\\utils.py:1452\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[0;32m   1444\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1445\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1446\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1447\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1448\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1449\u001b[0m     )\n\u001b[0;32m   1451\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1452\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[0;32m   1453\u001b[0m         input_ids,\n\u001b[0;32m   1454\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1455\u001b[0m         logits_warper\u001b[39m=\u001b[39mlogits_warper,\n\u001b[0;32m   1456\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1457\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1458\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1459\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1460\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1461\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1462\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1463\u001b[0m     )\n\u001b[0;32m   1465\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[0;32m   1466\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\transformers\\generation\\utils.py:2468\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2465\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2467\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2468\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2469\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2470\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2471\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2472\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2473\u001b[0m )\n\u001b[0;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2476\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\transformers\\models\\openai\\modeling_openai.py:575\u001b[0m, in \u001b[0;36mOpenAIGPTLMHeadModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    573\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m--> 575\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m    576\u001b[0m     input_ids,\n\u001b[0;32m    577\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    578\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m    579\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    580\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    581\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    582\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    583\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    584\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    585\u001b[0m )\n\u001b[0;32m    586\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    587\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\transformers\\models\\openai\\modeling_openai.py:486\u001b[0m, in \u001b[0;36mOpenAIGPTModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    483\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_layer)\n\u001b[0;32m    485\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokens_embed(input_ids)\n\u001b[0;32m    487\u001b[0m position_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositions_embed(position_ids)\n\u001b[0;32m    488\u001b[0m \u001b[39mif\u001b[39;00m token_type_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\hokarami\\Anaconda3\\envs\\hf\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# seq[L]\n",
    "# seq_pred[L]\n",
    "\n",
    "list_C = []\n",
    "list_I = []\n",
    "\n",
    "N_patients = len(raw_datasets['test'])\n",
    "\n",
    "for i in range(5):\n",
    "    text = raw_datasets['test'][i]['text']\n",
    "    seq = text.replace(\".\", \" .\").split(\" \")\n",
    "\n",
    "    C,I=0,0\n",
    "    for L in range(len(seq)):\n",
    "        temp = ' '.join(seq[:L]).replace(\" .\", \".\")\n",
    "        preds = generator(temp,max_new_tokens=1,bos_token_id=50256, num_return_sequences=1)\n",
    "        text_pred = preds[0]['generated_text']\n",
    "        seq_pred = text_pred.replace(\".\", \" .\").split(\" \")\n",
    "\n",
    "        if seq[L]==seq_pred[L]:\n",
    "            C+=1\n",
    "        else:\n",
    "            I+=1\n",
    "\n",
    "    list_C.append(C)\n",
    "    list_I.append(I)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[80, 51, 52, 64, 64]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[30, 84, 44, 97, 82]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_C\n",
    "list_I"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7057279a11ec1bf463ffd81238de3f02b33a50a166fbc2e69f9a8082c2c2e75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
