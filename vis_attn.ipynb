{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate line execution\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "# plotly\n",
    "import plotly.express as px  # (version 4.7.0 or higher)\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import custom libraries\n",
    "import sys\n",
    "# sys.path.append(\"C:\\\\DATA\\\\Tasks\\\\lib\\\\hk\")\n",
    "# import hk_utils\n",
    "\n",
    "# folder paths\n",
    "ADD_DATA = \"C:\\\\DATA\\\\data\\\\raw\\\\mimic4\\\\lookup\\\\\"\n",
    "ADD_DATA_proc = \"C:/DATA/data/processed/\"\n",
    "\n",
    "\n",
    "PATH_PAPER = \"C:\\\\DATA\\\\Tasks\\\\220704\\\\Alternate-Transactions-Articles-LaTeX-template\\\\images\\\\\"\n",
    "\n",
    "\n",
    "PATH_SYS=\"/mlodata1/hokarami/tedam/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for THP\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import transformer.Constants as Constants\n",
    "import Utils\n",
    "\n",
    "# from preprocess.Dataset import get_dataloader, get_dataloader2\n",
    "# from transformer.Models import Transformer\n",
    "# from transformer.hk_transformer import Transformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_allocated()\n",
    "# torch.cuda.memory_reserved()\n",
    "\n",
    "from sklearn import metrics\n",
    "# from hk_pytorch import save_checkpoint,load_checkpoint\n",
    "# import hk_pytorch\n",
    "\n",
    "\n",
    "# from custom2 import myparser\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Main\n",
    "import webbrowser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo conda install -c conda-forge dash --name paper2022\n",
    "# sudo conda install -c conda-forge jupyter-dash --name paper2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsnecuda import TSNE\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb -qqq\n",
    "import wandb\n",
    "# wandb.login()\n",
    "api = wandb.Api()\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"0f780ac8a470afe6cb7fc474ff3794772c660465\"\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"jup_res\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import dcc, html, Input, Output, no_update\n",
    "import dash_bootstrap_components as dbc\n",
    "# sudo conda install -c conda-forge dash-bootstrap-components --name paper2022\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from PIL import ImageDraw, Image\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Helper functions\n",
    "def np_image_to_base64(im_matrix,scale=4):\n",
    "\n",
    "    im_matrix = np.repeat(np.repeat(im_matrix,scale,axis=0),scale,axis=1)\n",
    "    im = Image.fromarray(im_matrix)\n",
    "    buffer = io.BytesIO()\n",
    "    im.save(buffer, format=\"jpeg\")\n",
    "    encoded_image = base64.b64encode(buffer.getvalue()).decode()\n",
    "    im_url = \"data:image/jpeg;base64, \" + encoded_image\n",
    "    return im_url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def binary_matrix_to_image(binary_matrix, grid_size=8, border_size=1):\n",
    "    # Calculate the size of the output image based on the size of the binary matrix\n",
    "    height, width = binary_matrix.shape[:2]\n",
    "    image_width = width * grid_size + (width + 1) * border_size\n",
    "    image_height = height * grid_size + (height + 1) * border_size\n",
    "    \n",
    "    # Create a new image and a draw object to draw the grid and borders\n",
    "    image = Image.new('RGB', (image_width, image_height), color='black')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Draw the white grids\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            if binary_matrix[i, j] == 1:\n",
    "                x1 = j * (grid_size + border_size) + border_size\n",
    "                y1 = i * (grid_size + border_size) + border_size\n",
    "                x2 = x1 + grid_size\n",
    "                y2 = y1 + grid_size\n",
    "                draw.rectangle((x1, y1, x2, y2), fill='white')\n",
    "                \n",
    "    \n",
    "    # Draw the black borders\n",
    "    for i in range(height + 1):\n",
    "        y = i * (grid_size + border_size)\n",
    "        draw.line((0, y, image_width, y), fill='white', width=border_size)\n",
    "        \n",
    "    for j in range(width + 1):\n",
    "        x = j * (grid_size + border_size)\n",
    "        draw.line((x, 0, x, image_height), fill='white', width=border_size)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_rgb(num, **rgb_params):\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(rgb_params)>0:\n",
    "\n",
    "        normalized = (num - rgb_params['offset'])/rgb_params['range']\n",
    "\n",
    "        \n",
    "        # Map to a color between blue, gray, and red\n",
    "        rgb_lower = rgb_params['rgb_lower']\n",
    "        rgb_middle = rgb_params['rgb_middle']\n",
    "        rgb_upper = rgb_params['rgb_upper']\n",
    "    else:\n",
    "    \n",
    "        # Normalize to the range of 0 to 1\n",
    "    \n",
    "    \n",
    "    \n",
    "        normalized = (num + 2) / 4\n",
    "        \n",
    "        # Map to a color between blue, gray, and red\n",
    "        rgb_lower = (0, 0, 255,1)\n",
    "        rgb_middle = (128, 128, 128,1)\n",
    "        rgb_upper = (255, 0, 0,1)\n",
    "    \n",
    "    if normalized < 0.5:\n",
    "        r = int((2 * normalized) * rgb_middle[0] + (1 - 2 * normalized) * rgb_lower[0])\n",
    "        g = int((2 * normalized) * rgb_middle[1] + (1 - 2 * normalized) * rgb_lower[1])\n",
    "        b = int((2 * normalized) * rgb_middle[2] + (1 - 2 * normalized) * rgb_lower[2])\n",
    "        a = int((2 * normalized) * rgb_middle[3] + (1 - 2 * normalized) * rgb_lower[3])\n",
    "    else:\n",
    "        r = int((2 * normalized - 1) * rgb_upper[0] + (2 - 2 * normalized) * rgb_middle[0])\n",
    "        g = int((2 * normalized - 1) * rgb_upper[1] + (2 - 2 * normalized) * rgb_middle[1])\n",
    "        b = int((2 * normalized - 1) * rgb_upper[2] + (2 - 2 * normalized) * rgb_middle[2])\n",
    "        a = int((2 * normalized - 1) * rgb_upper[3] + (2 - 2 * normalized) * rgb_middle[3])\n",
    "    \n",
    "    # if normalized>0.5:\n",
    "    #     print(normalized,a,rgb_upper[3])\n",
    "    #     term\n",
    "    return (r, g, b,a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import ImageDraw, Image, ImageFont\n",
    "\n",
    "def binary_matrix_to_image(binary_matrix, row_labels=None, font_path=None, grid_size=10, border_size=1, label_size=15, is_fill=True,**rgb_params):\n",
    "    # Add a dummy column to the binary matrix\n",
    "    ddd=2\n",
    "\n",
    "    height, width = binary_matrix.shape[:2]\n",
    "    binary_matrix = np.concatenate((np.zeros((height, ddd)), binary_matrix), axis=1)\n",
    "    width += ddd\n",
    "    \n",
    "    # Calculate the size of the output image based on the size of the binary matrix\n",
    "    image_width = (width + 1) * grid_size + (width + 2) * border_size\n",
    "    image_height = height * grid_size + (height + 1) * border_size\n",
    "    \n",
    "    # Create a new image and a draw object to draw the grid and borders\n",
    "    image = Image.new('RGBA', (image_width, image_height), color=(0,0,0,0))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Draw the white grids\n",
    "    for i in range(height):\n",
    "        for j in range(1, width):\n",
    "            # if binary_matrix[i, j] == 1:\n",
    "            #     x1 = j * (grid_size + border_size) + border_size\n",
    "            #     y1 = i * (grid_size + border_size) + border_size\n",
    "            #     x2 = x1 + grid_size\n",
    "            #     y2 = y1 + grid_size\n",
    "            #     draw.rectangle((x1, y1, x2, y2), fill='white')\n",
    "\n",
    "            if binary_matrix[i, j] != 0:\n",
    "                x1 = j * (grid_size + border_size) + border_size\n",
    "                y1 = i * (grid_size + border_size) + border_size\n",
    "                x2 = x1 + grid_size\n",
    "                y2 = y1 + grid_size\n",
    "\n",
    "                int_color = ( int(binary_matrix[i, j]*96),\n",
    "                             int(binary_matrix[i, j]*96),\n",
    "                             int(binary_matrix[i, j]*96))\n",
    "\n",
    "                \n",
    "                int_color = map_to_rgb(binary_matrix[i, j], **rgb_params)\n",
    "                if is_fill:\n",
    "                    draw.rectangle((x1, y1, x2, y2), fill= int_color)\n",
    "                else:\n",
    "                    draw.rectangle((x1, y1, x2, y2), outline=int_color,width=2, fill= None)\n",
    "\n",
    "\n",
    "    # Draw the borders\n",
    "    color_border = (0,0,0,16)\n",
    "    for i in range(height + 1):\n",
    "        y = i * (grid_size + border_size)\n",
    "        draw.line((grid_size, y, image_width, y), fill=color_border, width=border_size)\n",
    "        \n",
    "    for j in range(width + 1):\n",
    "        x = j * (grid_size + border_size)\n",
    "        draw.line((x, 0, x, image_height), fill=color_border, width=border_size)\n",
    "\n",
    "    \n",
    "    # Draw the row labels\n",
    "    if row_labels is not None:\n",
    "        font = ImageFont.truetype(PATH_SYS+'arial.ttf', size=label_size)\n",
    "        max_label_width = max([font.getsize(str(label))[0] for label in row_labels])\n",
    "        label_x = 0\n",
    "        label_y = border_size\n",
    "        for i, label in enumerate(row_labels):\n",
    "            draw.text((label_x, label_y), str(label), font=font, fill='black',align=\"right\")\n",
    "            label_y += grid_size + border_size\n",
    "            # if i == 0:\n",
    "            #     label_x += max_label_width + border_size + grid_size\n",
    "        \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def create_barplot_image(data, labels, filename):\n",
    "    \"\"\"\n",
    "    Creates a bar plot image of the input data and saves it as a PNG file with the given filename.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): A 1D array of data to plot.\n",
    "        labels (list): A list of labels for each data point.\n",
    "        filename (str): The name of the output PNG file.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: A PIL image object of the bar plot.\n",
    "    \"\"\"\n",
    "    # Create a figure and axis object\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Create a bar plot\n",
    "    _ = ax.bar(np.arange(len(data)), data)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    _ = ax.set_xlabel('Index')\n",
    "    _ = ax.set_ylabel('Value')\n",
    "    _ = ax.set_title('Bar plot')\n",
    "\n",
    "    # Set x-tick labels\n",
    "    _ = ax.set_xticks(np.arange(len(data)))\n",
    "    _ = ax.set_xticklabels(labels, rotation=45)\n",
    "\n",
    "    # Save the plot as a PNG file\n",
    "    _ = fig.savefig(filename)\n",
    "    plt.close(fig)\n",
    "    # Load the PNG image and return as a PIL image object\n",
    "    return Image.open(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event2mat(event_type, event_time,P):\n",
    "\n",
    "    m = event_type.sum(1)>0 # False are masked\n",
    "    e=event_type[m,:]\n",
    "    t=event_time[m]\n",
    "    # print(t)\n",
    "\n",
    "    indices = e.nonzero()\n",
    "    indices[:,0] = t[indices[:,0]].int()\n",
    "    # print(t[indices[:,0]])\n",
    "    # print(indices[:,0])\n",
    "\n",
    "    M = torch.zeros((P, e.shape[-1]))\n",
    "    M[indices[:,0],indices[:,1]]=1\n",
    "\n",
    "    return M.detach().cpu().numpy().transpose() # [M,P]\n",
    "\n",
    "def att_event2mat(event_type, event_time,P, tee_att):\n",
    "    # tee_att [h,L]\n",
    "    h = tee_att.shape[0]\n",
    "\n",
    "    m = event_type.sum(1)>0 # False are masked\n",
    "    e=event_type[m,:]\n",
    "    t=event_time[m]\n",
    "    tee_att = tee_att[:,m]\n",
    "\n",
    "    # indices = e.nonzero()\n",
    "    # indices[:,0] = t[indices[:,0]].int()\n",
    "\n",
    "    # M = torch.zeros(h, len(t))\n",
    "    M = torch.zeros(( h , P))\n",
    "\n",
    "    M[:,t.long()]=tee_att\n",
    "\n",
    "    return M.detach().cpu().numpy() # [h,L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state2mat(time, mod,M):\n",
    "    P = time.int().max().item() + 1\n",
    "\n",
    "    M = torch.zeros(P, M)\n",
    "    M[time[mod!=0].long(), mod[mod!=0]-1] = 1\n",
    "\n",
    "\n",
    "    return M.cpu().numpy().transpose() # [M,P]\n",
    "\n",
    "def att_state2mat(time, mod,M, dam_att):\n",
    "    # dam_att [P,h]\n",
    "    # h = dam_att.shape[-1]\n",
    "\n",
    "    # dam_att = dam_att[:,0] # [P]\n",
    "    P = time.int().max().item() + 1\n",
    "\n",
    "    M = torch.zeros(P, M)\n",
    "    \n",
    "    M[time[mod!=0].long(),  mod[mod!=0]-1] = dam_att[mod!=0]\n",
    "\n",
    "\n",
    "    return M.cpu().numpy().transpose() # [M,P]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cool_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cool_image(out,i_b,i,opt,CROP=0):\n",
    "\n",
    "    att_mat_img=None\n",
    "    img_ev=None\n",
    "\n",
    "    label_size = 12\n",
    "    rgb_params_values = {\n",
    "    'rgb_lower':(0, 0, 255,255),\n",
    "    'rgb_middle':(128, 128, 128,255),\n",
    "    'rgb_upper':(255, 0, 0,255),\n",
    "    'offset':-2,\n",
    "    'range':4\n",
    "\n",
    "    }\n",
    "    rgb_params_events = {\n",
    "    'rgb_lower':(255, 255, 255,255),\n",
    "    'rgb_middle':(196, 196, 196,255),\n",
    "    'rgb_upper':(128, 128, 128,255),\n",
    "    'offset':0,\n",
    "    'range':1\n",
    "\n",
    "    }\n",
    "    rgb_params_events_pred = {\n",
    "    'rgb_lower':(255, 255, 255,255),\n",
    "    'rgb_middle':(128, 255, 128,255),\n",
    "    'rgb_upper':(0, 255, 0,255),\n",
    "    'offset':0,\n",
    "    'range':1\n",
    "\n",
    "    }\n",
    "    # rgb_params_att = {\n",
    "    #     'rgb_lower':(64, 64, 64,0),\n",
    "    #     'rgb_middle':(164, 164, 32,32),\n",
    "    #     'rgb_upper':(255, 255, 0,255),\n",
    "    #     'offset':0,\n",
    "    #     'range':0.1\n",
    "\n",
    "    # }\n",
    "\n",
    "\n",
    "    rgb_params_att = {\n",
    "        'rgb_lower':(128, 128, 128,32),\n",
    "        # 'rgb_middle':(8, 164, 32,196),\n",
    "        'rgb_upper':(0, 255, 0,255),\n",
    "        'offset':0,\n",
    "        'range':0.2\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    rgb_params_att['rgb_middle'] = tuple([(int(i+j)/2) for i,j in zip(rgb_params_att['rgb_lower'],rgb_params_att['rgb_upper'])])\n",
    "    rgb_params_att['rgb_middle']\n",
    "    if hasattr(opt,'dict_map_states' ):\n",
    "        state_labels = list(opt.dict_map_states.keys())[CROP:]\n",
    "        state_labels = [str(i) for i in range(len(opt.dict_map_states))]\n",
    "\n",
    "    if hasattr(opt,'dict_map_events' ):\n",
    "        offset=len(opt.dict_map_states.keys())-len(opt.dict_map_events.keys())\n",
    "        event_labels = list(opt.dict_map_events.keys())\n",
    "        event_labels = [str(i+offset) for i in range(len(opt.dict_map_events))]\n",
    "\n",
    "\n",
    "    # event_labels = opt.dict_map_events.keys()\n",
    "    \n",
    "\n",
    "    # if len(out['event_type_list']) > 0:\n",
    "    \n",
    "\n",
    "    if len(out['state_time_list']) > 0:\n",
    "        st=out['state_time_list'][i_b][i]\n",
    "        sm=out['state_mod_list'][i_b][i]\n",
    "        sv=( out['state_value_list'][i_b][i] )\n",
    "\n",
    "        dam_att = out['list_DAM_att'][i_b][i] # [P,h]\n",
    "\n",
    "        n_state_mods = len(opt.dict_map_states.keys())\n",
    "\n",
    "        # ev.shape, t.shape\n",
    "        P = st.int().max().item() + 1\n",
    "\n",
    "        \n",
    "        # list_img_att = []\n",
    "        # for i_head in range(dam_att.shape[-1]):\n",
    "        #     binary_matrix = att_state2mat(st, sm, n_state_mods, dam_att[:,i_head])\n",
    "        #     img_att = binary_matrix_to_image(binary_matrix, row_labels=state_labels, font_path=None, grid_size=10, border_size=1, label_size=label_size,is_fill=False,**rgb_params_att)\n",
    "        #     list_img_att.append(img_att.convert(\"RGB\"))\n",
    "\n",
    "        binary_matrix = att_state2mat(st, sm, n_state_mods, dam_att.sum(1))[CROP:,:]\n",
    "        # print(f'binary_matrix.shape={binary_matrix.shape}')\n",
    "        \n",
    "        img_att = binary_matrix_to_image(binary_matrix, row_labels=state_labels, font_path=None, grid_size=10, border_size=2, label_size=label_size,is_fill=False,**rgb_params_att)\n",
    "\n",
    "\n",
    "        binary_matrix = att_state2mat(st, sm, n_state_mods, sv)[CROP:,:]\n",
    "        img_val = binary_matrix_to_image(binary_matrix, row_labels=state_labels, font_path=None, grid_size=10, border_size=1, label_size=label_size,is_fill=True,**rgb_params_values)\n",
    "\n",
    "        merged_img = Image.new('RGBA', img_val.size)\n",
    "        merged_img = Image.alpha_composite(merged_img, img_val)\n",
    "        # merged_img = Image.alpha_composite(merged_img, img_att)#.convert(\"RGB\")\n",
    "\n",
    "        if opt.event_enc:\n",
    "            ev = out['event_type_list'][i_b][i]\n",
    "            t = out['event_time_list'][i_b][i]\n",
    "            \n",
    "            ev_pred = out['next_event_type_list'][i_b][i]\n",
    "\n",
    "\n",
    "            m = (ev.sum(1)>0).sum() # False are masked\n",
    "            ev=ev[:m]\n",
    "            ev_pred=ev_pred[:m-1] # for 2 to L-1\n",
    "            ev_pred = nn.functional.pad(ev_pred,(0,0,1,0))\n",
    "            t=t[:m]\n",
    "\n",
    "\n",
    "            print(f'm={m}')\n",
    "\n",
    "            print(f'ev.shape={ev.shape}')\n",
    "            print(f'ev_pred.shape={ev_pred.shape}')\n",
    "\n",
    "                                # plot attention of last event\n",
    "            tee_att = out['list_TE_att'][i_b][i,:,m-1,:m] # [h,L]\n",
    "            print(f'tee_att.shape={tee_att.shape}')\n",
    "            # print(tee_att)\n",
    "            \n",
    "\n",
    "                                \n",
    "            # att_names = [f'h{i}' for i in range(tee_att.shape[0])]\n",
    "            # rgb_params_att['range']=tee_att.max()\n",
    "            # binary_matrix = att_event2mat(ev, t,P, tee_att)\n",
    "\n",
    "            # img_att_ev = binary_matrix_to_image(binary_matrix, row_labels=att_names, font_path=None, grid_size=10, border_size=1, label_size=label_size,is_fill=False,**rgb_params_att)\n",
    "            \n",
    "            # merged_img2 = Image.new('RGBA', (merged_img.size[0],merged_img.size[1]+img_att_ev.size[1]))\n",
    "            # merged_img2.paste(merged_img, (0,0))\n",
    "            # merged_img2.paste(img_att_ev, (0,merged_img.size[1]))\n",
    "            # merged_img = merged_img2\n",
    "\n",
    "\n",
    "\n",
    "                                # plot attention matrix\n",
    "            m = (ev.sum(1)>0).sum() # False are masked\n",
    "            tee_att_mat = out['list_TE_att'][i_b][i,:,:m,:m].mean(0) # [L,L]\n",
    "            \n",
    "            print(f'tee_att_mat.shape={tee_att_mat.shape}')\n",
    "            \n",
    "            # print(tee_att_mat.sum(1))\n",
    "            scaler=torch.arange(1,m+1)[:,None]\n",
    "            scaler= (1/tee_att_mat.max(1)[0])[:,None]\n",
    "            tee_att_mat *= scaler\n",
    "            rgb_params_att['range']=tee_att_mat.max()\n",
    "            # print(tee_att_mat.max(1))\n",
    "\n",
    "            # print(tee_att_mat[1])\n",
    "\n",
    "            tee_att_mat_spaced = att_event2mat(ev, t,P, tee_att_mat)\n",
    "            print(f'tee_att_mat_spaced.shape={tee_att_mat_spaced.shape}')\n",
    "\n",
    "            binary_matrix = tee_att_mat_spaced\n",
    "            img_val = binary_matrix_to_image(binary_matrix, row_labels=None, font_path=None, grid_size=10, border_size=1, label_size=label_size,is_fill=True,**rgb_params_att)\n",
    "\n",
    "            att_mat_img = Image.new('RGBA', img_val.size)\n",
    "            att_mat_img = Image.alpha_composite(att_mat_img, img_val)\n",
    "\n",
    "\n",
    "            # t=torch.arange(m)\n",
    "            # PP = t.int().max().item() + 1\n",
    "\n",
    "            print(t[:m])\n",
    "            ev_matrix = event2mat(ev,t,P)\n",
    "            print(f'ev_matrix.shape={ev_matrix.shape}')\n",
    "            print(P)\n",
    "            img_ev = binary_matrix_to_image(ev_matrix, row_labels=event_labels, font_path=None, grid_size=10, border_size=1, label_size=label_size,**rgb_params_events)\n",
    "\n",
    "\n",
    "            ev_pred_matrix = event2mat(ev_pred,t,P)\n",
    "            print(f'ev_pred_matrix.shape={ev_pred_matrix.shape}')\n",
    "\n",
    "            img_ev_pred = binary_matrix_to_image(ev_pred_matrix, row_labels=None, font_path=None, grid_size=10, border_size=1,is_fill=False, label_size=label_size,**rgb_params_events_pred)\n",
    "            \n",
    "            merged_img3 = Image.new('RGBA', (att_mat_img.size[0],att_mat_img.size[1]+img_ev.size[1]))\n",
    "            merged_img3.paste(att_mat_img, (0,0))\n",
    "            merged_img3.paste(img_ev, (0,att_mat_img.size[1]))\n",
    "            merged_img3.paste(img_ev_pred, (0,att_mat_img.size[1]))\n",
    "\n",
    "            temp_img = Image.new('RGBA', img_ev.size)\n",
    "            temp_img = Image.alpha_composite(temp_img, img_ev)\n",
    "            temp_img = Image.alpha_composite(temp_img, img_ev_pred)#.convert(\"RGB\")\n",
    "\n",
    "            merged_img3.paste(temp_img, (0,att_mat_img.size[1]))\n",
    "\n",
    "            att_mat_img = merged_img3\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        \n",
    "        ev = out['event_type_list'][i_b][i]\n",
    "        t = out['event_time_list'][i_b][i]\n",
    "        P = t.int().max().item() + 1\n",
    "        m = (ev.sum(1)>0).sum() # False are masked\n",
    "\n",
    "        tee_att = out['list_TE_att'][i_b][i,:,m-2,:m] # [h,L]\n",
    "        att_names = [f'h{i}' for i in range(tee_att.shape[0])]\n",
    "        rgb_params_att['range']=tee_att.max()\n",
    "\n",
    "        binary_matrix = event2mat(ev,t,P)\n",
    "        img_ev = binary_matrix_to_image(binary_matrix, row_labels=event_labels, font_path=None, grid_size=10, border_size=1, label_size=label_size,**rgb_params_events)\n",
    "\n",
    "        binary_matrix = att_event2mat(ev, t,P, tee_att)\n",
    "        img_att_ev = binary_matrix_to_image(binary_matrix, row_labels=None, font_path=None, grid_size=10, border_size=1, label_size=label_size,is_fill=False,**rgb_params_att)\n",
    "            \n",
    "        # print(binary_matrix.sum())\n",
    "        merged_img = Image.new('RGB', (img_ev.size[0],img_ev.size[1]+img_att_ev.size[1]))\n",
    "        merged_img.paste(img_ev, (0,0))\n",
    "        merged_img.paste(img_att_ev, (0,img_ev.size[1]))\n",
    "\n",
    "\n",
    "\n",
    "        # plot attention matrix\n",
    "        m = (ev.sum(1)>0).sum() # False are masked\n",
    "        tee_att_mat = out['list_TE_att'][i_b][i,:,:m,:m].mean(0) # [L,L]\n",
    "\n",
    "        binary_matrix = tee_att_mat\n",
    "        img_val = binary_matrix_to_image(binary_matrix, row_labels=None, font_path=None, grid_size=10, border_size=1, label_size=label_size,is_fill=True,**rgb_params_att)\n",
    "        \n",
    "        att_mat_img = Image.new('RGBA', img_val.size)\n",
    "        att_mat_img = Image.alpha_composite(att_mat_img, img_val)\n",
    "\n",
    "\n",
    "    return merged_img,(att_mat_img,img_ev)#(img_att.convert(\"RGB\"), img_val.convert(\"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM:13\n",
      "NUM:500\n",
      "NUM:296\n",
      "NUM:13\n",
      "hhhhhhhhhhhh 13 0 13\n",
      "x              float32\n",
      "y              float32\n",
      "color            int64\n",
      "id               int64\n",
      "i_b              int64\n",
      "i                int64\n",
      "color_t_max    float32\n",
      "dtype: object 768\n"
     ]
    }
   ],
   "source": [
    "def bar_summary(df,out, point_ids,res_labels, fig=None):\n",
    "\n",
    "    if fig is None:\n",
    "        fig = go.Figure()\n",
    "\n",
    "\n",
    "\n",
    "    list_summary = []\n",
    "    for pid in point_ids:\n",
    "\n",
    "        i_b = int(df.iloc[pid]['i_b'])\n",
    "        i = int(df.iloc[pid]['i'])\n",
    "        print('hhhhhhhhhhhh',pid,i_b,i)\n",
    "        print(df.dtypes,len(df))\n",
    "        ev = out['event_type_list'][i_b][i]\n",
    "        t = out['event_time_list'][i_b][i]\n",
    "        \n",
    "\n",
    "        P = t.int().max().item() + 1\n",
    "\n",
    "        M = event2mat(ev,t,P)\n",
    "\n",
    "\n",
    "        vector = M.sum(1)/M.shape[1]*24\n",
    "        list_summary.append(vector)\n",
    "\n",
    "    vec_mean = np.mean(list_summary,axis=0)\n",
    "    vec_std = np.std(list_summary,axis=0)\n",
    "\n",
    "    sum(vec_std)\n",
    "\n",
    "    \n",
    "    _ = fig.add_trace(go.Bar(\n",
    "        name=f'Summary',\n",
    "        x=list(res_labels), y=vec_mean,\n",
    "        error_y=dict(type='data', array=vec_std)\n",
    "    ))\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_wandb(run_path):\n",
    "\n",
    "    run = api.run(run_path)\n",
    "    for file in run.files():\n",
    "        file.download(replace=True,root=f'./local/{run_path}/')\n",
    "\n",
    "    opt = pickle.load(open(f'./local/{run_path}/opt.pkl','rb'))\n",
    "    opt = Main.config(opt, justLoad=True)\n",
    "    opt.diag_offset=1\n",
    "    print('ATTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT')\n",
    "    checkpoint = torch.load(f'./local/{run_path}/best_model.pkl')\n",
    "\n",
    "    model = Main.ATHP(\n",
    "       n_marks=opt.num_marks,\n",
    "        TE_config = opt.TE_config,\n",
    "        DAM_config = opt.DAM_config,\n",
    "        NOISE_config = opt.NOISE_config,\n",
    "\n",
    "        CIF_config = opt.CIF_config,\n",
    "        next_time_config = opt.next_time_config,\n",
    "        next_type_config = opt.next_type_config,\n",
    "        label_config = opt.label_config,\n",
    "\n",
    "        demo_config = opt.demo_config,\n",
    "\n",
    "        device=opt.device,\n",
    "        diag_offset=opt.diag_offset\n",
    "    )\n",
    "\n",
    "    _ = model.to(opt.device)\n",
    "    _ = model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    _ = model.eval()\n",
    "\n",
    "    return model, opt, run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_knn_pids (X, id_origin, n_knn=10):\n",
    "\n",
    "    # X [N,d] np.array\n",
    "    r = np.sqrt(   ((X-X[id_origin])**2).sum(-1)   )\n",
    "    knn_pids = list(np.argpartition(r,n_knn)[:n_knn])\n",
    "\n",
    "    if id_origin not in knn_pids:\n",
    "        print('bad')\n",
    "    else:\n",
    "        knn_pids.remove(id_origin)\n",
    "\n",
    "    # print(X.shape)\n",
    "    # term\n",
    "\n",
    "    \n",
    "    # x0 = df.iloc[id_origin]['x']\n",
    "    # y0 = df.iloc[id_origin]['y']\n",
    "\n",
    "    # df['r'] = df.apply(lambda row: ( (row['x']-x0)**2 + (row['y']-y0)**2  ), axis=1)\n",
    "\n",
    "    # knn_pids = list(df.nsmallest(n_knn,'r').index)\n",
    "\n",
    "\n",
    "    return knn_pids\n",
    "\n",
    "def cal_similarity(df, out, pid, knn_pids):\n",
    "\n",
    "    list_summary = []\n",
    "    for pid in knn_pids:\n",
    "        \n",
    "\n",
    "        i_b = df.iloc[pid]['i_b']\n",
    "        i = df.iloc[pid]['i']\n",
    "\n",
    "        ev = out['event_type_list'][i_b][i]\n",
    "        t = out['event_time_list'][i_b][i]\n",
    "        st=out['state_time_list'][i_b][i]\n",
    "        \n",
    "        P = st.int().max().item() + 1\n",
    "\n",
    "        M = event2mat(ev,t,P)\n",
    "\n",
    "        \n",
    "\n",
    "        vector = M.sum(1)/M.shape[1]*24\n",
    "\n",
    "        \n",
    "        list_summary.append(vector)\n",
    "        \n",
    "\n",
    "    dotp = [ np.dot(j,list_summary[0])/(np.linalg.norm(j) * np.linalg.norm(list_summary[0])) for  j in list_summary]\n",
    "    sim_score = np.mean( dotp )\n",
    "\n",
    "\n",
    "    return sim_score, list_summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cif(out,i_b,i, fig=None):\n",
    "\n",
    "    ev = out['event_type_list'][i_b][i]\n",
    "    t = out['event_time_list'][i_b][i]\n",
    "\n",
    "\n",
    "    tee_att = out['list_TE_att'][i_b][i,:,-4,:] # [h,L]\n",
    "    cifs = out['list_intens_at_samples'][i_b][i]\n",
    "    taus = out['list_taus'][i_b][i]\n",
    "\n",
    "    y_cifs = out['list_true_intens_at_evs'][i_b][i] # [L-1] 2 to L\n",
    "\n",
    "    P = t.int().max().item() + 1\n",
    "\n",
    "    m = ev.sum(1)>0 # False are masked\n",
    "    m = m.sum().item()\n",
    "\n",
    "    \n",
    "    taus = taus[:m-1,:,:] + t[:m-1,None,None]\n",
    "    cifs = cifs[:m-1,:,:]\n",
    "    y_cifs = y_cifs[:m-1,:]  # 1 to L-1\n",
    "\n",
    "\n",
    "    n_cifs=cifs.shape[1]\n",
    "    # print(n_cifs,taus.shape, cifs.shape)\n",
    "\n",
    "    cifs = cifs.reshape(n_cifs,-1)[0] # [0] is the first cif\n",
    "    taus = taus.reshape(1,-1)[0]\n",
    "    y_cifs = y_cifs.reshape(n_cifs,-1)[0]\n",
    "\n",
    "    print(taus)\n",
    "    print(cifs)\n",
    "    # print(n_cifs,taus.shape, cifs.shape)\n",
    "    temp = torch.argsort(taus)\n",
    "\n",
    "    taus = taus[temp]\n",
    "    cifs = cifs[temp]\n",
    "\n",
    "\n",
    "    if fig==None:\n",
    "        fig=go.Figure()\n",
    "\n",
    "    _ = fig.add_trace(go.Scatter(x=taus,y=cifs))\n",
    "\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=y_cifs*0+1,mode='markers'))\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=t[1:m],mode='markers'))\n",
    "\n",
    "    # print(m)\n",
    "    # print(t[1:5])\n",
    "    # print(y_cifs[:4])\n",
    "\n",
    "    _ = fig.add_trace(go.Scatter(x=t[:m],y=t[:m]*0,mode='markers'))\n",
    "\n",
    "    _ = fig.add_trace(go.Scatter(x=t[1:m],y=y_cifs,mode='markers'))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cif_cum(out,i_b,i, fig=None):\n",
    "\n",
    "    ev = out['event_type_list'][i_b][i]\n",
    "    t = out['event_time_list'][i_b][i]\n",
    "\n",
    "\n",
    "    tee_att = out['list_TE_att'][i_b][i,:,-4,:] # [h,L]\n",
    "    cifs = out['list_intens_at_samples'][i_b][i]\n",
    "    taus = out['list_taus'][i_b][i]\n",
    "\n",
    "    y_cifs = out['list_true_intens_at_evs'][i_b][i] # [L-1] 2 to L\n",
    "\n",
    "    P = t.int().max().item() + 1\n",
    "\n",
    "    m = ev.sum(1)>0 # False are masked\n",
    "    m = m.sum().item()\n",
    "\n",
    "    taus = taus[:m-1,:,:] + t[:m-1,None,None]\n",
    "    cifs = cifs[:m-1,:,:]\n",
    "    y_cifs = y_cifs[:m-1,:]  # 1 to L-1\n",
    "\n",
    "\n",
    "    n_cifs=cifs.shape[1]\n",
    "\n",
    "    cifs = cifs.reshape(n_cifs,-1)[0]\n",
    "    taus = taus.reshape(n_cifs,-1)[0]\n",
    "    y_cifs = y_cifs.reshape(n_cifs,-1)[0]\n",
    "\n",
    "    temp = torch.argsort(taus)\n",
    "\n",
    "    taus = taus[temp]\n",
    "    cifs = cifs[temp]\n",
    "\n",
    "    cifs_int = integrate.cumulative_trapezoid(cifs, taus, initial=0)/(taus)\n",
    "\n",
    "    if fig==None:\n",
    "        fig=go.Figure()\n",
    "\n",
    "    _ = fig.add_trace(go.Scatter(x=taus,y=cifs_int))\n",
    "\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=y_cifs*0+1,mode='markers'))\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=t[1:m],mode='markers'))\n",
    "\n",
    "    # print(m)\n",
    "    # print(t[1:5])\n",
    "    # print(y_cifs[:4])\n",
    "\n",
    "    _ = fig.add_trace(go.Scatter(x=t[:m],y=t[:m]*0,mode='markers'))\n",
    "\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=y_cifs,mode='markers'))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cif_cum(out,i_b,i, fig=None):\n",
    "\n",
    "    ev = out['event_type_list'][i_b][i]\n",
    "    t = out['event_time_list'][i_b][i]\n",
    "\n",
    "\n",
    "    tee_att = out['list_TE_att'][i_b][i,:,-4,:] # [h,L]\n",
    "    cifs = out['list_intens_at_samples'][i_b][i]\n",
    "    taus = out['list_taus'][i_b][i]\n",
    "\n",
    "    y_cifs = out['list_true_intens_at_evs'][i_b][i] # [L-1] 2 to L\n",
    "\n",
    "    P = t.int().max().item() + 1\n",
    "\n",
    "    m = ev.sum(1)>0 # False are masked\n",
    "    m = m.sum().item()\n",
    "\n",
    "    taus = taus[:m-1,:,:] + t[:m-1,None,None]\n",
    "    cifs = cifs[:m-1,:,:]\n",
    "    y_cifs = y_cifs[:m-1,:]  # 1 to L-1\n",
    "\n",
    "\n",
    "    n_cifs=cifs.shape[1]\n",
    "\n",
    "    cifs = cifs.reshape(n_cifs,-1)[0]\n",
    "    taus = taus.reshape(n_cifs,-1)[0]\n",
    "    y_cifs = y_cifs.reshape(n_cifs,-1)[0]\n",
    "\n",
    "    temp = torch.argsort(taus)\n",
    "\n",
    "    taus = taus[temp]\n",
    "    cifs = cifs[temp]\n",
    "\n",
    "    cifs_int = integrate.cumulative_trapezoid(cifs, taus, initial=0)/(taus)\n",
    "\n",
    "    if fig==None:\n",
    "        fig=go.Figure()\n",
    "\n",
    "    _ = fig.add_trace(go.Scatter(x=taus,y=cifs_int))\n",
    "\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=y_cifs*0+1,mode='markers'))\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=t[1:m],mode='markers'))\n",
    "\n",
    "    # print(m)\n",
    "    # print(t[1:5])\n",
    "    # print(y_cifs[:4])\n",
    "\n",
    "    _ = fig.add_trace(go.Scatter(x=t[:m],y=t[:m]*0,mode='markers'))\n",
    "\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=y_cifs,mode='markers'))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return taus,cifs_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.arange(6).reshape(2,3)\n",
    "a\n",
    "\n",
    "np.roll(a,-1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## att agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(row1, row2):\n",
    "    intersection = np.logical_and(row1, row2).sum()\n",
    "    union = np.logical_or(row1, row2).sum()\n",
    "    return intersection / union\n",
    "\n",
    "def compute_jaccard_similarity(A,B):\n",
    "\n",
    "    L, M = A.shape\n",
    "    M, C = B.shape\n",
    "    res = np.zeros((L, C))\n",
    "    for i in range(L):\n",
    "        for j in range(C):\n",
    "            res[i, j] = jaccard_similarity(A[i], B[:, j])\n",
    "    return res\n",
    "\n",
    "\n",
    "def att_map(out,i_b,i,common_patterns):\n",
    "    tee_mapped = np.zeros((common_patterns.shape[0], common_patterns.shape[0]))\n",
    "    norm = np.zeros((common_patterns.shape[0], common_patterns.shape[0]))\n",
    "\n",
    "    N_CLUSTERS = common_patterns.shape[0]\n",
    "    ev = out['event_type_list'][i_b][i].cpu().numpy()\n",
    "\n",
    "    m=(ev.sum(1)>0).sum()\n",
    "    m\n",
    "    if m==0:\n",
    "        # print(out['event_time_list'][i_b][i])\n",
    "        print(\"NO EVENTS!\")\n",
    "        return tee_mapped, norm\n",
    "    ev=ev[:m]\n",
    "    ev.shape\n",
    "\n",
    "    tee = out['list_TE_att'][i_b][i][:1,:m,:m].mean(0).cpu().numpy() # tee(i,j): influence of j-th event on (i+1)-th event\n",
    "    \n",
    "    tee[-1,:] = 0\n",
    "    \n",
    "    scaler=np.arange(1,m+1)[:,None]\n",
    "    # scaler= (1/tee_att_mat.max(1)[0])[:,None]\n",
    "    tee *= scaler\n",
    "    \n",
    "    tee.shape\n",
    "\n",
    "    # print(tee.sum(1))\n",
    "\n",
    "\n",
    "\n",
    "    # print(common_patterns.shape)\n",
    "    # print(ev.sum(1).shape)\n",
    "    # print(common_patterns.sum(0).shape)\n",
    "\n",
    "    # temp_intersect = ev @ common_patterns.T   # [L,n_Cluster]\n",
    "    # temp_union= np.outer(ev.sum(1), common_patterns.sum(1))-temp_intersect # [L,1][1,N_cluster]\n",
    "    # print(temp_union.shape, temp_intersect.shape)\n",
    "    # print(temp_union[0])\n",
    "    # print(temp_intersect[0])\n",
    "    # print(temp_intersect[0]/(temp_union[0]+1e-9))\n",
    "\n",
    "    temp = compute_jaccard_similarity(ev,common_patterns.T) # [L,M],[M,C]->[L,C]\n",
    "\n",
    "    # print(temp[0])\n",
    "    # print(ev[0])\n",
    "    # print(common_patterns)\n",
    "    # print(temp.shape)\n",
    "\n",
    "    # print(temp)\n",
    "\n",
    "    \n",
    "    temp=np.argmax(temp,axis=1)\n",
    "    temp.shape\n",
    "    temp\n",
    "\n",
    "    vec_map = temp\n",
    "\n",
    "    mat_map_1h = np.zeros((m,N_CLUSTERS))\n",
    "    mat_map_1h[np.arange(m),vec_map]=1\n",
    "    # print(mat_map_1h.shape)    # [L,N_clusters]\n",
    "\n",
    "    # IMPORTANT\n",
    "    mat_iplus1 = np.roll(mat_map_1h.T,-1)\n",
    "    mat_iplus1[:,-1]=0\n",
    "\n",
    "    # mat_iplus1 = mat_map_1h.T\n",
    "\n",
    "    tee_mapped = mat_iplus1 @ tee @ mat_map_1h  # tee_mapped(i,j): influence of j-th pattern on i-th pattern\n",
    "    tee_mapped.shape\n",
    "\n",
    "    tee_mapped\n",
    "\n",
    "\n",
    "    \n",
    "    sum1=mat_iplus1.sum(1)  # shape [N_CLUSTER]\n",
    "    sum2=mat_map_1h.sum(0)  # shape [N_CLUSTER]\n",
    "    norm = np.outer(sum1,sum2)\n",
    "\n",
    "    # print(norm.shape, norm)\n",
    "\n",
    "\n",
    "    return tee_mapped, norm\n",
    "\n",
    "def plot_heatmap_att_agg(df,out,point_ids,fig=None):\n",
    "\n",
    "    if fig is None:\n",
    "        fig=go.Figure()\n",
    "\n",
    "    att_maps=[]\n",
    "    att_norms=[]\n",
    "    for pid in point_ids:\n",
    "\n",
    "        i_b = int(df.iloc[pid]['i_b'])\n",
    "        i = int(df.iloc[pid]['i'])\n",
    "\n",
    "        tee_mapped, norm = att_map(out,i_b,i,common_patterns)\n",
    "        att_maps.append(tee_mapped  )\n",
    "        att_norms.append(norm  )\n",
    "\n",
    "    norms_matrix = sum(att_norms)\n",
    "    norms_matrix[norms_matrix==0]=1\n",
    "    agg_matrix = sum(att_maps)/norms_matrix\n",
    "    # agg_matrix[agg_matrix<0.8]=0\n",
    "\n",
    "    patt_str = [''.join(str(cell) for cell in row) for row in common_patterns]\n",
    "\n",
    "    fig = px.imshow(agg_matrix, x=patt_str, y=patt_str)\n",
    "\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_path=[\n",
    "# \"hokarami/TEEDAM_supervised/5uzxzoxd\",\n",
    "\"hokarami/TEEDAM_supervised/sb9zf1mm\",\n",
    "\"hokarami/TEEDAM_supervised/hue1qxw4\",  # seft [Q10-TE__nextmark-concat]1566371 with label\n",
    "\n",
    "# \"hokarami/TEEDAM_supervised/t43m0t0u\", # [Q10-DA__base-concat]1563864\n",
    "\n",
    "\"hokarami/TEEDAM_supervised/9gxq3dgy\", # [Q10-TEDA__nextmark-concat]1577576 with label\n",
    "\n",
    "\"hokarami/TEEDAM_supervised/56ecgyrq\", # [Q20-TEDA__nextmark-concat]1588433\n",
    "# \"hokarami/TEEDAM_supervised/g4x0ibmk\", # [Q20-TE__nextmark-concat]1585936 with label\n",
    "\n",
    "# \"hokarami/TEEDAM_supervised/lzvf7erg\", # [Q20-DA__base-concat]1586915\n",
    "\n",
    "]\n",
    "\n",
    "run_path = runs_path[-1]\n",
    "run_path\n",
    "\n",
    "\n",
    "# run = api.run(run_path)\n",
    "# for file in run.files():\n",
    "#     file.download(replace=True,root='./local/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.keys()\n",
    "# bs = opt.batch_size\n",
    "\n",
    "\n",
    "# out['event_type_list'][0].shape\n",
    "# out['event_time_list'][0].shape\n",
    "# out['non_pad_mask_list'][0].shape\n",
    "# out['list_TE_att'][0].shape\n",
    "\n",
    "# out['state_mod_list'][0].shape\n",
    "# out['list_DAM_att'][0].shape\n",
    "\n",
    "# out['y_state_true'].shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_matrix = state2mat(st, sm,n_state_mods)\n",
    "\n",
    "# # binary_matrix_to_image(binary_matrix, row_labels=state_labels, font_path=None, grid_size=10, border_size=1, label_size=8)\n",
    "\n",
    "\n",
    "# M = [state2mat(st, sm,n_state_mods) +  att_state2mat(st, sm, n_state_mods, dam_att.sum(1))*0,\n",
    "     \n",
    "#      att_event2mat(ev, t,P, tee_att)*12\n",
    "     \n",
    "#      ]\n",
    "\n",
    "# all_labels = [*state_labels,\n",
    "#               *[f\"Head_{i}\" for i in range(tee_att.shape[0])]\n",
    "#               ]\n",
    "# binary_matrix = np.concatenate(M, axis=0)\n",
    "\n",
    "# # binary_matrix_to_image(binary_matrix, row_labels=all_labels, font_path=None, grid_size=10, border_size=1, label_size=8)\n",
    "\n",
    "\n",
    "# # binary_matrix = event2mat(ev,t,P)\n",
    "# # event_labels = opt.dict_map_events.keys()\n",
    "\n",
    "# # binary_matrix_to_image(binary_matrix, row_labels=None, font_path=None, grid_size=10, border_size=1, label_size=8)\n",
    "\n",
    "\n",
    "# binary_matrix = att_state2mat(st, sm, n_state_mods, dam_att.sum(1))\n",
    "# img_att = binary_matrix_to_image(binary_matrix, row_labels=state_labels, font_path=None, grid_size=10, border_size=1, label_size=8,is_fill=False,**rgb_params_att)\n",
    "\n",
    "\n",
    "# binary_matrix = att_state2mat(st, sm, n_state_mods, sv)\n",
    "# img_val = binary_matrix_to_image(binary_matrix, row_labels=state_labels, font_path=None, grid_size=10, border_size=1, label_size=8,is_fill=True,**rgb_params_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # binary_matrix[binary_matrix!=0].shape\n",
    "# # binary_matrix.shape\n",
    "# # binary_matrix.min()\n",
    "# # binary_matrix.max()\n",
    "\n",
    "# i_b=3\n",
    "# i=18\n",
    "\n",
    "# merged_img,temp = cool_image(out,i_b,i,opt)\n",
    "\n",
    "# temp[2][0]\n",
    "# temp[2][1]\n",
    "# temp[0]\n",
    "# merged_img\n",
    "\n",
    "# map_to_rgb(-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'y_state_true' in out:\n",
    "\n",
    "#     y_state_pred = out['y_state_pred']\n",
    "#     y_state_true = out['y_state_true']\n",
    "#     y_state_score = out['y_state_score']\n",
    "# else:\n",
    "#     y_state_pred = None\n",
    "#     y_state_true = None\n",
    "#     y_state_score = None\n",
    "\n",
    "# y_pred = out['y_pred']\n",
    "# y_true = out['y_true']\n",
    "# y_score = out['y_score']\n",
    "\n",
    "\n",
    "# res=list()\n",
    "\n",
    "\n",
    "# if len(out['state_mod_list'])>0 and 0:\n",
    "\n",
    "#     res_labels = opt.dict_map_states.keys()\n",
    "\n",
    "#     n_cols = len(opt.dict_map_states.keys())\n",
    "\n",
    "#     state_mod_list = out['state_mod_list']\n",
    "#     state_time_list = out['state_time_list']\n",
    "#     len(state_mod_list)\n",
    "#     state_mod_list[0].shape\n",
    "\n",
    "#     for state_mod,state_time in zip(state_mod_list,state_time_list):\n",
    "#         xs = torch.unbind(state_time,0)\n",
    "#         ys = torch.unbind(state_mod,0)\n",
    "\n",
    "#         for x,y in zip(xs,ys):\n",
    "#             # y = y[y~=0]\n",
    "#             n_rows = x.int().max().item() + 1\n",
    "#             matrix = torch.zeros(n_rows, n_cols)\n",
    "#             matrix[x[y!=0].long(), y[y!=0]-1] = 1\n",
    "#             res.append(matrix.cpu().numpy().transpose()) # [n_marks * times]\n",
    "\n",
    "\n",
    "# else:\n",
    "#     non_pad_mask_list = out['non_pad_mask_list']\n",
    "#     event_type_list = out['event_type_list']\n",
    "#     event_time_list = out['event_time_list']\n",
    "\n",
    "#     event_type_list[0].shape\n",
    "#     non_pad_mask_list[0].shape\n",
    "\n",
    "#     num_marks = event_type_list[0].shape[-1]\n",
    "#     res_labels = opt.dict_map_events.keys()\n",
    "\n",
    "\n",
    "\n",
    "#     for event_type, event_time, non_pad_mask in zip(event_type_list,event_time_list,non_pad_mask_list):\n",
    "\n",
    "#         B = event_type.shape[0]\n",
    "#         for i in range(B):\n",
    "#             m = non_pad_mask[i]\n",
    "#             e=event_type[i,:m.sum().int(),:]\n",
    "#             t=event_time[i,:m.sum().int()]\n",
    "            \n",
    "#             indices = e.nonzero()\n",
    "#             indices[:,0] = t[indices[:,0]]\n",
    "\n",
    "#             M = torch.zeros((indices[:,0].max()+1, e.shape[-1]))\n",
    "#             M[indices[:,0],indices[:,1]]=1\n",
    "\n",
    "\n",
    "#             res.append( M.cpu().numpy().astype(np.uint8).transpose() )\n",
    "\n",
    "\n",
    "#         # temp = torch.unbind(event_type,0)\n",
    "#         # lens = non_pad_mask.sum(1).long()\n",
    "#         # term\n",
    "#         # for i,x in enumerate(temp):\n",
    "#         #     res.append( x[:lens[i],:].cpu().numpy().astype(np.uint8).transpose() )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# res[1].shape\n",
    "\n",
    "# len(res_labels)\n",
    "# res_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # for i,pattern in enumerate(res):\n",
    "# #     image = Image.fromarray(pattern)\n",
    "\n",
    "\n",
    "# #     # row_labels = opt.dict_map_states.keys()\n",
    "# #     # image = binary_matrix_to_image(pattern, row_labels=row_labels, grid_size=50, border_size=2, label_size=20)\n",
    "# #     # image.save(f'./local/images/img{i}.jpeg')\n",
    "# all_images = [f'./local/images/img{i}.jpeg' for i in range(len(res))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE of Learned Rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNE_LIMIT = 6000\n",
    "\n",
    "def compute_tsne(r_enc_list, model):\n",
    "    tsne = TSNE(n_components=2, perplexity=30, learning_rate=10,n_jobs=4)\n",
    "\n",
    "    # r_enc_list = out['r_enc_list']\n",
    "\n",
    "    X = np.concatenate(r_enc_list,axis=0)[:,:]\n",
    "    X_tsne = tsne.fit_transform(X[:TSNE_LIMIT,:])\n",
    "\n",
    "    X_tsne_split=dict()\n",
    "    if model.d_out_te>0:\n",
    "        X_te = np.concatenate(r_enc_list,axis=0)[:,:model.d_out_te]\n",
    "        X_te_tsne = tsne.fit_transform(X_te[:TSNE_LIMIT,:])\n",
    "        X_tsne_split['tee']=X_te_tsne\n",
    "\n",
    "    if model.d_out_dam>0:    \n",
    "        X_dam = np.concatenate(r_enc_list,axis=0)[:,model.d_out_te:]\n",
    "        X_dam_tsne = tsne.fit_transform(X_dam[:TSNE_LIMIT,:])\n",
    "        X_tsne_split['dam']=X_dam_tsne\n",
    "    return X_tsne, X_tsne_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_df(out,opt, X_tsne):\n",
    "    if 'y_state_true' in out:\n",
    "\n",
    "        y_state_pred = out['y_state_pred']\n",
    "        y_state_true = out['y_state_true']\n",
    "        y_state_score = out['y_state_score']\n",
    "    else:\n",
    "        y_state_pred = None\n",
    "        y_state_true = None\n",
    "        y_state_score = None\n",
    "\n",
    "    y_pred = out['y_pred']\n",
    "    y_true = out['y_true']\n",
    "    y_score = out['y_score']\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['x']=X_tsne[:,0]\n",
    "    df['y']=X_tsne[:,1]\n",
    "\n",
    "    # df['x']=X_te_tsne[:,0]\n",
    "    # df['y']=X_te_tsne[:,1]\n",
    "\n",
    "    # df['x']=X_dam_tsne[:,0]\n",
    "    # df['y']=X_dam_tsne[:,1]\n",
    "\n",
    "    df['color']=0\n",
    "    df['id']=np.arange(len(df))\n",
    "\n",
    "\n",
    "    if y_state_true is not None:\n",
    "\n",
    "        TP = (y_state_true[:TSNE_LIMIT]*y_state_pred[:TSNE_LIMIT])==1\n",
    "        FN = (y_state_true[:TSNE_LIMIT]-y_state_pred[:TSNE_LIMIT])==1\n",
    "        FP = (y_state_true[:TSNE_LIMIT]-y_state_pred[:TSNE_LIMIT])==-1\n",
    "\n",
    "        TN = (y_state_true[:TSNE_LIMIT]+y_state_pred[:TSNE_LIMIT])==0\n",
    "\n",
    "        FP_FN = (y_state_true[:TSNE_LIMIT]+y_state_pred[:TSNE_LIMIT])==1\n",
    "        TP_TN = (y_state_true[:TSNE_LIMIT]-y_state_pred[:TSNE_LIMIT])==0\n",
    "\n",
    "\n",
    "        # df.loc[TN, 'color']='True Negatives'\n",
    "        df.loc[TP, 'color']='True Positives'\n",
    "        df.loc[FN, 'color']='False Negatives'\n",
    "        df.loc[FP, 'color']='False Positives'\n",
    "        df.loc[TN, 'color']='True Negatives'\n",
    "\n",
    "        df.loc[TP_TN, 'color_true_pred']='True Predicted'\n",
    "        df.loc[FP_FN, 'color_true_pred']='False Predicted'\n",
    "\n",
    "\n",
    "        df.loc[y_state_true[:TSNE_LIMIT].astype(bool).flatten(), 'color_true']='Positive Samples'\n",
    "        df.loc[~y_state_true[:TSNE_LIMIT].astype(bool).flatten(), 'color_true']='Negative Samples'\n",
    "\n",
    "\n",
    "        df.loc[y_state_pred[:TSNE_LIMIT].astype(bool).flatten(), 'color_pred']='Positive Predicted'\n",
    "        df.loc[~y_state_pred[:TSNE_LIMIT].astype(bool).flatten(), 'color_pred']='Negative Predicted'\n",
    "\n",
    "        # df.loc[y_state_pred[:TSNE_LIMIT].astype(bool).flatten(), 'color_true_pred']='Positive Predicted'\n",
    "        # df.loc[~y_state_pred[:TSNE_LIMIT].astype(bool).flatten(), 'color_true_pred']='Negative Predicted'\n",
    "\n",
    "\n",
    "    df['i_b'] = df['id'].apply(lambda x:int(x / opt.batch_size) )\n",
    "    df['i'] = df['id'].apply(lambda x:x % opt.batch_size)\n",
    "\n",
    "    t_max = np.concatenate( [t.max(1)[0] for t in out['event_time_list']] )\n",
    "\n",
    "    df['color_t_max'] = t_max\n",
    "    if len(out['list_log_sum'])>0:\n",
    "        df['color_log_sum'] = np.concatenate(out['list_log_sum'],axis=0) / t_max\n",
    "        df['color_integral_'] = np.concatenate(out['list_integral_'],axis=0) / t_max\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    'Positive Samples': \"#E52B50\",\n",
    "    'Negative Samples': \"#3B7A57\",\n",
    "    'True Predicted': \"#E52B50\",\n",
    "    'False Predicted': \"#3B7A57\",\n",
    "\n",
    "    \n",
    "    0: \"#3B7A57\",\n",
    "    # 'Positive Predicted': \"#3DDC84\",\n",
    "    # 'Negative Predicted': \"#FFBF00\",\n",
    "\n",
    "    'Positive Predicted': \"#E52B50\",\n",
    "    'Negative Predicted': \"#3B7A57\",\n",
    "\n",
    "    5: \"#915C83\",\n",
    "    'True Positives': \"#008000\",\n",
    "    'False Negatives': \"#7FFFD4\",\n",
    "    'False Positives': \"#E9D66B\",\n",
    "    'True Negatives': \"#007FFF\",\n",
    "}\n",
    "\n",
    "def plot_tsne(df, title=\"\"):\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    if 'color_true' in df:\n",
    "        labels = df['color_true'].values\n",
    "        colors = [color_map[label] for i,label in enumerate(labels)]\n",
    "\n",
    "        _ = fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['x'],\n",
    "                y=df['y'],\n",
    "                # z=tsne[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    # size=2,\n",
    "                    color=colors,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        )\n",
    "    else:\n",
    "        labels = df['color'].values\n",
    "        colors = [color_map[label] for i,label in enumerate(labels)]\n",
    "\n",
    "        _ = fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['x'],\n",
    "                y=df['y'],\n",
    "                # z=tsne[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    # size=2,\n",
    "                    color=colors,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        )\n",
    "\n",
    "    if 'color_pred' in df:\n",
    "        labels = df['color_pred'].values\n",
    "        colors = [color_map[label] for i,label in enumerate(labels)]\n",
    "        _ = fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['x'],\n",
    "                y=df['y'],\n",
    "                # z=tsne[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    symbol='circle-open',\n",
    "                    # size=10,\n",
    "                    color=colors,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        )\n",
    "\n",
    "        \n",
    "    if 'color_integral_' in df:\n",
    "        colors = df['color_integral_'].values\n",
    "\n",
    "        _ = fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['x'],\n",
    "                y=df['y'],\n",
    "                # z=tsne[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    # size=8,\n",
    "                    color=colors,\n",
    "                    colorbar=dict(\n",
    "                        title=\"cif_integral\"\n",
    "                    ),\n",
    "                    colorscale=\"RdBu\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        )\n",
    "\n",
    "    if 'color_t_max' in df:\n",
    "        colors = df['color_t_max'].values\n",
    "\n",
    "        _ = fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['x'],\n",
    "                y=df['y'],\n",
    "                # z=tsne[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    # size=8,\n",
    "                    color=colors,\n",
    "                    colorbar=dict(\n",
    "                        title=\"t_max\"\n",
    "                    ),\n",
    "                    colorscale=\"RdBu\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        )\n",
    "\n",
    "    _=fig.update_layout(\n",
    "        # autosize=False,\n",
    "        title=title,\n",
    "        width=600,\n",
    "        height=600,\n",
    "        showlegend=True,\n",
    "\n",
    "    )\n",
    "    _=fig.update_traces(\n",
    "        hoverinfo=\"none\",\n",
    "        hovertemplate=None,\n",
    "    )\n",
    "    # _=fig.update_layout(\n",
    "    #     scene=dict(\n",
    "    #         xaxis=dict(range=[-10,10]),\n",
    "    #         yaxis=dict(range=[-10,10]),\n",
    "    #         # zaxis=dict(range=[-10,10]),   \n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # save to run\n",
    "    # fig.write_html(f\"local/{run.name}.html\")\n",
    "    # run.upload_file(f\"local/{run.name}.html\")\n",
    "\n",
    "\n",
    "    # tsne of TE_nextmark[label]\n",
    "        # https://storage.googleapis.com/wandb-production.appspot.com/hokarami/TEEDAM_supervised/hue1qxw4/local/temp.html?Expires=1676986614&GoogleAccessId=wandb-production%40appspot.gserviceaccount.com&Signature=xLOuNCu8qoWVwFRPCspoPKTFbpF5aOSJMAFICqsDRYxf2wfudxEiFFEIlXBIq9YyYZcPtpSKSzYb6nUr0lloLZ6GHg1w4AOBZyieBKBsuKmIDGqhRY4ojK0FPHZHD%2BeoKERgjD42F19vP8E5Qf%2BA7PlgB2E%2BWutwJxx1sc2TtCgjUdkEK%2BEwaTzfQBQ0hXGIWC9MeirU7hRSmn4%2BXzIRaRcUFjF0vOtxCHBjsEhldGSwaUFv21kt4qvr2hSeR5Ku5gS7webtvzVi6fw55Muq78%2FB90r3EqWqy9Sn68T%2FDKx%2F%2FzHVfge4TsxlIjRE%2F9HBCo7qZeNeeHnlHjVZXdvTKg%3D%3D\n",
    "\n",
    "    # tsne of DA[label]\n",
    "        # https://storage.googleapis.com/wandb-production.appspot.com/hokarami/TEEDAM_supervised/t43m0t0u/local/temp.html?Expires=1676987035&GoogleAccessId=wandb-production%40appspot.gserviceaccount.com&Signature=jX44t4Nblmtzd0CCZC2Gn0cw43iZZbZtuofN%2BrlmzgennSxNz36YE4BwYU%2Ft21iuFdSwmqPLnnI%2B2saLWT9kZsBHNo0e2Nsti0vhf7216iT1wsnnFukn%2B8CswPFeQUQBmSyvEO0IJRTE%2BLmjORdY5NFMdgeXchVwLlf8LBIJFoxAfyiaYLmCNcjWwSmbkvVf1DwOxL2sjneA8TtkgQiSCZiC3GgMJBfh8wg1bf6wS%2Bxa95APq%2BPo6J7%2Ffq0sqmqCiAp28E309PlM29C0C89y63dvzqkRhBhvdffx3O%2FFUAmK%2FjRF82ohd7JSgxMva9oWz%2By7aHESGKr0b49KrUxM2Q%3D%3D\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_dash_app(fig_tsne,out,opt, df,port=None):\n",
    "    if port is None:\n",
    "        port = np.random.randint(2000,5000)\n",
    "\n",
    "    app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "    app.layout = dbc.Container([\n",
    "        # className=\"container\",\n",
    "        # children=[\n",
    "\n",
    "            dbc.Row([\n",
    "                    html.Div(id=\"dummy2\"),\n",
    "                    html.Div(id=\"dummy\"),\n",
    "                ]),\n",
    "\n",
    "            dbc.Row([\n",
    "                \n",
    "                dbc.Col(\n",
    "                    dcc.Graph(id=\"graph-5\", figure=fig_tsne, clear_on_unhover=True,),\n",
    "\n",
    "                    \n",
    "                    width=6),  # first column with graph\n",
    "\n",
    "\n",
    "\n",
    "                dbc.Col([\n",
    "                    html.Img(id='image_merge',src='./local/images/C0-1296.jpeg',style={\"width\": \"500px\", 'display': 'block', 'margin': '0 auto'},),\n",
    "\n",
    "                    html.Img(id='image_att',src='./local/images/C0-1296.jpeg',style={\"width\": \"500px\", 'display': 'block', 'margin': '0 auto'},),\n",
    "\n",
    "                ], width=6),  # second column with image\n",
    "            ]),\n",
    "\n",
    "            # dbc.Row([\n",
    "            #     dcc.Graph(id=\"graph-cif\", figure=go.Figure(), clear_on_unhover=True,),\n",
    "\n",
    "        \n",
    "            # ]),\n",
    "\n",
    "            dbc.Row([\n",
    "                dcc.Graph(id=\"graph-cif\", figure=go.Figure(), clear_on_unhover=True,),\n",
    "\n",
    "        \n",
    "            ]),\n",
    "\n",
    "            dbc.Row([\n",
    "                \n",
    "                dbc.Col(\n",
    "                    dcc.Graph(id=\"graph-att-agg\", figure=go.Figure(), clear_on_unhover=True,),\n",
    "\n",
    "                    \n",
    "                    width=6),  # first column with graph\n",
    "\n",
    "                dbc.Col(\n",
    "                    dcc.Graph(id=\"graph-summary\", figure=go.Figure(), clear_on_unhover=True,),\n",
    "\n",
    "                    \n",
    "                    width=6),  # first column with graph\n",
    "\n",
    "                # dbc.Col(\n",
    "                #     html.Img(id='image',src='./local/images/C0-1296.jpeg',style={\"height\": \"400px\", 'display': 'block', 'margin': '0 auto'},),\n",
    "                \n",
    "                \n",
    "                # width=6),  # second column with image\n",
    "            ]),\n",
    "\n",
    "            dcc.Tooltip(id=\"graph-tooltip-5\", direction='bottom'),\n",
    "\n",
    "\n",
    "        \n",
    "        ])\n",
    "\n",
    "    @app.callback(\n",
    "        # Output(\"graph-tooltip-5\", \"show\"),\n",
    "        # Output(\"graph-tooltip-5\", \"bbox\"),\n",
    "        # Output(\"graph-tooltip-5\", \"children\"),\n",
    "        Output('image_merge', 'src'),\n",
    "        Output('image_att', 'src'),\n",
    "        Output(\"dummy2\", \"children\"),\n",
    "        Output(\"graph-cif\", \"figure\"),\n",
    "        Input(\"graph-5\", \"hoverData\"),\n",
    "    )\n",
    "    def display_hover(hoverData):\n",
    "        if hoverData is None:\n",
    "            # return False, no_update, no_update, no_update, no_update\n",
    "            return no_update,no_update, no_update, no_update\n",
    "\n",
    "        num = 111111\n",
    "        # demo only shows the first point, but other points may also be available\n",
    "        hover_data = hoverData[\"points\"][0]\n",
    "        bbox = hover_data[\"bbox\"]\n",
    "        num = hover_data[\"pointNumber\"]\n",
    "\n",
    "        print(f'NUM:{num}')\n",
    "\n",
    "        # im_matrix = res[num].astype(int)\n",
    "        # # im_url = np_image_to_base64(im_matrix)\n",
    "        # # im_url = binary_matrix_to_image(im_matrix)\n",
    "        # im_url = binary_matrix_to_image(im_matrix, row_labels=res_labels, grid_size=50, border_size=2, label_size=20)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # # bar plot\n",
    "        # vector = im_matrix.sum(1)/im_matrix.shape[1]*24\n",
    "        # # im_url = plot_bar_chart(vector, labels=res_labels)\n",
    "        # im_url = create_barplot_image(vector, res_labels, './local/images/temp.png')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        # NEW\n",
    "        i_b = int(df.iloc[num]['i_b'])\n",
    "        i = int(df.iloc[num]['i'])\n",
    "        im_url, temp = cool_image(out,i_b,i,opt)\n",
    "\n",
    "        \n",
    "        output_str = f\"{num} - {df.iloc[num]['color']} - i_b {i_b} - i {i}\"\n",
    "        \n",
    "        im_url.save(\"./local/images/hover_img.png\")\n",
    "        if temp[0] is not None:\n",
    "            im_url.save(\"./local/images/att.png\")\n",
    "\n",
    "        im_url_path = './local/images/C2-203.jpeg'\n",
    "        \n",
    "        \n",
    "        \n",
    "        children = [\n",
    "            html.Div([\n",
    "                html.Img(\n",
    "                    src=im_url,\n",
    "                    style={\"height\": \"400px\", 'display': 'block', 'margin': '0 auto'},\n",
    "                ),\n",
    "                # html.P(\"MNIST Digit \" + str(labels[num]), style={'font-weight': 'bold'})\n",
    "                html.P(f\"Patterns-id={num} - i_b {i_b} - i {i}\" , style={'font-weight': 'bold'})\n",
    "\n",
    "            ])\n",
    "            \n",
    "        ]\n",
    "\n",
    "        fig_cif = plot_cif(out,i_b,i)\n",
    "        # fig_cif = plot_cif_cum(out,i_b,i)\n",
    "\n",
    "\n",
    "        # return True, bbox, children,im_url, output_str\n",
    "        return im_url,temp[0], output_str,fig_cif        # temp[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Define a callback function to print the selected point IDs\n",
    "    @app.callback(Output(\"dummy\", \"children\"),Output(\"graph-summary\", \"figure\"), Output(\"graph-att-agg\", \"figure\"), [Input(\"graph-5\", \"selectedData\"), Input(\"graph-summary\", \"figure\")])\n",
    "    def display_selected_data(selected_data, fig_prev):\n",
    "        if selected_data is None:\n",
    "            return \"No points selected.\",no_update,no_update\n",
    "        else:\n",
    "\n",
    "\n",
    "            new_fig = go.Figure(data=fig_prev['data'],layout=fig_prev['layout'])\n",
    "            fig_att_agg = go.Figure(data=fig_prev['data'],layout=fig_prev['layout'])\n",
    "\n",
    "\n",
    "            point_ids = [point[\"pointIndex\"] for point in selected_data[\"points\"]]\n",
    "\n",
    "            new_fig = bar_summary(df,out, point_ids,res_labels, fig=new_fig)\n",
    "            \n",
    "            print(point_ids)\n",
    "            fig_att_agg = plot_heatmap_att_agg(df,out,point_ids)\n",
    "\n",
    "            # # save selected to local\n",
    "            # for pid in point_ids:\n",
    "            #     # NEW\n",
    "            #     i_b = df.iloc[pid]['i_b']\n",
    "            #     i = df.iloc[pid]['i']\n",
    "            #     im_url, temp = cool_image(out,i_b,i,opt)\n",
    "            #     im_url.save(f\"./local/images/selected{pid}.png\")\n",
    "\n",
    "            # print(f\"{point_ids},\\n\")\n",
    "            return f\"{point_ids},\\n\",new_fig,fig_att_agg\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # app.run_server(mode='inline', debug=True)\n",
    "        app.run_server(mode='external',port=port)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEDAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path = \"hokarami/TEEDAM_supervised/56ecgyrq\" # [Q20-TEDA__nextmark-concat]1588433\n",
    "run_path = \"hokarami/TEEDAM_supervised/3x52mwgi\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/3x52mwgi/overview?workspace=user-g-hojatkarami\n",
    "run_path = \"hokarami/TEEDAM_unsupervised/ibzmm7wm\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/ibzmm7wm/overview?workspace=\n",
    "# run_path = \"hokarami/TEEDAM_supervised/v1nb7bhq\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/v1nb7bhq/overview?workspace=user-g-hojatkarami\n",
    "run_path = \"hokarami/TEEDAM_unsupervised/jvpugrlq\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/jvpugrlq/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "\n",
    "run_path = \"hokarami/TEEDAM_unsupervised/a95wkhvj\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/a95wkhvj/overview?workspace=user-g-hojatkarami\n",
    "run_path = \"hokarami/TEEDAM_supervised/e2ty765j\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/e2ty765j/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "# h0s0\n",
    "run_path = \"hokarami/TEEDAM_supervised/noufk1a0\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/noufk1a0/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "# seft\n",
    "run_path = \"hokarami/TEEDAM_supervised/g0g8zo1n\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/g0g8zo1n/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "# unsup\n",
    "run_path = \"hokarami/TEEDAM_unsupervised/fyn5cxoi\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/fyn5cxoi/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "run_path = \"hokarami/TEEDAM_supervised/g0g8zo1n\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/g0g8zo1n/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# p19\n",
    "run_path = \"hokarami/TEEDAM_supervised/ah1p114x\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/ah1p114x?workspace=user-g-hojatkarami\n",
    "\n",
    "\n",
    "# p12 unsup TE only\n",
    "run_path = \"hokarami/TEEDAM_unsupervised/0k296zum\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/0k296zum/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "\n",
    "model1, opt1, run1 = read_from_wandb(run_path)\n",
    "dict_metrics1, out1 = Main.valid_epoch_tsne(model1, opt1.validloader, opt1.pred_loss_func, opt1)\n",
    "X_tsne1, X_tsne_split1 = compute_tsne(out1['r_enc_list'], model1)\n",
    "\n",
    "res_labels = opt1.dict_map_events.keys()\n",
    "\n",
    "df1 = build_df(out1,opt1, X_tsne1)\n",
    "fig_tsne1 = plot_tsne(df1,title=run1.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = build_df(out1,opt1, X_tsne1)\n",
    "fig_temp = plot_tsne(df_temp,title=run1.name)\n",
    "fig_temp\n",
    "\n",
    "\n",
    "df_temp = build_df(out1,opt1, X_tsne_split1['dam'])\n",
    "fig_temp = plot_tsne(df_temp,title=run1.name)\n",
    "fig_temp\n",
    "\n",
    "df_temp = build_df(out1,opt1, X_tsne_split1['tee'])\n",
    "fig_temp = plot_tsne(df_temp,title=run1.name)\n",
    "fig_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x              float32\n",
       "y              float32\n",
       "color            int64\n",
       "id               int64\n",
       "i_b              int64\n",
       "i                int64\n",
       "color_t_max    float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x                8.176332\n",
       "y               38.434216\n",
       "color            0.000000\n",
       "id             146.000000\n",
       "i_b              1.000000\n",
       "i               18.000000\n",
       "color_t_max     39.150002\n",
       "Name: 146, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhhhhhhhhhhh 146 1.0 18.0\n"
     ]
    }
   ],
   "source": [
    "df1.dtypes\n",
    "len(df1)\n",
    "df1.i.dtypes\n",
    "print('dfd')\n",
    "df1.iloc[146]\n",
    "df1.iloc[146].dtypes\n",
    "\n",
    "i_b = df1.iloc[146]['i_b']\n",
    "i = df1.iloc[146]['i']\n",
    "print('hhhhhhhhhhhh',146,i_b,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:4686/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM:733\n",
      "NUM:306\n",
      "NUM:16\n",
      "NUM:280\n",
      "NUM:375\n",
      "NUM:614\n",
      "NUM:443\n",
      "NUM:345\n",
      "NUM:345\n",
      "hhhhhhhhhhhh 90 0.0 90.0\n",
      "NUM:494\n",
      "NUM:443\n"
     ]
    }
   ],
   "source": [
    "df1 = build_df(out1,opt1, X_tsne1)\n",
    "fig_tsne1 = plot_tsne(df1,title=run1.name)\n",
    "\n",
    "run_dash_app(fig_tsne1,out1,opt1,df1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path =  \"hokarami/TEEDAM_supervised/lzvf7erg\" # [Q20-DA__base-concat]1586915\n",
    "\n",
    "\n",
    "\n",
    "run_path = \"hokarami/TEEDAM_unsupervised/su87zi5z\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/su87zi5z/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "# h0s0\n",
    "run_path = \"hokarami/TEEDAM_supervised/bxawmk71\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/bxawmk71/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "# seft\n",
    "run_path = \"hokarami/TEEDAM_supervised/qvdve9d5\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/qvdve9d5?workspace=user-g-hojatkarami\n",
    "\n",
    "# seft\n",
    "run_path = \"hokarami/TEEDAM_supervised/qvdve9d5\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/qvdve9d5?workspace=user-g-hojatkarami\n",
    "\n",
    "\n",
    "# so\n",
    "# run_path = \"hokarami/TEEDAM_unsupervised_timeCat/22xvwu9y\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/su87zi5z/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "\n",
    "# p19\n",
    "run_path =  \"hokarami/TEEDAM_supervised/yftm62nl\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/yftm62nl?workspace=user-g-hojatkarami\n",
    "\n",
    "\n",
    "\n",
    "model2, opt2, run2 = read_from_wandb(run_path)\n",
    "dict_metrics2, out2 = Main.valid_epoch_tsne(model2, opt2.validloader, opt2.pred_loss_func, opt2)\n",
    "X_tsne2, X_tsne_split2 = compute_tsne(out2['r_enc_list'], model2)\n",
    "\n",
    "# res_labels = opt2.dict_map_events.keys()\n",
    "\n",
    "df2 = build_df(out2,opt2, X_tsne2)\n",
    "fig_tsne2 = plot_tsne(df2,title=run2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = build_df(out2,opt2, X_tsne2)\n",
    "fig_temp = plot_tsne(df_temp,title=run2.name)\n",
    "fig_temp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "run_dash_app(fig_tsne2,out2,opt2,df2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid=90\n",
    "i_b = int(df1.iloc[pid]['i_b'])\n",
    "i = int(df1.iloc[pid]['i'])\n",
    "i_b,i\n",
    "_,temp = cool_image(out1,i_b,i,opt1)\n",
    "temp[0]\n",
    "\n",
    "\n",
    "fig_cif = plot_cif(out1,i_b,i)\n",
    "fig_cif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.keys()\n",
    "out1['next_event_type_list'][10][94].shape\n",
    "out1['event_type_list'][10][94].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(2)\n",
    "df2.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn-ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_knn=5\n",
    "pid_all = list(df1.index)\n",
    "\n",
    "pid_positive = list(df1[df1.color_true=='Positive Samples'].index)\n",
    "\n",
    "list_sim_score1 = []\n",
    "list_sim_score2 = []\n",
    "\n",
    "X1 = np.concatenate(out1['r_enc_list'],axis=0)[:,:model1.d_out_te]\n",
    "X2 = np.concatenate(out2['r_enc_list'],axis=0)[:,:]\n",
    "\n",
    "for pid in tqdm( pid_positive ):\n",
    "    id_origin = pid\n",
    "\n",
    "    knn_pids = find_knn_pids (X1, id_origin, n_knn=n_knn)\n",
    "    sim_score1, list_summary = cal_similarity(df1, out1, pid, knn_pids)    \n",
    "\n",
    "\n",
    "    knn_pids = find_knn_pids (X2, id_origin, n_knn=n_knn)\n",
    "    sim_score2, list_summary = cal_similarity(df2, out2, pid, knn_pids)    \n",
    "    \n",
    "    if ( not np.isnan(sim_score1) ) and ( not np.isnan(sim_score2) ):\n",
    "        list_sim_score1.append(sim_score1)\n",
    "        list_sim_score2.append(sim_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list_sim_score1), np.std(list_sim_score1)\n",
    "np.mean(list_sim_score2), np.std(list_sim_score2)\n",
    "\n",
    "diff = np.array(list_sim_score1) - np.array(list_sim_score2)\n",
    "diff.sum()/len(diff)\n",
    "diff.argsort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "group1 = [x-0.0 for x in list_sim_score1]\n",
    "group2 = list_sim_score2\n",
    "t_statistic, p_value = ttest_ind(group1, group2, alternative='greater')\n",
    "\n",
    "# Print the results\n",
    "print('t-statistic =', t_statistic)\n",
    "print('p-value =', p_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save selected to local\n",
    "n_knn=5\n",
    "id_origin = 90\n",
    "\n",
    "len_CROP = len(opt1.dict_map_states.keys()) - len(opt1.dict_map_events.keys())\n",
    "len_CROP\n",
    "\n",
    "X1 = np.concatenate(out1['r_enc_list'],axis=0)[:,:model1.d_out_te]\n",
    "X2 = np.concatenate(out2['r_enc_list'],axis=0)[:,:]\n",
    "\n",
    "\n",
    "            # plot and save id_origin\n",
    "i_b = df1.iloc[id_origin]['i_b']\n",
    "i = df1.iloc[id_origin]['i']\n",
    "im_url, temp = cool_image(out1,i_b,i,opt1)\n",
    "im_url.save(f\"./local/images/Origin_{id_origin}.png\")\n",
    "\n",
    "            # save event attentions\n",
    "_,temp = cool_image(out1,i_b,i,opt1)\n",
    "temp[0].save(f\"./local/images/Origin_{id_origin}_att.png\")\n",
    "\n",
    "            # save cif plot\n",
    "fig_cif = plot_cif(out1,i_b,i)\n",
    "fig_cif.write_image(f\"./local/images/Origin_{id_origin}_CIF.svg\")\n",
    "\n",
    "\n",
    "i_g=0\n",
    "for out,opt,df,X in zip([out1,out2],[opt1,opt2],[df1,df2],[X1,X2]):\n",
    "    knn_pids = find_knn_pids (X, id_origin, n_knn=n_knn)\n",
    "    knn_pids\n",
    "    for pid in knn_pids:\n",
    "        # NEW\n",
    "        i_b = df.iloc[pid]['i_b']\n",
    "        i = df.iloc[pid]['i']\n",
    "        im_url, temp = cool_image(out,i_b,i,opt,CROP=len_CROP)\n",
    "        im_url.save(f\"./local/images/C{i_g}_{pid}.png\")\n",
    "\n",
    "\n",
    "\n",
    "    i_g+=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cif diff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$log(\\lambda)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pid_positive = list(df1[df1.color_true=='Positive Samples'].index)\n",
    "pid_negative = list(df1[df1.color_true=='Negative Samples'].index)\n",
    "\n",
    "list_sim_score1 = []\n",
    "list_sim_score2 = []\n",
    "\n",
    "lens = np.concatenate([x.sum(1).cpu().detach() for x in out1['non_pad_mask_list']])\n",
    "X = np.concatenate(out1['list_log_sum'],axis=0) # [n_samples]\n",
    "Y = np.concatenate(out1['list_integral_'],axis=0)/lens # [n_samples]\n",
    "\n",
    "\n",
    "X[pid_positive].mean(), X[pid_positive].std()\n",
    "\n",
    "X[pid_negative].mean(), X[pid_negative].std()\n",
    "\n",
    "\n",
    "\n",
    "Y[pid_positive].mean(), Y[pid_positive].std()\n",
    "\n",
    "Y[pid_negative].mean(), Y[pid_negative].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "group1 = X[pid_positive]\n",
    "group2 = X[pid_negative]\n",
    "t_statistic, p_value = ttest_ind(group1, group2, alternative='greater')\n",
    "\n",
    "# Print the results\n",
    "print('t-statistic =', t_statistic)\n",
    "print('p-value =', p_value)\n",
    "\n",
    "\n",
    "group1 = Y[pid_positive]\n",
    "group2 = Y[pid_negative]\n",
    "t_statistic, p_value = ttest_ind(group1, group2, alternative='greater')\n",
    "\n",
    "# Print the results\n",
    "print('t-statistic =', t_statistic)\n",
    "print('p-value =', p_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clusters = [\n",
    "\n",
    "[1715, 1212, 957, 834, 1401, 1532, 33, 514, 559, 1493],\n",
    "[1715, 1542, 808, 957, 1541, 1296, 252, 978, 34, 1204]\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "df = df1\n",
    "out = out1\n",
    "\n",
    "fig2 = go.Figure()\n",
    "for i_c, cluster in enumerate(clusters):\n",
    "    list_summary = []\n",
    "    for pid in cluster:\n",
    "        \n",
    "\n",
    "        i_b = df.iloc[pid]['i_b']\n",
    "        i = df.iloc[pid]['i']\n",
    "\n",
    "        ev = out['event_type_list'][i_b][i]\n",
    "        t = out['event_time_list'][i_b][i]\n",
    "        st=out['state_time_list'][i_b][i]\n",
    "        \n",
    "        P = st.int().max().item() + 1\n",
    "\n",
    "        M = event2mat(ev,t,P)\n",
    "\n",
    "        \n",
    "\n",
    "        vector = M.sum(1)/M.shape[1]*24\n",
    "\n",
    "        \n",
    "        list_summary.append(vector)\n",
    "        \n",
    "\n",
    "    dotp = [ np.dot(j,list_summary[0])/(np.linalg.norm(j) * np.linalg.norm(list_summary[0])) for  j in list_summary]\n",
    "    np.mean( dotp )\n",
    "\n",
    "    vec_mean = np.mean(list_summary,axis=0)\n",
    "    vec_std = np.std(list_summary,axis=0)\n",
    "\n",
    "    # sum(vec_std)\n",
    "\n",
    "    \n",
    "    _ = fig2.add_trace(go.Bar(\n",
    "        name=f'Cluster {i_c}',\n",
    "        x=list(res_labels), y=vec_mean,\n",
    "        error_y=dict(type='data', array=vec_std)\n",
    "    ))\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_tsne1.write_image(\"local/images/fig1.svg\")\n",
    "fig_tsne2.write_image(\"local/images/fig2.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save selected to local\n",
    "n_knn=5\n",
    "id_origin = 420\n",
    "\n",
    "X1 = np.concatenate(out1['r_enc_list'],axis=0)[:,:model1.d_out_te]\n",
    "X2 = np.concatenate(out2['r_enc_list'],axis=0)[:,:]\n",
    "\n",
    "i_g=0\n",
    "for out,opt,df,X in zip([out1,out2],[opt1,opt2],[df1,df2],[X1,X2]):\n",
    "    \n",
    "    knn_pids = find_knn_pids (X, id_origin, n_knn=n_knn)\n",
    "    knn_pids\n",
    "    # for pid in knn_pids:\n",
    "    #     # NEW\n",
    "    #     i_b = df.iloc[pid]['i_b']\n",
    "    #     i = df.iloc[pid]['i']\n",
    "    #     im_url, temp = cool_image(out,i_b,i,opt,CROP=11)\n",
    "    #     # im_url.save(f\"./local/images/C{i_g}_{pid}.png\")\n",
    "    i_g+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fig =  go.Figure()\n",
    "# new_fig = go.Figure(data=fig_prev['data'],layout=fig_prev['layout'])\n",
    "\n",
    "\n",
    "# new_fig = bar_summary(df,out, point_ids,res_labels, fig=new_fig)\n",
    "\n",
    "n_knn=5\n",
    "id_origin = 583\n",
    "\n",
    "X1 = np.concatenate(out1['r_enc_list'],axis=0)[:,:model1.d_out_te]\n",
    "X2 = np.concatenate(out2['r_enc_list'],axis=0)[:,:]\n",
    "\n",
    "new_fig = bar_summary(df,out, [id_origin],res_labels, fig=new_fig)\n",
    "\n",
    "for out,opt,df,X in zip([out1,out2],[opt1,opt2],[df1,df2],[X1,X2]):\n",
    "    knn_pids = find_knn_pids (X, id_origin, n_knn=n_knn)\n",
    "    \n",
    "    new_fig = bar_summary(df,out, knn_pids,res_labels, fig=new_fig)\n",
    "\n",
    "new_fig\n",
    "new_fig.write_image(\"local/images/bar.svg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIF vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df1\n",
    "out=out1\n",
    "pid_positive = list(df1[df1.color_true=='Positive Samples'].index)\n",
    "pid_negative = list(df1[df1.color_true=='Negative Samples'].index)\n",
    "\n",
    "\n",
    "\n",
    "fig=go.Figure()\n",
    "\n",
    "for pid in pid_positive[:50]:\n",
    "\n",
    "    i_b = int(df.iloc[pid]['i_b'])\n",
    "    i = int(df.iloc[pid]['i'])\n",
    "    taus,cifs_int = cif_cum(out1,i_b,i, fig=None)\n",
    "    _ = fig.add_trace(go.Scatter(x=taus,y=cifs_int,marker=dict(\n",
    "                    # size=2,\n",
    "                    color='red',\n",
    "                )))\n",
    "    \n",
    "for pid in pid_negative[:50]:\n",
    "\n",
    "    i_b = int(df.iloc[pid]['i_b'])\n",
    "    i = int(df.iloc[pid]['i'])\n",
    "    taus,cifs_int = cif_cum(out1,i_b,i, fig=None)\n",
    "    _ = fig.add_trace(go.Scatter(x=taus,y=cifs_int,marker=dict(\n",
    "                    # size=2,\n",
    "                    color='blue',\n",
    "                )))\n",
    "\n",
    "\n",
    "_=fig.update_layout(\n",
    "        # autosize=False,\n",
    "        # title=title,\n",
    "        width=600,\n",
    "        height=600,\n",
    "        showlegend=False,\n",
    "\n",
    "    )\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1.dict_map_events.keys()\n",
    "\n",
    "\n",
    "for i,k in enumerate(opt1.dict_map_events.keys()):\n",
    "    print(f'{i}:{k}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=out1\n",
    "\n",
    "out.keys()\n",
    "\n",
    "\n",
    "all_events=np.concatenate(out['event_type_list'],axis=1) # [B,LLLLLL,K]\n",
    "all_events.shape\n",
    "masks=all_events.sum(-1)>0\n",
    "\n",
    "\n",
    "\n",
    "TSNE_LIMIT=16000\n",
    "X = all_events[masks][:TSNE_LIMIT]\n",
    "\n",
    "\n",
    "X_str = [''.join(str(cell) for cell in row) for row in X]\n",
    "\n",
    "X.shape\n",
    "len(X_str)\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['val']=list(X)\n",
    "df['str']=X_str\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=60, learning_rate=10,n_jobs=4)\n",
    "X_tsne = tsne.fit_transform(X[:TSNE_LIMIT,:])\n",
    "\n",
    "df['x']=X_tsne[:,0]\n",
    "df['y']=X_tsne[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['str'].unique())\n",
    "\n",
    "s = df['str'].value_counts()/df['str'].value_counts().sum()\n",
    "s[s>0.001].head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "N_CLUSTERS=10\n",
    "\n",
    "# Generate sample binary data\n",
    "data = X\n",
    "\n",
    "# Initialize KMeans object with 2 clusters\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS)\n",
    "\n",
    "# Fit the data to the KMeans object\n",
    "_ = kmeans.fit(data)\n",
    "\n",
    "# Get the labels and cluster centers\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "labels\n",
    "\n",
    "\n",
    "\n",
    "df['cluster_kmeans']=labels\n",
    "\n",
    "fig=px.scatter(df,x='x',y='y',color='cluster_kmeans',hover_data=[\"str\"])\n",
    "# fig\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initialize DBSCAN object\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "\n",
    "# Fit the data to the DBSCAN object\n",
    "dbscan.fit(data)\n",
    "\n",
    "# Get the labels\n",
    "labels = dbscan.labels_\n",
    "\n",
    "df['cluster_dbscan']=labels\n",
    "\n",
    "fig=px.scatter(df,x='x',y='y',color='cluster_dbscan',hover_data=[\"str\"])\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = 'cluster_kmeans'\n",
    "\n",
    "common_patterns = []\n",
    "\n",
    "for i in range(N_CLUSTERS):\n",
    "    temp = df[df[METHOD]==i]['val'].values\n",
    "    temp = (temp.sum()/temp.shape[0]*100).astype(int)\n",
    "\n",
    "    temp[temp<50]=0\n",
    "    temp[temp>=50]=1\n",
    "    print(temp,'\\n')\n",
    "    common_patterns.append(temp)\n",
    "\n",
    "common_patterns = np.array(common_patterns)\n",
    "common_patterns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca5b395ee1d8a1cd2783c3fccc5aaaf2f1d95e614c8b133e284cded792af89cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
