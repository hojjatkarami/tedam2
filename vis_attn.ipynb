{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate line execution\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "# plotly\n",
    "import plotly.express as px  # (version 4.7.0 or higher)\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import custom libraries\n",
    "import sys\n",
    "# sys.path.append(\"C:\\\\DATA\\\\Tasks\\\\lib\\\\hk\")\n",
    "# import hk_utils\n",
    "\n",
    "# folder paths\n",
    "ADD_DATA = \"C:\\\\DATA\\\\data\\\\raw\\\\mimic4\\\\lookup\\\\\"\n",
    "ADD_DATA_proc = \"C:/DATA/data/processed/\"\n",
    "\n",
    "\n",
    "PATH_PAPER = \"C:\\\\DATA\\\\Tasks\\\\220704\\\\Alternate-Transactions-Articles-LaTeX-template\\\\images\\\\\"\n",
    "\n",
    "\n",
    "PATH_SYS=\"/mlodata1/hokarami/tedam/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for THP\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import transformer.Constants as Constants\n",
    "import Utils\n",
    "\n",
    "# from preprocess.Dataset import get_dataloader, get_dataloader2\n",
    "# from transformer.Models import Transformer\n",
    "# from transformer.hk_transformer import Transformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_allocated()\n",
    "# torch.cuda.memory_reserved()\n",
    "\n",
    "from sklearn import metrics\n",
    "# from hk_pytorch import save_checkpoint,load_checkpoint\n",
    "# import hk_pytorch\n",
    "\n",
    "\n",
    "# from custom2 import myparser\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/paper2022/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import Main\n",
    "import webbrowser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo conda install -c conda-forge dash --name paper2022\n",
    "# sudo conda install -c conda-forge jupyter-dash --name paper2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsnecuda import TSNE\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb -qqq\n",
    "import wandb\n",
    "# wandb.login()\n",
    "api = wandb.Api()\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"0f780ac8a470afe6cb7fc474ff3794772c660465\"\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"jup_res\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Project is specified by <entity/project-name>\n",
    "def dl_runs(all_runs, selected_tag=None):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    summary_list, config_list, name_list, path_list = [], [], [], []\n",
    "    for run in all_runs: \n",
    "\n",
    "        if (selected_tag not in run.tags) and (selected_tag is not None):\n",
    "            continue\n",
    "        # .summary contains the output keys/values for metrics like accuracy.\n",
    "        #  We call ._json_dict to omit large files \n",
    "        summary_list.append(run.summary._json_dict)\n",
    "\n",
    "        # .config contains the hyperparameters.\n",
    "        #  We remove special values that start with _.\n",
    "        \n",
    "\n",
    "        config_list.append(\n",
    "            {k: v for k,v in run.config.items()\n",
    "            if not k.startswith('_')})\n",
    "\n",
    "        # .name is the human-readable name of the run.\n",
    "        name_list.append(run.name)\n",
    "        path_list.append(run.path)\n",
    "\n",
    "    runs_df = pd.DataFrame({\n",
    "        \"summary\": summary_list,\n",
    "        \"config\": config_list,\n",
    "        \"name\": name_list,\n",
    "        \"path\": path_list\n",
    "\n",
    "        })\n",
    "\n",
    "    # runs_df.to_csv(\"project.csv\")\n",
    "    return runs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import dcc, html, Input, Output, no_update\n",
    "import dash_bootstrap_components as dbc\n",
    "# sudo conda install -c conda-forge dash-bootstrap-components jupyter-dash --name paper2022\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from PIL import ImageDraw, Image\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Helper functions\n",
    "def np_image_to_base64(im_matrix,scale=4):\n",
    "\n",
    "    im_matrix = np.repeat(np.repeat(im_matrix,scale,axis=0),scale,axis=1)\n",
    "    im = Image.fromarray(im_matrix)\n",
    "    buffer = io.BytesIO()\n",
    "    im.save(buffer, format=\"jpeg\")\n",
    "    encoded_image = base64.b64encode(buffer.getvalue()).decode()\n",
    "    im_url = \"data:image/jpeg;base64, \" + encoded_image\n",
    "    return im_url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def binary_matrix_to_image(binary_matrix, grid_size=8, border_size=1):\n",
    "    # Calculate the size of the output image based on the size of the binary matrix\n",
    "    height, width = binary_matrix.shape[:2]\n",
    "    image_width = width * grid_size + (width + 1) * border_size\n",
    "    image_height = height * grid_size + (height + 1) * border_size\n",
    "    \n",
    "    # Create a new image and a draw object to draw the grid and borders\n",
    "    image = Image.new('RGB', (image_width, image_height), color='black')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Draw the white grids\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            if binary_matrix[i, j] == 1:\n",
    "                x1 = j * (grid_size + border_size) + border_size\n",
    "                y1 = i * (grid_size + border_size) + border_size\n",
    "                x2 = x1 + grid_size\n",
    "                y2 = y1 + grid_size\n",
    "                draw.rectangle((x1, y1, x2, y2), fill='white')\n",
    "                \n",
    "    \n",
    "    # Draw the black borders\n",
    "    for i in range(height + 1):\n",
    "        y = i * (grid_size + border_size)\n",
    "        draw.line((0, y, image_width, y), fill='white', width=border_size)\n",
    "        \n",
    "    for j in range(width + 1):\n",
    "        x = j * (grid_size + border_size)\n",
    "        draw.line((x, 0, x, image_height), fill='white', width=border_size)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_rgb(num, **rgb_params):\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(rgb_params)>0:\n",
    "\n",
    "        normalized = (num - rgb_params['offset'])/rgb_params['range']\n",
    "\n",
    "        \n",
    "        # Map to a color between blue, gray, and red\n",
    "        rgb_lower = rgb_params['rgb_lower']\n",
    "        rgb_middle = rgb_params['rgb_middle']\n",
    "        rgb_upper = rgb_params['rgb_upper']\n",
    "    else:\n",
    "    \n",
    "        # Normalize to the range of 0 to 1\n",
    "    \n",
    "    \n",
    "    \n",
    "        normalized = (num + 2) / 4\n",
    "        \n",
    "        # Map to a color between blue, gray, and red\n",
    "        rgb_lower = (0, 0, 255,1)\n",
    "        rgb_middle = (128, 128, 128,1)\n",
    "        rgb_upper = (255, 0, 0,1)\n",
    "    \n",
    "    if normalized < 0.5:\n",
    "        r = int((2 * normalized) * rgb_middle[0] + (1 - 2 * normalized) * rgb_lower[0])\n",
    "        g = int((2 * normalized) * rgb_middle[1] + (1 - 2 * normalized) * rgb_lower[1])\n",
    "        b = int((2 * normalized) * rgb_middle[2] + (1 - 2 * normalized) * rgb_lower[2])\n",
    "        a = int((2 * normalized) * rgb_middle[3] + (1 - 2 * normalized) * rgb_lower[3])\n",
    "    else:\n",
    "        r = int((2 * normalized - 1) * rgb_upper[0] + (2 - 2 * normalized) * rgb_middle[0])\n",
    "        g = int((2 * normalized - 1) * rgb_upper[1] + (2 - 2 * normalized) * rgb_middle[1])\n",
    "        b = int((2 * normalized - 1) * rgb_upper[2] + (2 - 2 * normalized) * rgb_middle[2])\n",
    "        a = int((2 * normalized - 1) * rgb_upper[3] + (2 - 2 * normalized) * rgb_middle[3])\n",
    "    \n",
    "    # if normalized>0.5:\n",
    "    #     print(normalized,a,rgb_upper[3])\n",
    "    #     term\n",
    "    return (r, g, b,a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import ImageDraw, Image, ImageFont\n",
    "\n",
    "def binary_matrix_to_image(binary_matrix, row_labels=None, font_path=None, grid_size=10, border_size=1, label_size=15, is_fill=True,**rgb_params):\n",
    "    # Add a dummy column to the binary matrix\n",
    "    ddd=2\n",
    "\n",
    "    height, width = binary_matrix.shape[:2]\n",
    "    binary_matrix = np.concatenate((np.zeros((height, ddd)), binary_matrix), axis=1)\n",
    "    width += ddd\n",
    "    \n",
    "    # Calculate the size of the output image based on the size of the binary matrix\n",
    "    image_width = (width + 1) * grid_size + (width + 2) * border_size\n",
    "    image_height = height * grid_size + (height + 1) * border_size\n",
    "    \n",
    "    # Create a new image and a draw object to draw the grid and borders\n",
    "    image = Image.new('RGBA', (image_width, image_height), color=(0,0,0,0))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Draw the white grids\n",
    "    for i in range(height):\n",
    "        for j in range(1, width):\n",
    "            # if binary_matrix[i, j] == 1:\n",
    "            #     x1 = j * (grid_size + border_size) + border_size\n",
    "            #     y1 = i * (grid_size + border_size) + border_size\n",
    "            #     x2 = x1 + grid_size\n",
    "            #     y2 = y1 + grid_size\n",
    "            #     draw.rectangle((x1, y1, x2, y2), fill='white')\n",
    "\n",
    "            if binary_matrix[i, j] != 0:\n",
    "                x1 = j * (grid_size + border_size) + border_size\n",
    "                y1 = i * (grid_size + border_size) + border_size\n",
    "                x2 = x1 + grid_size\n",
    "                y2 = y1 + grid_size\n",
    "\n",
    "                int_color = ( int(binary_matrix[i, j]*96),\n",
    "                             int(binary_matrix[i, j]*96),\n",
    "                             int(binary_matrix[i, j]*96))\n",
    "\n",
    "                \n",
    "                int_color = map_to_rgb(binary_matrix[i, j], **rgb_params)\n",
    "                if is_fill:\n",
    "                    draw.rectangle((x1, y1, x2, y2), fill= int_color)\n",
    "                else:\n",
    "                    draw.rectangle((x1, y1, x2, y2), outline=int_color,width=2, fill= None)\n",
    "\n",
    "\n",
    "    # Draw the borders\n",
    "    color_border = (0,0,0,16)\n",
    "    for i in range(height + 1):\n",
    "        y = i * (grid_size + border_size)\n",
    "        draw.line((grid_size, y, image_width, y), fill=color_border, width=border_size)\n",
    "        \n",
    "    for j in range(width + 1):\n",
    "        x = j * (grid_size + border_size)\n",
    "        draw.line((x, 0, x, image_height), fill=color_border, width=border_size)\n",
    "\n",
    "    \n",
    "    # Draw the row labels\n",
    "    if row_labels is not None:\n",
    "        font = ImageFont.truetype(PATH_SYS+'arial.ttf', size=label_size)\n",
    "        max_label_width = max([font.getsize(str(label))[0] for label in row_labels])\n",
    "        label_x = 0\n",
    "        label_y = border_size\n",
    "        for i, label in enumerate(row_labels):\n",
    "            draw.text((label_x, label_y), str(label), font=font, fill='black',align=\"right\")\n",
    "            label_y += grid_size + border_size\n",
    "            # if i == 0:\n",
    "            #     label_x += max_label_width + border_size + grid_size\n",
    "        \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def create_barplot_image(data, labels, filename):\n",
    "    \"\"\"\n",
    "    Creates a bar plot image of the input data and saves it as a PNG file with the given filename.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): A 1D array of data to plot.\n",
    "        labels (list): A list of labels for each data point.\n",
    "        filename (str): The name of the output PNG file.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: A PIL image object of the bar plot.\n",
    "    \"\"\"\n",
    "    # Create a figure and axis object\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Create a bar plot\n",
    "    _ = ax.bar(np.arange(len(data)), data)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    _ = ax.set_xlabel('Index')\n",
    "    _ = ax.set_ylabel('Value')\n",
    "    _ = ax.set_title('Bar plot')\n",
    "\n",
    "    # Set x-tick labels\n",
    "    _ = ax.set_xticks(np.arange(len(data)))\n",
    "    _ = ax.set_xticklabels(labels, rotation=45)\n",
    "\n",
    "    # Save the plot as a PNG file\n",
    "    _ = fig.savefig(filename)\n",
    "    plt.close(fig)\n",
    "    # Load the PNG image and return as a PIL image object\n",
    "    return Image.open(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event2mat(event_type, event_time,P):\n",
    "\n",
    "    m = event_type.sum(1)>0 # False are masked\n",
    "    e=event_type[m,:]\n",
    "    t=event_time[m]\n",
    "    # print(t)\n",
    "\n",
    "    indices = e.nonzero()\n",
    "    indices[:,0] = t[indices[:,0]].int()\n",
    "    # print(t[indices[:,0]])\n",
    "    # print(indices[:,0])\n",
    "\n",
    "    M = torch.zeros((P, e.shape[-1]))\n",
    "    M[indices[:,0],indices[:,1]]=1\n",
    "\n",
    "    return M.detach().cpu().numpy().transpose() # [M,P]\n",
    "\n",
    "def att_event2mat(event_type, event_time,P, tee_att):\n",
    "    # tee_att [h,L]\n",
    "    h = tee_att.shape[0]\n",
    "\n",
    "    m = event_type.sum(1)>0 # False are masked\n",
    "    e=event_type[m,:]\n",
    "    t=event_time[m]\n",
    "    tee_att = tee_att[:,m]\n",
    "\n",
    "    # indices = e.nonzero()\n",
    "    # indices[:,0] = t[indices[:,0]].int()\n",
    "\n",
    "    # M = torch.zeros(h, len(t))\n",
    "    M = torch.zeros(( h , P))\n",
    "\n",
    "    M[:,t.long()]=tee_att\n",
    "\n",
    "    return M.detach().cpu().numpy() # [h,L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state2mat(time, mod,M):\n",
    "    P = time.int().max().item() + 1\n",
    "\n",
    "    M = torch.zeros(P, M)\n",
    "    M[time[mod!=0].long(), mod[mod!=0]-1] = 1\n",
    "\n",
    "\n",
    "    return M.cpu().numpy().transpose() # [M,P]\n",
    "\n",
    "def att_state2mat(time, mod,M, dam_att):\n",
    "    # dam_att [P,h]\n",
    "    # h = dam_att.shape[-1]\n",
    "\n",
    "    # dam_att = dam_att[:,0] # [P]\n",
    "    P = time.int().max().item() + 1\n",
    "\n",
    "    M = torch.zeros(P, M)\n",
    "    \n",
    "    M[time[mod!=0].long(),  mod[mod!=0]-1] = dam_att[mod!=0]\n",
    "\n",
    "\n",
    "    return M.cpu().numpy().transpose() # [M,P]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cool_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM:3255\n",
      "NUM:2292\n",
      "NUM:3801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1207/753296189.py:62: DeprecationWarning:\n",
      "\n",
      "getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM:1732\n",
      "NUM:3577\n",
      "NUM:1084\n",
      "NUM:1246\n",
      "NUM:2728\n",
      "NUM:394\n",
      "NUM:3577\n",
      "NUM:162\n",
      "NUM:162\n",
      "NUM:3378\n",
      "NUM:3378\n",
      "NUM:162\n",
      "NUM:2263\n",
      "NUM:2942\n",
      "NUM:1202\n",
      "NUM:2263\n",
      "NUM:2942\n",
      "NUM:1803\n",
      "NUM:2942\n",
      "NUM:1202\n",
      "NUM:1565\n",
      "NUM:1310\n",
      "NUM:6\n",
      "NUM:1495\n",
      "NUM:1619\n",
      "NUM:1565\n",
      "NUM:1619\n"
     ]
    }
   ],
   "source": [
    "def cool_image(out,i_b,i,opt,CROP=0):\n",
    "\n",
    "    att_mat_img=None\n",
    "    img_ev=None\n",
    "\n",
    "    label_size = 12\n",
    "    rgb_params_values = {\n",
    "    'rgb_lower':(0, 0, 255,255),\n",
    "    'rgb_middle':(128, 128, 128,255),\n",
    "    'rgb_upper':(255, 0, 0,255),\n",
    "    'offset':-2,\n",
    "    'range':4\n",
    "\n",
    "    }\n",
    "    rgb_params_events = {\n",
    "    'rgb_lower':(255, 255, 255,255),\n",
    "    'rgb_middle':(196, 196, 196,255),\n",
    "    'rgb_upper':(128, 128, 128,255),\n",
    "    'offset':0,\n",
    "    'range':1\n",
    "\n",
    "    }\n",
    "    rgb_params_events_pred = {\n",
    "    'rgb_lower':(255, 255, 255,255),\n",
    "    'rgb_middle':(128, 255, 128,255),\n",
    "    'rgb_upper':(0, 255, 0,255),\n",
    "    'offset':0,\n",
    "    'range':1\n",
    "\n",
    "    }\n",
    "    # rgb_params_att = {\n",
    "    #     'rgb_lower':(64, 64, 64,0),\n",
    "    #     'rgb_middle':(164, 164, 32,32),\n",
    "    #     'rgb_upper':(255, 255, 0,255),\n",
    "    #     'offset':0,\n",
    "    #     'range':0.1\n",
    "\n",
    "    # }\n",
    "\n",
    "\n",
    "    rgb_params_att = {\n",
    "        'rgb_lower':(128, 128, 128,32),\n",
    "        # 'rgb_middle':(8, 164, 32,196),\n",
    "        'rgb_upper':(0, 255, 0,255),\n",
    "        'offset':0,\n",
    "        'range':0.2\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    rgb_params_att['rgb_middle'] = tuple([(int(i+j)/2) for i,j in zip(rgb_params_att['rgb_lower'],rgb_params_att['rgb_upper'])])\n",
    "    rgb_params_att['rgb_middle']\n",
    "    if hasattr(opt,'dict_map_states' ):\n",
    "        state_labels = list(opt.dict_map_states.keys())[CROP:]\n",
    "        # state_labels = [str(i) for i in range(len(opt.dict_map_states))]\n",
    "\n",
    "    if hasattr(opt,'dict_map_events' ):\n",
    "        offset=len(opt.dict_map_states.keys())-len(opt.dict_map_events.keys())\n",
    "        event_labels = list(opt.dict_map_events.keys())\n",
    "        # event_labels = [str(i+offset) for i in range(len(opt.dict_map_events))]\n",
    "\n",
    "\n",
    "    # event_labels = opt.dict_map_events.keys()\n",
    "    \n",
    "\n",
    "    # if len(out['event_type_list']) > 0:\n",
    "    \n",
    "\n",
    "    if len(out['state_time_list']) > 0:\n",
    "        st=out['state_time_list'][i_b][i]\n",
    "        sm=out['state_mod_list'][i_b][i]\n",
    "        sv=( out['state_value_list'][i_b][i] )\n",
    "\n",
    "        dam_att = out['list_DAM_att'][i_b][i] # [P,h]\n",
    "\n",
    "        n_state_mods = len(opt.dict_map_states.keys())\n",
    "\n",
    "        # ev.shape, t.shape\n",
    "        P = st.int().max().item() + 1\n",
    "\n",
    "        \n",
    "        # list_img_att = []\n",
    "        # for i_head in range(dam_att.shape[-1]):\n",
    "        #     binary_matrix = att_state2mat(st, sm, n_state_mods, dam_att[:,i_head])\n",
    "        #     img_att = binary_matrix_to_image(binary_matrix, row_labels=state_labels, font_path=None, grid_size=10, border_size=1, label_size=label_size,is_fill=False,**rgb_params_att)\n",
    "        #     list_img_att.append(img_att.convert(\"RGB\"))\n",
    "\n",
    "        binary_matrix = att_state2mat(st, sm, n_state_mods, dam_att.sum(1))[CROP:,:]\n",
    "        # print(f'binary_matrix.shape={binary_matrix.shape}')\n",
    "        \n",
    "        img_att = binary_matrix_to_image(binary_matrix, row_labels=state_labels, font_path=None, grid_size=10, border_size=2, label_size=label_size,is_fill=False,**rgb_params_att)\n",
    "\n",
    "\n",
    "        binary_matrix = att_state2mat(st, sm, n_state_mods, sv)[CROP:,:]\n",
    "        img_val = binary_matrix_to_image(binary_matrix, row_labels=state_labels, font_path=None, grid_size=10, border_size=2, label_size=label_size,is_fill=True,**rgb_params_values)\n",
    "\n",
    "        merged_img = Image.new('RGBA', img_val.size)\n",
    "        merged_img = Image.alpha_composite(merged_img, img_val)\n",
    "        tempp=state_labels\n",
    "        merged_img = Image.alpha_composite(merged_img, img_att)#.convert(\"RGB\")\n",
    "\n",
    "        if opt.event_enc:\n",
    "            ev = out['event_type_list'][i_b][i]\n",
    "            t = out['event_time_list'][i_b][i]\n",
    "            \n",
    "            ev_pred = out['next_event_type_list'][i_b][i]\n",
    "\n",
    "\n",
    "            m = (ev.sum(1)>0).sum() # False are masked\n",
    "            ev=ev[:m]\n",
    "            ev_pred=ev_pred[:m-1] # for 2 to L-1\n",
    "            ev_pred = nn.functional.pad(ev_pred,(0,0,1,0))\n",
    "            t=t[:m]\n",
    "\n",
    "\n",
    "            # print(f'm={m}')\n",
    "\n",
    "            # print(f'ev.shape={ev.shape}')\n",
    "            # print(f'ev_pred.shape={ev_pred.shape}')\n",
    "\n",
    "                                # plot attention of last event\n",
    "            tee_att = out['list_TE_att'][i_b][i,:,m-1,:m] # [h,L]\n",
    "            # print(f'tee_att.shape={tee_att.shape}')\n",
    "            # # print(tee_att)\n",
    "            \n",
    "\n",
    "                                \n",
    "            # att_names = [f'h{i}' for i in range(tee_att.shape[0])]\n",
    "            # rgb_params_att['range']=tee_att.max()\n",
    "            # binary_matrix = att_event2mat(ev, t,P, tee_att)\n",
    "\n",
    "            # img_att_ev = binary_matrix_to_image(binary_matrix, row_labels=att_names, font_path=None, grid_size=10, border_size=1, label_size=label_size,is_fill=False,**rgb_params_att)\n",
    "            \n",
    "            # merged_img2 = Image.new('RGBA', (merged_img.size[0],merged_img.size[1]+img_att_ev.size[1]))\n",
    "            # merged_img2.paste(merged_img, (0,0))\n",
    "            # merged_img2.paste(img_att_ev, (0,merged_img.size[1]))\n",
    "            # merged_img = merged_img2\n",
    "\n",
    "\n",
    "\n",
    "                                # plot attention matrix\n",
    "            m = (ev.sum(1)>0).sum() # False are masked\n",
    "            tee_att_mat = out['list_TE_att'][i_b][i,:,:m,:m].mean(0) # [L,L]\\\n",
    "            # tee = out['list_TE_att'][i_b][i][:1,:m,:m].mean(0)#.cpu().numpy() \n",
    "            print('A1',tee_att_mat)\n",
    "            # tee_att_mat[tee_att_mat<0.02]=0\n",
    "            # print(f'tee_att_mat.shape={tee_att_mat.shape}')\n",
    "            print('aaaa',tee_att_mat)\n",
    "\n",
    "            # # print(tee_att_mat.sum(1))\n",
    "            scaler=torch.arange(1,m+1)[:,None]\n",
    "            print(scaler)\n",
    "            # scaler= (1/tee_att_mat.max(1)[0])[:,None]\n",
    "            tee_att_mat *= scaler\n",
    "            print('A2',tee_att_mat)\n",
    "\n",
    "            tee_att_mat[tee_att_mat<0.02]=0\n",
    "\n",
    "            print(type(tee_att_mat))\n",
    "            rgb_params_att['range'] = tee_att_mat.max()\n",
    "            # # print(tee_att_mat.max(1))\n",
    "            print('dafas',tee_att_mat)\n",
    "            # # print(tee_att_mat[1])\n",
    "\n",
    "            tee_att_mat_spaced = att_event2mat(ev, t,P, tee_att_mat)\n",
    "            # print(f'tee_att_mat_spaced.shape={tee_att_mat_spaced.shape}')\n",
    "\n",
    "            binary_matrix = tee_att_mat_spaced\n",
    "            img_val = binary_matrix_to_image(binary_matrix, row_labels=None, font_path=None, grid_size=10, border_size=1, label_size=label_size,is_fill=True,**rgb_params_att)\n",
    "\n",
    "            att_mat_img = Image.new('RGBA', img_val.size)\n",
    "            att_mat_img = Image.alpha_composite(att_mat_img, img_val)\n",
    "\n",
    "\n",
    "            # t=torch.arange(m)\n",
    "            # PP = t.int().max().item() + 1\n",
    "\n",
    "            # print(t[:m])\n",
    "            ev_matrix = event2mat(ev,t,P)\n",
    "            # print(f'ev_matrix.shape={ev_matrix.shape}')\n",
    "            # print(P)\n",
    "            img_ev = binary_matrix_to_image(ev_matrix, row_labels=event_labels, font_path=None, grid_size=10, border_size=1, label_size=label_size,**rgb_params_events)\n",
    "\n",
    "\n",
    "            ev_pred_matrix = event2mat(ev_pred,t,P)\n",
    "            # print(f'ev_pred_matrix.shape={ev_pred_matrix.shape}')\n",
    "\n",
    "            img_ev_pred = binary_matrix_to_image(ev_pred_matrix, row_labels=None, font_path=None, grid_size=10, border_size=1,is_fill=False, label_size=label_size,**rgb_params_events_pred)\n",
    "            \n",
    "            merged_img3 = Image.new('RGBA', (att_mat_img.size[0],att_mat_img.size[1]+img_ev.size[1]))\n",
    "            merged_img3.paste(att_mat_img, (0,0))\n",
    "            merged_img3.paste(img_ev, (0,att_mat_img.size[1]))\n",
    "            merged_img3.paste(img_ev_pred, (0,att_mat_img.size[1]))\n",
    "\n",
    "            temp_img = Image.new('RGBA', img_ev.size)\n",
    "            temp_img = Image.alpha_composite(temp_img, img_ev)\n",
    "            temp_img = Image.alpha_composite(temp_img, img_ev_pred)#.convert(\"RGB\")\n",
    "\n",
    "            merged_img3.paste(temp_img, (0,att_mat_img.size[1]))\n",
    "\n",
    "            att_mat_img = merged_img3\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        \n",
    "        ev = out['event_type_list'][i_b][i]\n",
    "        t = out['event_time_list'][i_b][i]\n",
    "        P = t.int().max().item() + 1\n",
    "        m = (ev.sum(1)>0).sum() # False are masked\n",
    "\n",
    "        tee_att = out['list_TE_att'][i_b][i,:,m-2,:m] # [h,L]\n",
    "        att_names = [f'h{i}' for i in range(tee_att.shape[0])]\n",
    "        rgb_params_att['range']=tee_att.max()\n",
    "\n",
    "        binary_matrix = event2mat(ev,t,P)\n",
    "        img_ev = binary_matrix_to_image(binary_matrix, row_labels=event_labels, font_path=None, grid_size=10, border_size=1, label_size=label_size,**rgb_params_events)\n",
    "\n",
    "        binary_matrix = att_event2mat(ev, t,P, tee_att)\n",
    "        img_att_ev = binary_matrix_to_image(binary_matrix, row_labels=None, font_path=None, grid_size=10, border_size=1, label_size=label_size,is_fill=False,**rgb_params_att)\n",
    "            \n",
    "        # print(binary_matrix.sum())\n",
    "        merged_img = Image.new('RGB', (img_ev.size[0],img_ev.size[1]+img_att_ev.size[1]))\n",
    "        merged_img.paste(img_ev, (0,0))\n",
    "        merged_img.paste(img_att_ev, (0,img_ev.size[1]))\n",
    "\n",
    "\n",
    "\n",
    "        # plot attention matrix\n",
    "        m = (ev.sum(1)>0).sum() # False are masked\n",
    "        tee_att_mat = out['list_TE_att'][i_b][i,:,:m,:m].mean(0) # [L,L]\n",
    "\n",
    "        binary_matrix = tee_att_mat\n",
    "        img_val = binary_matrix_to_image(binary_matrix, row_labels=None, font_path=None, grid_size=10, border_size=1, label_size=label_size,is_fill=True,**rgb_params_att)\n",
    "        \n",
    "        att_mat_img = Image.new('RGBA', img_val.size)\n",
    "        att_mat_img = Image.alpha_composite(att_mat_img, img_val)\n",
    "\n",
    "\n",
    "    return merged_img,(att_mat_img,img_ev)#(img_att.convert(\"RGB\"), img_val.convert(\"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_summary(df,out, point_ids,res_labels, fig=None):\n",
    "\n",
    "    if fig is None:\n",
    "        fig = go.Figure()\n",
    "\n",
    "\n",
    "\n",
    "    list_summary = []\n",
    "    for pid in point_ids:\n",
    "\n",
    "        i_b = int(df.iloc[pid]['i_b'])\n",
    "        i = int(df.iloc[pid]['i'])\n",
    "\n",
    "        ev = out['event_type_list'][i_b][i]\n",
    "        t = out['event_time_list'][i_b][i]\n",
    "        \n",
    "\n",
    "        P = t.int().max().item() + 1\n",
    "\n",
    "        M = event2mat(ev,t,P)\n",
    "\n",
    "\n",
    "        vector = M.sum(1)/M.shape[1]*24\n",
    "        list_summary.append(vector)\n",
    "\n",
    "    vec_mean = np.mean(list_summary,axis=0)\n",
    "    vec_std = np.std(list_summary,axis=0)\n",
    "\n",
    "    sum(vec_std)\n",
    "\n",
    "    \n",
    "    _ = fig.add_trace(go.Bar(\n",
    "        name=f'Summary',\n",
    "        x=list(res_labels), y=vec_mean,\n",
    "        error_y=dict(type='data', array=vec_std)\n",
    "    ))\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_wandb(run_path, consider_sample_labels = False):\n",
    "\n",
    "    run = api.run(run_path)\n",
    "    for file in run.files():\n",
    "        file.download(replace=True,root=f'./local/{run_path}/')\n",
    "\n",
    "    opt = pickle.load(open(f'./local/{run_path}/opt.pkl','rb'))\n",
    "    \n",
    "    if consider_sample_labels:\n",
    "        opt.sample_label=True\n",
    "\n",
    "\n",
    "    opt = Main.config(opt, justLoad=True)\n",
    "    if not hasattr(opt,'diag_offset'):\n",
    "        opt.diag_offset=1\n",
    "        print('ATTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT')\n",
    "    checkpoint = torch.load(f'./local/{run_path}/best_model.pkl')\n",
    "\n",
    "    model = Main.ATHP(\n",
    "       n_marks=opt.num_marks,\n",
    "        TE_config = opt.TE_config,\n",
    "        DAM_config = opt.DAM_config,\n",
    "        NOISE_config = opt.NOISE_config,\n",
    "\n",
    "        CIF_config = opt.CIF_config,\n",
    "        next_time_config = opt.next_time_config,\n",
    "        next_type_config = opt.next_type_config,\n",
    "        label_config = opt.label_config,\n",
    "\n",
    "        demo_config = opt.demo_config,\n",
    "\n",
    "        device=opt.device,\n",
    "        diag_offset=opt.diag_offset\n",
    "    )\n",
    "\n",
    "    _ = model.to(opt.device)\n",
    "    _ = model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    _ = model.eval()\n",
    "\n",
    "    return model, opt, run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_knn_pids (X, id_origin, n_knn=10):\n",
    "\n",
    "    # X [N,d] np.array\n",
    "    r = np.sqrt(   ((X-X[id_origin])**2).sum(-1)   )\n",
    "    knn_pids = list(np.argpartition(r,n_knn)[:n_knn])\n",
    "\n",
    "    if id_origin not in knn_pids:\n",
    "        print('bad')\n",
    "    else:\n",
    "        knn_pids.remove(id_origin)\n",
    "\n",
    "    # print(X.shape)\n",
    "    # term\n",
    "\n",
    "    \n",
    "    # x0 = df.iloc[id_origin]['x']\n",
    "    # y0 = df.iloc[id_origin]['y']\n",
    "\n",
    "    # df['r'] = df.apply(lambda row: ( (row['x']-x0)**2 + (row['y']-y0)**2  ), axis=1)\n",
    "\n",
    "    # knn_pids = list(df.nsmallest(n_knn,'r').index)\n",
    "\n",
    "\n",
    "    return knn_pids\n",
    "\n",
    "def cal_similarity(df, out, pid, knn_pids):\n",
    "\n",
    "    list_summary = []\n",
    "    for pid in knn_pids:\n",
    "        \n",
    "\n",
    "        i_b = df.iloc[pid]['i_b']\n",
    "        i = df.iloc[pid]['i']\n",
    "\n",
    "        ev = out['event_type_list'][i_b][i]\n",
    "        t = out['event_time_list'][i_b][i]\n",
    "        st=out['state_time_list'][i_b][i]\n",
    "        \n",
    "        P = st.int().max().item() + 1\n",
    "\n",
    "        M = event2mat(ev,t,P)\n",
    "\n",
    "        \n",
    "\n",
    "        vector = M.sum(1)/M.shape[1]*24\n",
    "\n",
    "        \n",
    "        list_summary.append(vector)\n",
    "        \n",
    "\n",
    "    dotp = [ np.dot(j,list_summary[0])/(np.linalg.norm(j) * np.linalg.norm(list_summary[0])) for  j in list_summary]\n",
    "    sim_score = np.mean( dotp )\n",
    "\n",
    "\n",
    "    return sim_score, list_summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cif(out,i_b,i, fig=None):\n",
    "\n",
    "    ev = out['event_type_list'][i_b][i]\n",
    "    t = out['event_time_list'][i_b][i]\n",
    "\n",
    "\n",
    "    tee_att = out['list_TE_att'][i_b][i,:,-4,:] # [h,L]\n",
    "    cifs = out['list_intens_at_samples'][i_b][i]\n",
    "    taus = out['list_taus'][i_b][i]\n",
    "\n",
    "    y_cifs = out['list_true_intens_at_evs'][i_b][i] # [L-1] 2 to L\n",
    "\n",
    "    P = t.int().max().item() + 1\n",
    "\n",
    "    m = ev.sum(1)>0 # False are masked\n",
    "    m = m.sum().item()\n",
    "\n",
    "    \n",
    "    taus = taus[:m-1,:,:] + t[:m-1,None,None]\n",
    "    cifs = cifs[:m-1,:,:]\n",
    "    y_cifs = y_cifs[:m-1,:]  # 1 to L-1\n",
    "\n",
    "\n",
    "    n_cifs=cifs.shape[1]\n",
    "    # print(n_cifs,taus.shape, cifs.shape)\n",
    "\n",
    "    cifs = cifs.reshape(n_cifs,-1)[0] # [0] is the first cif\n",
    "    taus = taus.reshape(1,-1)[0]\n",
    "    y_cifs = y_cifs.reshape(n_cifs,-1)[0]\n",
    "\n",
    "    print(taus)\n",
    "    print(cifs)\n",
    "    # print(n_cifs,taus.shape, cifs.shape)\n",
    "    temp = torch.argsort(taus)\n",
    "\n",
    "    taus = taus[temp]\n",
    "    cifs = cifs[temp]\n",
    "\n",
    "\n",
    "    if fig==None:\n",
    "        fig=go.Figure()\n",
    "\n",
    "    _ = fig.add_trace(go.Scatter(x=taus,y=cifs))\n",
    "\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=y_cifs*0+1,mode='markers'))\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=t[1:m],mode='markers'))\n",
    "\n",
    "    # print(m)\n",
    "    # print(t[1:5])\n",
    "    # print(y_cifs[:4])\n",
    "\n",
    "    _ = fig.add_trace(go.Scatter(x=t[:m],y=t[:m]*0,mode='markers'))\n",
    "\n",
    "    _ = fig.add_trace(go.Scatter(x=t[1:m],y=y_cifs,mode='markers'))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cif_cum(out,i_b,i, fig=None):\n",
    "\n",
    "    ev = out['event_type_list'][i_b][i]\n",
    "    t = out['event_time_list'][i_b][i]\n",
    "\n",
    "\n",
    "    tee_att = out['list_TE_att'][i_b][i,:,-4,:] # [h,L]\n",
    "    cifs = out['list_intens_at_samples'][i_b][i]\n",
    "    taus = out['list_taus'][i_b][i]\n",
    "\n",
    "    y_cifs = out['list_true_intens_at_evs'][i_b][i] # [L-1] 2 to L\n",
    "\n",
    "    P = t.int().max().item() + 1\n",
    "\n",
    "    m = ev.sum(1)>0 # False are masked\n",
    "    m = m.sum().item()\n",
    "\n",
    "    taus = taus[:m-1,:,:] + t[:m-1,None,None]\n",
    "    cifs = cifs[:m-1,:,:]\n",
    "    y_cifs = y_cifs[:m-1,:]  # 1 to L-1\n",
    "\n",
    "\n",
    "    n_cifs=cifs.shape[1]\n",
    "\n",
    "    cifs = cifs.reshape(n_cifs,-1)[0]\n",
    "    taus = taus.reshape(n_cifs,-1)[0]\n",
    "    y_cifs = y_cifs.reshape(n_cifs,-1)[0]\n",
    "\n",
    "    temp = torch.argsort(taus)\n",
    "\n",
    "    taus = taus[temp]\n",
    "    cifs = cifs[temp]\n",
    "\n",
    "    cifs_int = integrate.cumulative_trapezoid(cifs, taus, initial=0)/(taus)\n",
    "\n",
    "    if fig==None:\n",
    "        fig=go.Figure()\n",
    "\n",
    "    _ = fig.add_trace(go.Scatter(x=taus,y=cifs_int))\n",
    "\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=y_cifs*0+1,mode='markers'))\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=t[1:m],mode='markers'))\n",
    "\n",
    "    # print(m)\n",
    "    # print(t[1:5])\n",
    "    # print(y_cifs[:4])\n",
    "\n",
    "    _ = fig.add_trace(go.Scatter(x=t[:m],y=t[:m]*0,mode='markers'))\n",
    "\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=y_cifs,mode='markers'))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cif_cum(out,i_b,i, fig=None):\n",
    "\n",
    "    ev = out['event_type_list'][i_b][i]\n",
    "    t = out['event_time_list'][i_b][i]\n",
    "\n",
    "\n",
    "    tee_att = out['list_TE_att'][i_b][i,:,-4,:] # [h,L]\n",
    "    cifs = out['list_intens_at_samples'][i_b][i]\n",
    "    taus = out['list_taus'][i_b][i]\n",
    "\n",
    "    y_cifs = out['list_true_intens_at_evs'][i_b][i] # [L-1] 2 to L\n",
    "\n",
    "    P = t.int().max().item() + 1\n",
    "\n",
    "    m = ev.sum(1)>0 # False are masked\n",
    "    m = m.sum().item()\n",
    "\n",
    "    taus = taus[:m-1,:,:] + t[:m-1,None,None]\n",
    "    cifs = cifs[:m-1,:,:]\n",
    "    y_cifs = y_cifs[:m-1,:]  # 1 to L-1\n",
    "\n",
    "\n",
    "    n_cifs=cifs.shape[1]\n",
    "\n",
    "    cifs = cifs.reshape(n_cifs,-1)[0]\n",
    "    taus = taus.reshape(n_cifs,-1)[0]\n",
    "    y_cifs = y_cifs.reshape(n_cifs,-1)[0]\n",
    "\n",
    "    temp = torch.argsort(taus)\n",
    "\n",
    "    taus = taus[temp]\n",
    "    cifs = cifs[temp]\n",
    "\n",
    "    cifs_int = integrate.cumulative_trapezoid(cifs, taus, initial=0)/(taus)\n",
    "\n",
    "    if fig==None:\n",
    "        fig=go.Figure()\n",
    "\n",
    "    _ = fig.add_trace(go.Scatter(x=taus,y=cifs_int))\n",
    "\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=y_cifs*0+1,mode='markers'))\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=t[1:m],mode='markers'))\n",
    "\n",
    "    # print(m)\n",
    "    # print(t[1:5])\n",
    "    # print(y_cifs[:4])\n",
    "\n",
    "    _ = fig.add_trace(go.Scatter(x=t[:m],y=t[:m]*0,mode='markers'))\n",
    "\n",
    "    # _ = fig.add_trace(go.Scatter(x=t[1:m],y=y_cifs,mode='markers'))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return taus,cifs_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.arange(6).reshape(2,3)\n",
    "a\n",
    "\n",
    "np.roll(a,-1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## att agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(row1, row2):\n",
    "    intersection = np.logical_and(row1, row2).sum()\n",
    "    union = np.logical_or(row1, row2).sum()\n",
    "    return intersection / union\n",
    "\n",
    "def compute_jaccard_similarity(A,B):\n",
    "\n",
    "    L, M = A.shape\n",
    "    M, C = B.shape\n",
    "    res = np.zeros((L, C))\n",
    "    for i in range(L):\n",
    "        for j in range(C):\n",
    "            res[i, j] = jaccard_similarity(A[i], B[:, j])\n",
    "    return res\n",
    "\n",
    "\n",
    "def att_map(out,i_b,i,common_patterns):\n",
    "\n",
    "    # common_patterns is of shape [C,M]\n",
    "\n",
    "    tee_mapped = np.zeros((common_patterns.shape[0], common_patterns.shape[0])) # [C,C]\n",
    "    norm = np.zeros((common_patterns.shape[0], common_patterns.shape[0])) # [C,C]\n",
    "\n",
    "    N_CLUSTERS = common_patterns.shape[0]\n",
    "    ev = out['event_type_list'][i_b][i].cpu().numpy()   # [L,M]\n",
    "    \n",
    "    \n",
    "    if len(ev.shape)==1:    # if multi-class\n",
    "        identity = np.eye(common_patterns.shape[1]+1,dtype=np.int8)\n",
    "        ev = identity.take(ev, axis=0)[:,1:] #  [B,LLLL] -> [B,LLLLLL,K]\n",
    "    \n",
    "    m=(ev.sum(1)>0).sum()\n",
    "    m\n",
    "    if m==0:\n",
    "        # print(out['event_time_list'][i_b][i])\n",
    "        print(\"NO EVENTS!\")\n",
    "        return tee_mapped, norm\n",
    "    ev=ev[:m]\n",
    "    tee = out['list_TE_att'][i_b][i][:1,:m,:m].mean(0).cpu().numpy() \n",
    "    print('A1',tee)\n",
    "\n",
    "    # tee(i,j): influence of j-th event on (i+1)-th event    \n",
    "    tee[-1,:] = 0\n",
    "    scaler=np.arange(1,m+1)[:,None]\n",
    "    # scaler= (1/tee_att_mat.max(1)[0])[:,None]\n",
    "    tee *= scaler\n",
    "    print('A2',tee)\n",
    "\n",
    "    print(tee[:6,:6])\n",
    "\n",
    "    q=(tee>0.02).astype(np.int8)\n",
    "\n",
    "    # print(tee[-12:,-12:])\n",
    "    TH=3\n",
    "    tee[tee<TH]=0\n",
    "    tee[tee>=TH]=1\n",
    "\n",
    "    \n",
    "    # print(tee[-12:,-12:].astype(int))\n",
    "    # print(q[-12:,-12:])\n",
    "    # print(tee.sum(), q.sum())\n",
    "\n",
    "    \n",
    "    tee.shape\n",
    "\n",
    "\n",
    "    temp = compute_jaccard_similarity(ev,common_patterns.T) # [L,M],[M,C]->[L,C]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    temp=np.argmax(temp,axis=1) # [L]\n",
    "    temp.shape\n",
    "    temp\n",
    "\n",
    "    vec_map = temp\n",
    "\n",
    "    mat_map_1h = np.zeros((m,N_CLUSTERS),dtype= np.int8) # [L,C]\n",
    "    mat_map_1h[np.arange(m),vec_map]=1\n",
    "    # print(mat_map_1h.shape)    # [L,N_clusters]\n",
    "\n",
    "    # IMPORTANT\n",
    "    mat_iplus1 = np.roll(mat_map_1h.T,-1) # [C,L]\n",
    "    mat_iplus1[:,-1]=0\n",
    "\n",
    "    # mat_iplus1 = mat_map_1h.T\n",
    "\n",
    "    tee_mapped = mat_iplus1 @ tee @ mat_map_1h  # tee_mapped(i,j): influence of j-th pattern on i-th pattern\n",
    "    tee_mapped.shape\n",
    "\n",
    "    tee_mapped\n",
    "\n",
    "    # NEW method\n",
    "    a=np.einsum('ab,cd->adbc',mat_iplus1,mat_map_1h) # [C,L] [L,C]-> [C,C,L,L]\n",
    "    # q=np.tril(np.ones((m,m),dtype=np.int8)) # [L,L]\n",
    "    # q=(tee>0).astype(np.int8)\n",
    "    norm=np.sum(  np.einsum('ij,adij->adij',q,a)   ,(-1,-2)) # [C,C]\n",
    "    # norm=np.einsum('ij,adij->ad',q,a)    # [C,C]\n",
    "\n",
    "    # OLD method\n",
    "    sum1=mat_iplus1.sum(1)  # shape [N_CLUSTER]\n",
    "    sum2=mat_map_1h.sum(0)  # shape [N_CLUSTER]\n",
    "    # norm = np.outer(sum1,sum2)\n",
    "    # print(sum1,sum2)\n",
    "    # print(norm.shape, norm)\n",
    "\n",
    "    \n",
    "    # print(((tee_mapped[:6,:6]-norm[:6,:6])>0).sum())\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (((tee_mapped[:6,:6]-norm[:6,:6])>0).sum()):\n",
    "        print(tee_mapped[:6,:6])\n",
    "        print(norm[:6,:6])\n",
    "        print(i_b,i)\n",
    "        term\n",
    "    \n",
    "    return tee_mapped, norm\n",
    "\n",
    "def plot_heatmap_att_agg(df,out,point_ids,fig=None, event_labels=None):\n",
    "\n",
    "    # common_patterns,_ = compute_common_patterns(out1,opt1,N_CLUSTERS=10)\n",
    "\n",
    "    NORM_TH = 0.1\n",
    "\n",
    "    if fig is None:\n",
    "        fig=go.Figure()\n",
    "\n",
    "    att_maps=[]\n",
    "    att_norms=[]\n",
    "    for pid in point_ids:\n",
    "\n",
    "        i_b = int(df.iloc[pid]['i_b'])\n",
    "        i = int(df.iloc[pid]['i'])\n",
    "\n",
    "        tee_mapped, norm = att_map(out,i_b,i,common_patterns)\n",
    "        att_maps.append(tee_mapped  )\n",
    "        att_norms.append(norm  )\n",
    "\n",
    "    norms_matrix = sum(att_norms)\n",
    "    \n",
    "    id_unwanted = (norms_matrix/len(point_ids)<NORM_TH)\n",
    "    # norms_matrix[norms_matrix<0.5]=1.0001\n",
    "    agg_matrix = sum(att_maps)/norms_matrix\n",
    "  \n",
    "    norms_matrix = norms_matrix/len(point_ids)\n",
    "\n",
    "    agg_matrix[id_unwanted]=0\n",
    "    agg_matrix[agg_matrix<0.1]=0\n",
    "\n",
    "\n",
    "\n",
    "    # agg_matrix[agg_matrix<0.5]=0\n",
    "\n",
    "    if len(out['event_type_list'][0].shape)==2: # if multi_class\n",
    "        patt_str = [f'P {i+1}' for i in range(common_patterns.shape[0])]\n",
    "    else:\n",
    "        patt_str = [''.join(str(cell) for cell in row) for row in common_patterns]\n",
    "\n",
    "    patt_str = [f'P{i+1}' for i in range(common_patterns.shape[0])]\n",
    "\n",
    "    fig_agg_mat = px.imshow(agg_matrix, x=patt_str, y=patt_str,\n",
    "    \n",
    "        color_continuous_scale=[\n",
    "        (0, \"#f0f1fc\"),\n",
    "        (0.1, \"#f0f1fc\"),\n",
    "\n",
    "        # (0.001/np.max(agg_matrix), \"#FFFFFF\"),\n",
    "        # (0.001/np.max(agg_matrix), \"#A9D3FF\"),\n",
    "        (0.5, \"#a7b1fa\"),\n",
    "\n",
    "        (1, \"#021ff7\")] )\n",
    "\n",
    "    # fig_agg_mat = go.Figure(\n",
    "    #                 data=go.Heatmap(\n",
    "    #                 z=agg_matrix,#x=patt_str, y=patt_str,\n",
    "    #                 line=dict(width=1, color='white')\n",
    "    #                     )\n",
    "    #             )\n",
    "\n",
    "    perc_pos = (df.iloc[point_ids]['color_true']=='Positive Samples').sum()/len(point_ids)*100\n",
    "    fig_agg_mat.update_layout(\n",
    "        title=f\"per: {perc_pos}\",\n",
    "        # title=\"Plot Title\",\n",
    "        # xaxis_title=\"X Axis Title\",\n",
    "        # yaxis_title=\"Y Axis Title\",\n",
    "        # legend_title=\"Legend Title\",\n",
    "        font=dict(\n",
    "            # family=\"Courier New, monospace\",\n",
    "            size=10,\n",
    "            # color=\"RebeccaPurple\"\n",
    "        )\n",
    "    )\n",
    "    fig_agg_mat.update_xaxes(tickangle=-45)\n",
    "\n",
    "    \n",
    "\n",
    "    # norms_matrix = norms_matrix.astype(int)\n",
    "    # norms_matrix[norms_matrix<1] = 0 \n",
    "    fig_freqs = px.imshow((norms_matrix), x=patt_str, y=patt_str,\n",
    "                          \n",
    "     color_continuous_scale=[(0, \"#FFFFFF\"),\n",
    "    #  (1, \"#FFFFFF\"),     \n",
    "    #  (NORM_TH/np.max(norms_matrix), \"#A9C8FF\"),\n",
    "    #  (0.75, \"#176BFF\"),\n",
    "    # (np.median(norms_matrix)/np.max(norms_matrix), \"#a7b1fa\"),\n",
    "    (0.9, \"#f25252\"), # \n",
    "\n",
    "     (1, \"#FF3D17\")]) # \n",
    "    fig_freqs.update_xaxes(tickangle=-45)\n",
    "    fig_freqs.update_layout(\n",
    "        # title=\"Plot Title\",\n",
    "        # xaxis_title=\"X Axis Title\",\n",
    "        # yaxis_title=\"Y Axis Title\",\n",
    "        # legend_title=\"Legend Title\",\n",
    "        font=dict(\n",
    "            # family=\"Courier New, monospace\",\n",
    "            size=10,\n",
    "            # color=\"RebeccaPurple\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    fig_patterns = px.imshow(common_patterns, x=event_labels, y=patt_str,\n",
    "                             width=600,height=600,\n",
    "     color_continuous_scale=px.colors.sequential.Blues)\n",
    "    fig_patterns.update_xaxes(tickangle=-45)\n",
    "    fig_patterns.update_layout(\n",
    "        # title=\"Plot Title\",\n",
    "        xaxis_title=\"X Axis Title\",\n",
    "        # yaxis_title=\"Y Axis Title\",\n",
    "        # legend_title=\"Legend Title\",\n",
    "        font=dict(\n",
    "            # family=\"Courier New, monospace\",\n",
    "            size=10,\n",
    "            # color=\"RebeccaPurple\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    fig_agg_mat.write_image(\"local/images/fig_agg_mat.svg\")\n",
    "    fig_freqs.write_image(\"local/images/fig_freqs.svg\")\n",
    "    fig_patterns.write_image(\"local/images/fig_patterns.svg\")\n",
    "\n",
    "    return fig_agg_mat, fig_freqs, fig_patterns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## common patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_common_patterns(out,opt,N_CLUSTERS=20):\n",
    "\n",
    "\n",
    "    \n",
    "    if opt.data_label=='multiclass':\n",
    "        common_patterns=np.eye(opt.num_marks,dtype=np.int8)\n",
    "        fig_clustering = None\n",
    "    else:\n",
    "\n",
    "\n",
    "        all_events=np.concatenate(out['event_type_list'],axis=1) # [B,LLLLLL,K] or [B,LLLL]\n",
    "        masks=all_events.sum(-1)>0\n",
    "\n",
    "\n",
    "\n",
    "        TSNE_LIMIT=16000\n",
    "        X = all_events[masks][:TSNE_LIMIT]\n",
    "\n",
    "        X_str = [''.join(str(cell) for cell in row) for row in X]\n",
    "\n",
    "        df=pd.DataFrame()\n",
    "        df['val']=list(X)\n",
    "        df['str']=X_str\n",
    "\n",
    "        df_temp = df.groupby('str').size().sort_values(ascending=False).head(N_CLUSTERS).reset_index(name='count')\n",
    "        \n",
    "        df_temp = df_temp.merge(df.drop_duplicates(subset='str'),how='inner',on='str')\n",
    "        common_patterns = np.stack(df_temp['val'].values)\n",
    "    #     # kmeans clustering\n",
    "    #     # Generate sample binary data\n",
    "    #     data = X\n",
    "\n",
    "    #     # Initialize KMeans object with 2 clusters\n",
    "    #     kmeans = KMeans(n_clusters=N_CLUSTERS)\n",
    "\n",
    "    #     # Fit the data to the KMeans object\n",
    "    #     _ = kmeans.fit(data)\n",
    "\n",
    "    #     # Get the labels and cluster centers\n",
    "    #     labels = kmeans.labels_\n",
    "    #     centers = kmeans.cluster_centers_\n",
    "\n",
    "    #     labels\n",
    "\n",
    "\n",
    "\n",
    "    #     df['cluster_kmeans']=labels\n",
    "\n",
    "    #     # # TSNE\n",
    "    #     # tsne = TSNE(n_components=2, perplexity=60, learning_rate=10,n_jobs=4)\n",
    "    #     # X_tsne = tsne.fit_transform(X[:TSNE_LIMIT,:])\n",
    "    #     # df['x']=X_tsne[:,0]\n",
    "    #     # df['y']=X_tsne[:,1]\n",
    "\n",
    "    #     # fig_clustering=px.scatter(df,x='x',y='y',color='cluster_kmeans',hover_data=[\"str\"])\n",
    "    #     fig_clustering=None\n",
    "\n",
    "\n",
    "    #     # finding common patterns\n",
    "    #     METHOD = 'cluster_kmeans'\n",
    "    #     common_patterns = []\n",
    "\n",
    "    #     for i in range(N_CLUSTERS):\n",
    "    #         temp = df[df[METHOD]==i]['val'].values\n",
    "    #         temp = (temp.sum()/temp.shape[0]*100).astype(int)\n",
    "\n",
    "    #         temp[temp<30]=0\n",
    "    #         temp[temp>=30]=1\n",
    "    #         print(temp,'\\n')\n",
    "    #         common_patterns.append(temp)\n",
    "\n",
    "    #     common_patterns = np.array(common_patterns)\n",
    "    # common_patterns.shape\n",
    "\n",
    "\n",
    "\n",
    "    fig_clustering = None\n",
    "    return common_patterns, fig_clustering\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hokarami/TEEDAM_supervised/56ecgyrq'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs_path=[\n",
    "# \"hokarami/TEEDAM_supervised/5uzxzoxd\",\n",
    "\"hokarami/TEEDAM_supervised/sb9zf1mm\",\n",
    "\"hokarami/TEEDAM_supervised/hue1qxw4\",  # seft [Q10-TE__nextmark-concat]1566371 with label\n",
    "\n",
    "# \"hokarami/TEEDAM_supervised/t43m0t0u\", # [Q10-DA__base-concat]1563864\n",
    "\n",
    "\"hokarami/TEEDAM_supervised/9gxq3dgy\", # [Q10-TEDA__nextmark-concat]1577576 with label\n",
    "\n",
    "\"hokarami/TEEDAM_supervised/56ecgyrq\", # [Q20-TEDA__nextmark-concat]1588433\n",
    "# \"hokarami/TEEDAM_supervised/g4x0ibmk\", # [Q20-TE__nextmark-concat]1585936 with label\n",
    "\n",
    "# \"hokarami/TEEDAM_supervised/lzvf7erg\", # [Q20-DA__base-concat]1586915\n",
    "\n",
    "]\n",
    "\n",
    "run_path = runs_path[-1]\n",
    "run_path\n",
    "\n",
    "\n",
    "# run = api.run(run_path)\n",
    "# for file in run.files():\n",
    "#     file.download(replace=True,root='./local/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.keys()\n",
    "# bs = opt.batch_size\n",
    "\n",
    "\n",
    "# out['event_type_list'][0].shape\n",
    "# out['event_time_list'][0].shape\n",
    "# out['non_pad_mask_list'][0].shape\n",
    "# out['list_TE_att'][0].shape\n",
    "\n",
    "# out['state_mod_list'][0].shape\n",
    "# out['list_DAM_att'][0].shape\n",
    "\n",
    "# out['y_state_true'].shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_matrix = state2mat(st, sm,n_state_mods)\n",
    "\n",
    "# # binary_matrix_to_image(binary_matrix, row_labels=state_labels, font_path=None, grid_size=10, border_size=1, label_size=8)\n",
    "\n",
    "\n",
    "# M = [state2mat(st, sm,n_state_mods) +  att_state2mat(st, sm, n_state_mods, dam_att.sum(1))*0,\n",
    "     \n",
    "#      att_event2mat(ev, t,P, tee_att)*12\n",
    "     \n",
    "#      ]\n",
    "\n",
    "# all_labels = [*state_labels,\n",
    "#               *[f\"Head_{i}\" for i in range(tee_att.shape[0])]\n",
    "#               ]\n",
    "# binary_matrix = np.concatenate(M, axis=0)\n",
    "\n",
    "# # binary_matrix_to_image(binary_matrix, row_labels=all_labels, font_path=None, grid_size=10, border_size=1, label_size=8)\n",
    "\n",
    "\n",
    "# # binary_matrix = event2mat(ev,t,P)\n",
    "# # event_labels = opt.dict_map_events.keys()\n",
    "\n",
    "# # binary_matrix_to_image(binary_matrix, row_labels=None, font_path=None, grid_size=10, border_size=1, label_size=8)\n",
    "\n",
    "\n",
    "# binary_matrix = att_state2mat(st, sm, n_state_mods, dam_att.sum(1))\n",
    "# img_att = binary_matrix_to_image(binary_matrix, row_labels=state_labels, font_path=None, grid_size=10, border_size=1, label_size=8,is_fill=False,**rgb_params_att)\n",
    "\n",
    "\n",
    "# binary_matrix = att_state2mat(st, sm, n_state_mods, sv)\n",
    "# img_val = binary_matrix_to_image(binary_matrix, row_labels=state_labels, font_path=None, grid_size=10, border_size=1, label_size=8,is_fill=True,**rgb_params_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # binary_matrix[binary_matrix!=0].shape\n",
    "# # binary_matrix.shape\n",
    "# # binary_matrix.min()\n",
    "# # binary_matrix.max()\n",
    "\n",
    "# i_b=3\n",
    "# i=18\n",
    "\n",
    "# merged_img,temp = cool_image(out,i_b,i,opt)\n",
    "\n",
    "# temp[2][0]\n",
    "# temp[2][1]\n",
    "# temp[0]\n",
    "# merged_img\n",
    "\n",
    "# map_to_rgb(-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'y_state_true' in out:\n",
    "\n",
    "#     y_state_pred = out['y_state_pred']\n",
    "#     y_state_true = out['y_state_true']\n",
    "#     y_state_score = out['y_state_score']\n",
    "# else:\n",
    "#     y_state_pred = None\n",
    "#     y_state_true = None\n",
    "#     y_state_score = None\n",
    "\n",
    "# y_pred = out['y_pred']\n",
    "# y_true = out['y_true']\n",
    "# y_score = out['y_score']\n",
    "\n",
    "\n",
    "# res=list()\n",
    "\n",
    "\n",
    "# if len(out['state_mod_list'])>0 and 0:\n",
    "\n",
    "#     res_labels = opt.dict_map_states.keys()\n",
    "\n",
    "#     n_cols = len(opt.dict_map_states.keys())\n",
    "\n",
    "#     state_mod_list = out['state_mod_list']\n",
    "#     state_time_list = out['state_time_list']\n",
    "#     len(state_mod_list)\n",
    "#     state_mod_list[0].shape\n",
    "\n",
    "#     for state_mod,state_time in zip(state_mod_list,state_time_list):\n",
    "#         xs = torch.unbind(state_time,0)\n",
    "#         ys = torch.unbind(state_mod,0)\n",
    "\n",
    "#         for x,y in zip(xs,ys):\n",
    "#             # y = y[y~=0]\n",
    "#             n_rows = x.int().max().item() + 1\n",
    "#             matrix = torch.zeros(n_rows, n_cols)\n",
    "#             matrix[x[y!=0].long(), y[y!=0]-1] = 1\n",
    "#             res.append(matrix.cpu().numpy().transpose()) # [n_marks * times]\n",
    "\n",
    "\n",
    "# else:\n",
    "#     non_pad_mask_list = out['non_pad_mask_list']\n",
    "#     event_type_list = out['event_type_list']\n",
    "#     event_time_list = out['event_time_list']\n",
    "\n",
    "#     event_type_list[0].shape\n",
    "#     non_pad_mask_list[0].shape\n",
    "\n",
    "#     num_marks = event_type_list[0].shape[-1]\n",
    "#     res_labels = opt.dict_map_events.keys()\n",
    "\n",
    "\n",
    "\n",
    "#     for event_type, event_time, non_pad_mask in zip(event_type_list,event_time_list,non_pad_mask_list):\n",
    "\n",
    "#         B = event_type.shape[0]\n",
    "#         for i in range(B):\n",
    "#             m = non_pad_mask[i]\n",
    "#             e=event_type[i,:m.sum().int(),:]\n",
    "#             t=event_time[i,:m.sum().int()]\n",
    "            \n",
    "#             indices = e.nonzero()\n",
    "#             indices[:,0] = t[indices[:,0]]\n",
    "\n",
    "#             M = torch.zeros((indices[:,0].max()+1, e.shape[-1]))\n",
    "#             M[indices[:,0],indices[:,1]]=1\n",
    "\n",
    "\n",
    "#             res.append( M.cpu().numpy().astype(np.uint8).transpose() )\n",
    "\n",
    "\n",
    "#         # temp = torch.unbind(event_type,0)\n",
    "#         # lens = non_pad_mask.sum(1).long()\n",
    "#         # term\n",
    "#         # for i,x in enumerate(temp):\n",
    "#         #     res.append( x[:lens[i],:].cpu().numpy().astype(np.uint8).transpose() )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# res[1].shape\n",
    "\n",
    "# len(res_labels)\n",
    "# res_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # for i,pattern in enumerate(res):\n",
    "# #     image = Image.fromarray(pattern)\n",
    "\n",
    "\n",
    "# #     # row_labels = opt.dict_map_states.keys()\n",
    "# #     # image = binary_matrix_to_image(pattern, row_labels=row_labels, grid_size=50, border_size=2, label_size=20)\n",
    "# #     # image.save(f'./local/images/img{i}.jpeg')\n",
    "# all_images = [f'./local/images/img{i}.jpeg' for i in range(len(res))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE of Learned Rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE_LIMIT = 6000\n",
    "\n",
    "def compute_tsne(r_enc_list, model, TSNE_LIMIT=6000):\n",
    "    X_tsne = dict()\n",
    "    tsne = TSNE(n_components=2, perplexity=30, learning_rate=10,n_jobs=4)\n",
    "\n",
    "    # r_enc_list = out['r_enc_list']\n",
    "\n",
    "    X = np.concatenate(r_enc_list,axis=0)[:,:]\n",
    "    X_tsne_full = tsne.fit_transform(X[:TSNE_LIMIT,:])\n",
    "    X_tsne.update({'full': X_tsne_full})\n",
    "    X_tsne_split=dict()\n",
    "    if model.d_out_te>0:\n",
    "        X_te = np.concatenate(r_enc_list,axis=0)[:,:model.d_out_te]\n",
    "        X_te_tsne = tsne.fit_transform(X_te[:TSNE_LIMIT,:])\n",
    "        X_tsne_split['tee']=X_te_tsne\n",
    "        X_tsne.update({'tee': X_te_tsne})\n",
    "\n",
    "    if model.d_out_dam>0:    \n",
    "        X_dam = np.concatenate(r_enc_list,axis=0)[:,model.d_out_te:]\n",
    "        X_dam_tsne = tsne.fit_transform(X_dam[:TSNE_LIMIT,:])\n",
    "        X_tsne_split['dam']=X_dam_tsne\n",
    "        X_tsne.update({'dam': X_dam_tsne})\n",
    "\n",
    "    return X_tsne"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_df(out,opt, X_tsne=None, TSNE_LIMIT=5000):\n",
    "    if 'y_state_true' in out:\n",
    "\n",
    "        y_state_pred = out['y_state_pred']\n",
    "        y_state_true = out['y_state_true']\n",
    "        y_state_score = out['y_state_score']\n",
    "    else:\n",
    "        y_state_pred = None\n",
    "        y_state_true = None\n",
    "        y_state_score = None\n",
    "\n",
    "    y_pred = out['y_pred']\n",
    "    y_true = out['y_true']\n",
    "    y_score = out['y_score']\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    if X_tsne is not None:\n",
    "        df['x']=X_tsne[:,0]\n",
    "        df['y']=X_tsne[:,1]\n",
    "    else:\n",
    "        df['x'] = y_state_true\n",
    "        TSNE_LIMIT=100000000\n",
    "\n",
    "    # df['x']=X_te_tsne[:,0]\n",
    "    # df['y']=X_te_tsne[:,1]\n",
    "\n",
    "    # df['x']=X_dam_tsne[:,0]\n",
    "    # df['y']=X_dam_tsne[:,1]\n",
    "\n",
    "    df['color']=0\n",
    "    df['id']=np.arange(len(df))\n",
    "\n",
    "\n",
    "    if y_state_true is not None:\n",
    "\n",
    "        TP = (y_state_true[:TSNE_LIMIT]*y_state_pred[:TSNE_LIMIT])==1\n",
    "        FN = (y_state_true[:TSNE_LIMIT]-y_state_pred[:TSNE_LIMIT])==1\n",
    "        FP = (y_state_true[:TSNE_LIMIT]-y_state_pred[:TSNE_LIMIT])==-1\n",
    "\n",
    "        TN = (y_state_true[:TSNE_LIMIT]+y_state_pred[:TSNE_LIMIT])==0\n",
    "\n",
    "        FP_FN = (y_state_true[:TSNE_LIMIT]+y_state_pred[:TSNE_LIMIT])==1\n",
    "        TP_TN = (y_state_true[:TSNE_LIMIT]-y_state_pred[:TSNE_LIMIT])==0\n",
    "\n",
    "\n",
    "        # df.loc[TN, 'color']='True Negatives'\n",
    "        df.loc[TP, 'color']='True Positives'\n",
    "        df.loc[FN, 'color']='False Negatives'\n",
    "        df.loc[FP, 'color']='False Positives'\n",
    "        df.loc[TN, 'color']='True Negatives'\n",
    "\n",
    "        df.loc[TP_TN, 'color_true_pred']='True Predicted'\n",
    "        df.loc[FP_FN, 'color_true_pred']='False Predicted'\n",
    "\n",
    "\n",
    "        df.loc[y_state_true[:TSNE_LIMIT].astype(bool).flatten(), 'color_true']='Positive Samples'\n",
    "        df.loc[~y_state_true[:TSNE_LIMIT].astype(bool).flatten(), 'color_true']='Negative Samples'\n",
    "\n",
    "\n",
    "        df.loc[y_state_pred[:TSNE_LIMIT].astype(bool).flatten(), 'color_pred']='Positive Predicted'\n",
    "        df.loc[~y_state_pred[:TSNE_LIMIT].astype(bool).flatten(), 'color_pred']='Negative Predicted'\n",
    "\n",
    "        # df.loc[y_state_pred[:TSNE_LIMIT].astype(bool).flatten(), 'color_true_pred']='Positive Predicted'\n",
    "        # df.loc[~y_state_pred[:TSNE_LIMIT].astype(bool).flatten(), 'color_true_pred']='Negative Predicted'\n",
    "\n",
    "\n",
    "    df['i_b'] = df['id'].apply(lambda x:int(x / opt.batch_size) )\n",
    "    df['i'] = df['id'].apply(lambda x:x % opt.batch_size)\n",
    "\n",
    "    t_max = np.concatenate( [t.max(1)[0] for t in out['event_time_list']] )\n",
    "\n",
    "    df['color_t_max'] = t_max\n",
    "    if len(out['list_log_sum'])>0:\n",
    "        df['color_log_sum'] = np.concatenate(out['list_log_sum'],axis=0) / t_max\n",
    "        df['color_integral_'] = np.concatenate(out['list_integral_'],axis=0) / t_max\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    'Positive Samples': \"#E52B50\",\n",
    "    'Negative Samples': \"#3B7A57\",\n",
    "    'True Predicted': \"#E52B50\",\n",
    "    'False Predicted': \"#3B7A57\",\n",
    "\n",
    "    \n",
    "    0: \"#3B7A57\",\n",
    "    # 'Positive Predicted': \"#3DDC84\",\n",
    "    # 'Negative Predicted': \"#FFBF00\",\n",
    "\n",
    "    'Positive Predicted': \"#E52B50\",\n",
    "    'Negative Predicted': \"#3B7A57\",\n",
    "\n",
    "    5: \"#915C83\",\n",
    "    'True Positives': \"#008000\",\n",
    "    'False Negatives': \"#7FFFD4\",\n",
    "    'False Positives': \"#E9D66B\",\n",
    "    'True Negatives': \"#007FFF\",\n",
    "}\n",
    "\n",
    "def plot_tsne(df, title=\"\"):\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    if 'color_true' in df:\n",
    "        labels = df['color_true'].values\n",
    "        colors = [color_map[label] for i,label in enumerate(labels)]\n",
    "\n",
    "        _ = fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['x'],\n",
    "                y=df['y'],\n",
    "                # z=tsne[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    # size=2,\n",
    "                    color=colors,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        )\n",
    "    else:\n",
    "        labels = df['color'].values\n",
    "        colors = [color_map[label] for i,label in enumerate(labels)]\n",
    "\n",
    "        _ = fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['x'],\n",
    "                y=df['y'],\n",
    "                # z=tsne[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    # size=2,\n",
    "                    color=colors,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        )\n",
    "\n",
    "    if 'color_pred' in df:\n",
    "        labels = df['color_pred'].values\n",
    "        colors = [color_map[label] for i,label in enumerate(labels)]\n",
    "        _ = fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['x'],\n",
    "                y=df['y'],\n",
    "                # z=tsne[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    symbol='circle-open',\n",
    "                    # size=10,\n",
    "                    color=colors,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        )\n",
    "\n",
    "        \n",
    "    if 'color_integral_' in df:\n",
    "        colors = df['color_integral_'].values\n",
    "\n",
    "        _ = fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['x'],\n",
    "                y=df['y'],\n",
    "                # z=tsne[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    # size=8,\n",
    "                    color=colors,\n",
    "                    colorbar=dict(\n",
    "                        title=\"cif_integral\"\n",
    "                    ),\n",
    "                    colorscale=\"RdBu\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        )\n",
    "\n",
    "    if 'color_t_max' in df:\n",
    "        colors = df['color_t_max'].values\n",
    "\n",
    "        _ = fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['x'],\n",
    "                y=df['y'],\n",
    "                # z=tsne[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    # size=8,\n",
    "                    color=colors,\n",
    "                    colorbar=dict(\n",
    "                        title=\"t_max\"\n",
    "                    ),\n",
    "                    colorscale=\"RdBu\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        )\n",
    "\n",
    "    _=fig.update_layout(\n",
    "        # autosize=False,\n",
    "        title=title,\n",
    "        width=600,\n",
    "        height=600,\n",
    "        showlegend=True,\n",
    "\n",
    "    )\n",
    "    _=fig.update_traces(\n",
    "        hoverinfo=\"none\",\n",
    "        hovertemplate=None,\n",
    "    )\n",
    "    # _=fig.update_layout(\n",
    "    #     scene=dict(\n",
    "    #         xaxis=dict(range=[-10,10]),\n",
    "    #         yaxis=dict(range=[-10,10]),\n",
    "    #         # zaxis=dict(range=[-10,10]),   \n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # save to run\n",
    "    # fig.write_html(f\"local/{run.name}.html\")\n",
    "    # run.upload_file(f\"local/{run.name}.html\")\n",
    "\n",
    "\n",
    "    # tsne of TE_nextmark[label]\n",
    "        # https://storage.googleapis.com/wandb-production.appspot.com/hokarami/TEEDAM_supervised/hue1qxw4/local/temp.html?Expires=1676986614&GoogleAccessId=wandb-production%40appspot.gserviceaccount.com&Signature=xLOuNCu8qoWVwFRPCspoPKTFbpF5aOSJMAFICqsDRYxf2wfudxEiFFEIlXBIq9YyYZcPtpSKSzYb6nUr0lloLZ6GHg1w4AOBZyieBKBsuKmIDGqhRY4ojK0FPHZHD%2BeoKERgjD42F19vP8E5Qf%2BA7PlgB2E%2BWutwJxx1sc2TtCgjUdkEK%2BEwaTzfQBQ0hXGIWC9MeirU7hRSmn4%2BXzIRaRcUFjF0vOtxCHBjsEhldGSwaUFv21kt4qvr2hSeR5Ku5gS7webtvzVi6fw55Muq78%2FB90r3EqWqy9Sn68T%2FDKx%2F%2FzHVfge4TsxlIjRE%2F9HBCo7qZeNeeHnlHjVZXdvTKg%3D%3D\n",
    "\n",
    "    # tsne of DA[label]\n",
    "        # https://storage.googleapis.com/wandb-production.appspot.com/hokarami/TEEDAM_supervised/t43m0t0u/local/temp.html?Expires=1676987035&GoogleAccessId=wandb-production%40appspot.gserviceaccount.com&Signature=jX44t4Nblmtzd0CCZC2Gn0cw43iZZbZtuofN%2BrlmzgennSxNz36YE4BwYU%2Ft21iuFdSwmqPLnnI%2B2saLWT9kZsBHNo0e2Nsti0vhf7216iT1wsnnFukn%2B8CswPFeQUQBmSyvEO0IJRTE%2BLmjORdY5NFMdgeXchVwLlf8LBIJFoxAfyiaYLmCNcjWwSmbkvVf1DwOxL2sjneA8TtkgQiSCZiC3GgMJBfh8wg1bf6wS%2Bxa95APq%2BPo6J7%2Ffq0sqmqCiAp28E309PlM29C0C89y63dvzqkRhBhvdffx3O%2FFUAmK%2FjRF82ohd7JSgxMva9oWz%2By7aHESGKr0b49KrUxM2Q%3D%3D\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_dash_app(fig_tsne,out,opt, df,port=None):\n",
    "    if port is None:\n",
    "        port = np.random.randint(2000,5000)\n",
    "\n",
    "    app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "    app.layout = dbc.Container([\n",
    "        # className=\"container\",\n",
    "        # children=[\n",
    "\n",
    "            dbc.Row([\n",
    "                    html.Div(id=\"dummy2\"),\n",
    "                    html.Div(id=\"dummy\"),\n",
    "                ]),\n",
    "\n",
    "            dbc.Row([\n",
    "                \n",
    "                dbc.Col(\n",
    "                    dcc.Graph(id=\"graph-5\", figure=fig_tsne, clear_on_unhover=True,),\n",
    "\n",
    "                    \n",
    "                    width=6),  # first column with graph\n",
    "\n",
    "\n",
    "\n",
    "                dbc.Col([\n",
    "                    html.Img(id='image_merge',src='./local/images/C0-1296.jpeg',style={\"hieght\": \"500px\", 'display': 'block', 'margin': '0 auto'},),\n",
    "\n",
    "                    html.Img(id='image_att',src='./local/images/C0-1296.jpeg',style={\"hieght\": \"500px\", 'display': 'block', 'margin': '0 auto'},),\n",
    "\n",
    "                ], width=6),  # second column with image\n",
    "            ]),\n",
    "\n",
    "            # dbc.Row([\n",
    "            #     dcc.Graph(id=\"graph-cif\", figure=go.Figure(), clear_on_unhover=True,),\n",
    "\n",
    "        \n",
    "            # ]),\n",
    "\n",
    "             dbc.Row([\n",
    "\n",
    "                # fig_agg_mat, fig_freqs, fig_patterns\n",
    "                \n",
    "                dbc.Col(\n",
    "                    dcc.Graph(id=\"fig_agg_mat\", figure=go.Figure(), clear_on_unhover=True,),\n",
    "\n",
    "                    \n",
    "                    width=4),  # first column with graph\n",
    "                dbc.Col(\n",
    "                    dcc.Graph(id=\"fig_freqs\", figure=go.Figure(), clear_on_unhover=True,),\n",
    "\n",
    "                    \n",
    "                    width=4),  # first column with graph\n",
    "                dbc.Col(\n",
    "                    dcc.Graph(id=\"fig_patterns\", figure=go.Figure(), clear_on_unhover=True,),\n",
    "\n",
    "                    \n",
    "                    width=4),  # first column with graph\n",
    "\n",
    "             ]),\n",
    "\n",
    "\n",
    "            dbc.Row([\n",
    "                dcc.Graph(id=\"graph-cif\", figure=go.Figure(), clear_on_unhover=True,),\n",
    "\n",
    "        \n",
    "            ]),\n",
    "\n",
    "            dbc.Row([\n",
    "                \n",
    "               \n",
    "\n",
    "                dbc.Col(\n",
    "                    dcc.Graph(id=\"graph-summary\", figure=go.Figure(), clear_on_unhover=True,),\n",
    "\n",
    "                    \n",
    "                    width=6),  # first column with graph\n",
    "\n",
    "                # dbc.Col(\n",
    "                #     html.Img(id='image',src='./local/images/C0-1296.jpeg',style={\"height\": \"400px\", 'display': 'block', 'margin': '0 auto'},),\n",
    "                \n",
    "                \n",
    "                # width=6),  # second column with image\n",
    "            ]),\n",
    "\n",
    "            dcc.Tooltip(id=\"graph-tooltip-5\", direction='bottom'),\n",
    "\n",
    "\n",
    "        \n",
    "        ])\n",
    "\n",
    "    @app.callback(\n",
    "        # Output(\"graph-tooltip-5\", \"show\"),\n",
    "        # Output(\"graph-tooltip-5\", \"bbox\"),\n",
    "        # Output(\"graph-tooltip-5\", \"children\"),\n",
    "        Output('image_merge', 'src'),\n",
    "        Output('image_att', 'src'),\n",
    "        Output(\"dummy2\", \"children\"),\n",
    "        Output(\"graph-cif\", \"figure\"),\n",
    "        Input(\"graph-5\", \"hoverData\"),\n",
    "    )\n",
    "    def display_hover(hoverData):\n",
    "        if hoverData is None:\n",
    "            # return False, no_update, no_update, no_update, no_update\n",
    "            return no_update,no_update, no_update, no_update\n",
    "\n",
    "        num = 111111\n",
    "        # demo only shows the first point, but other points may also be available\n",
    "        hover_data = hoverData[\"points\"][0]\n",
    "        bbox = hover_data[\"bbox\"]\n",
    "        num = hover_data[\"pointNumber\"]\n",
    "\n",
    "        print(f'NUM:{num}')\n",
    "\n",
    "        # im_matrix = res[num].astype(int)\n",
    "        # # im_url = np_image_to_base64(im_matrix)\n",
    "        # # im_url = binary_matrix_to_image(im_matrix)\n",
    "        # im_url = binary_matrix_to_image(im_matrix, row_labels=res_labels, grid_size=50, border_size=2, label_size=20)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # # bar plot\n",
    "        # vector = im_matrix.sum(1)/im_matrix.shape[1]*24\n",
    "        # # im_url = plot_bar_chart(vector, labels=res_labels)\n",
    "        # im_url = create_barplot_image(vector, res_labels, './local/images/temp.png')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        # NEW\n",
    "        i_b = int(df.iloc[num]['i_b'])\n",
    "        i = int(df.iloc[num]['i'])\n",
    "        im_url, temp = cool_image(out,i_b,i,opt)\n",
    "\n",
    "        \n",
    "        output_str = f\"{num} - {df.iloc[num]['color']} - i_b {i_b} - i {i}\"\n",
    "        \n",
    "        im_url.save(\"./local/images/hover_img.png\")\n",
    "        if temp[0] is not None:\n",
    "            im_url.save(\"./local/images/att.png\")\n",
    "\n",
    "        im_url_path = './local/images/C2-203.jpeg'\n",
    "        \n",
    "        \n",
    "        \n",
    "        children = [\n",
    "            html.Div([\n",
    "                html.Img(\n",
    "                    src=im_url,\n",
    "                    style={\"height\": \"400px\", 'display': 'block', 'margin': '0 auto'},\n",
    "                ),\n",
    "                # html.P(\"MNIST Digit \" + str(labels[num]), style={'font-weight': 'bold'})\n",
    "                html.P(f\"Patterns-id={num} - i_b {i_b} - i {i}\" , style={'font-weight': 'bold'})\n",
    "\n",
    "            ])\n",
    "            \n",
    "        ]\n",
    "\n",
    "        # fig_cif = plot_cif(out,i_b,i)\n",
    "        fig_cif = go.Figure()\n",
    "        # fig_cif = plot_cif_cum(out,i_b,i)\n",
    "\n",
    "\n",
    "        # return True, bbox, children,im_url, output_str\n",
    "        return im_url,temp[0], output_str,fig_cif        # temp[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Define a callback function to print the selected point IDs\n",
    "    # fig_agg_mat, fig_freqs, fig_patterns\n",
    "    @app.callback(\n",
    "        Output(\"dummy\", \"children\"),\n",
    "        Output(\"graph-summary\", \"figure\"), \n",
    "        Output(\"fig_agg_mat\", \"figure\"), \n",
    "        Output(\"fig_freqs\", \"figure\"), \n",
    "        Output(\"fig_patterns\", \"figure\"), \n",
    "\n",
    "        [Input(\"graph-5\", \"selectedData\"), Input(\"graph-summary\", \"figure\")])\n",
    "    def display_selected_data(selected_data, fig_prev):\n",
    "        if selected_data is None:\n",
    "            return \"No points selected.\",no_update,no_update,no_update,no_update\n",
    "        else:\n",
    "\n",
    "\n",
    "            new_fig = go.Figure(data=fig_prev['data'],layout=fig_prev['layout'])\n",
    "            fig_att_agg = go.Figure(data=fig_prev['data'],layout=fig_prev['layout'])\n",
    "\n",
    "\n",
    "            point_ids = [point[\"pointIndex\"] for point in selected_data[\"points\"]]\n",
    "\n",
    "            # new_fig = bar_summary(df,out, point_ids,res_labels, fig=new_fig)\n",
    "            \n",
    "            print(\"point_ids:\\n\",point_ids)\n",
    "            fig_agg_mat, fig_freqs, fig_patterns = plot_heatmap_att_agg(df,out,point_ids,event_labels=list(opt.dict_map_events.keys()))\n",
    "\n",
    "            # # save selected to local\n",
    "            # for pid in point_ids:\n",
    "            #     # NEW\n",
    "            #     i_b = df.iloc[pid]['i_b']\n",
    "            #     i = df.iloc[pid]['i']\n",
    "            #     im_url, temp = cool_image(out,i_b,i,opt)\n",
    "            #     im_url.save(f\"./local/images/selected{pid}.png\")\n",
    "\n",
    "            # print(f\"{point_ids},\\n\")\n",
    "            return f\"{point_ids},\\n\",new_fig,fig_agg_mat, fig_freqs, fig_patterns\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # app.run_server(mode='inline', debug=True)\n",
    "        app.run_server(mode='external',port=port)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEDAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## CUDA True\n",
      "[Info] INPUT DAM --> none-mark-label\n",
      "[Info] parameters: Namespace(data='/mlodata1/hokarami/tedam/p19-raindrop/split3/', data_label='multilabel', cuda=1, wandb=True, wandb_project='TEEDAM_supervised', log_freq=1, prof=False, per=100, balanced_batch=True, transfer_learning='', freeze='', ES_pat=100, setting='raindrop', test_center='', split='3', log='log.txt', user_prefix='[RAIN--DA__base-concat]', pos_alpha=1.0, epoch=25, batch_size=128, lr=0.01, smooth=0.0, weight_decay=0.1, diag_offset=0, event_enc=0, time_enc='concat', te_d_mark=32, te_d_time=16, te_d_rnn=256, te_d_inner=128, te_d_k=32, te_d_v=32, te_n_head=4, te_n_layers=4, te_dropout=0.1, state=True, demo=True, num_states=1, noise=False, mod='none', int_dec='sahp', w_event=1.0, next_mark=1, w_class=False, w_pos=False, mark_detach=1, w_time=1.0, sample_label=True, w_pos_label=0.5, w_sample_label=100.0, hparams2write={'data': '/mlodata1/hokarami/tedam/p19/', 'data_label': 'multilabel', 'cuda': 1, 'wandb': True, 'wandb_project': 'TEEDAM_supervised', 'log_freq': 1, 'prof': False, 'per': 100, 'balanced_batch': True, 'transfer_learning': '', 'freeze': '', 'ES_pat': 100, 'setting': 'raindrop', 'test_center': '', 'split': '3', 'log': 'log.txt', 'user_prefix': '[RAIN--DA__base-concat]', 'pos_alpha': 1.0, 'epoch': 25, 'batch_size': 128, 'lr': 0.01, 'smooth': 0.0, 'weight_decay': 0.1, 'diag_offset': 0, 'event_enc': 0, 'time_enc': 'concat', 'te_d_mark': 32, 'te_d_time': 16, 'te_d_rnn': 256, 'te_d_inner': 128, 'te_d_k': 32, 'te_d_v': 32, 'te_n_head': 4, 'te_n_layers': 4, 'te_dropout': 0.1, 'state': True, 'demo': True, 'num_states': 1, 'noise': False, 'mod': 'none', 'int_dec': 'sahp', 'w_event': 1.0, 'next_mark': 1, 'w_class': False, 'w_pos': False, 'mark_detach': 1, 'w_time': 1.0, 'sample_label': 1, 'w_pos_label': 0.5, 'w_sample_label': 100.0}, date='16-05-23--01-05-13', run_id='2414035', dataset='P19', str_config='-raindrop/split3', run_name='[RAIN--DA__base-concat]2414035', run_path='/mlodata1/hokarami/tedam/p19-raindrop/split3/[RAIN--DA__base-concat]2414035/', device=device(type='cuda'), INPUT='DAM', OUTPUT='none-mark-label')\n",
      "[Info] Loading train data...\n",
      "[Info] Loading dev data...\n",
      "[Info] Loading test data...\n",
      "[info] 100% of data will be considered\n",
      "[Info] Loading train STATE...\n",
      "[Info] Loading dev STATE...\n",
      "[Info] Loading test STATE...\n",
      "[info] 100% of data will be considered\n",
      "[info] True/total = 0.0403\n",
      "[info] balanced mini batches\n",
      "[info] STATE will be considered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "run_path = \"hokarami/TEEDAM_supervised/56ecgyrq\" # [Q20-TEDA__nextmark-concat]1588433\n",
    "run_path = \"hokarami/TEEDAM_supervised/3x52mwgi\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/3x52mwgi/overview?workspace=user-g-hojatkarami\n",
    "run_path = \"hokarami/TEEDAM_unsupervised/ibzmm7wm\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/ibzmm7wm/overview?workspace=\n",
    "# run_path = \"hokarami/TEEDAM_supervised/v1nb7bhq\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/v1nb7bhq/overview?workspace=user-g-hojatkarami\n",
    "run_path = \"hokarami/TEEDAM_unsupervised/jvpugrlq\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/jvpugrlq/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "\n",
    "run_path = \"hokarami/TEEDAM_unsupervised/a95wkhvj\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/a95wkhvj/overview?workspace=user-g-hojatkarami\n",
    "run_path = \"hokarami/TEEDAM_supervised/e2ty765j\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/e2ty765j/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "# h0s0\n",
    "run_path = \"hokarami/TEEDAM_supervised/noufk1a0\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/noufk1a0/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "# seft\n",
    "run_path = \"hokarami/TEEDAM_supervised/g0g8zo1n\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/g0g8zo1n/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "# unsup\n",
    "run_path = \"hokarami/TEEDAM_unsupervised/fyn5cxoi\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/fyn5cxoi/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "run_path = \"hokarami/TEEDAM_supervised/g0g8zo1n\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/g0g8zo1n/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# p19\n",
    "run_path = \"hokarami/TEEDAM_supervised/ah1p114x\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/ah1p114x?workspace=user-g-hojatkarami\n",
    "\n",
    "\n",
    "# p12 unsup TE only\n",
    "run_path = \"hokarami/TEEDAM_unsupervised/agefradk\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/agefradk/overview?workspace=user-g-hojatkarami\n",
    "# run_path = \"hokarami/TEEDAM_unsupervised/00gzdjvg\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/7732nkn7/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "# # p19 sc tedam diag 0\n",
    "# run_path = \"hokarami/TEEDAM_unsupervised/lg21r2z2\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/lg21r2z2/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "# # TEDA freeze TE\n",
    "# run_path = \"hokarami/TEEDAM_supervised/mfn0tkkj\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/347dttsz/overview?workspace=user-g-hojatkarami\n",
    "# # TEDA no freeze\n",
    "# run_path = \"hokarami/TEEDAM_supervised/5ft25axi\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/347dttsz/overview?workspace=user-g-hojatkarami\n",
    "# # DAM baseline\n",
    "# # run_path = \"hokarami/TEEDAM_supervised/jjolnx6e\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/347dttsz/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "# p19 raindrop TE\n",
    "# run_path = \"hokarami/TEEDAM_unsupervised/5jw5p26v\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/5jw5p26v/overview?workspace=user-g-hojatkarami\n",
    "run_path = \"hokarami/TEEDAM_unsupervised/rn1lepki\"\n",
    "\n",
    "\n",
    "# p12 TEDAM sup rain\n",
    "run_path = \"hokarami/TEEDAM_supervised/kn42cnta\"\n",
    "# rain\n",
    "run_path = \"hokarami/TEEDAM_supervised/978ladk1\"\n",
    "# run_path = 'hokarami/TEEDAM_supervised/9qay1a75' # https://wandb.ai/hokarami/TEEDAM_supervised/runs/9qay1a75/overview?workspace=\n",
    "\n",
    "model1, opt1, run1 = read_from_wandb(run_path,consider_sample_labels=True)\n",
    "dict_metrics1, out1 = Main.valid_epoch_tsne(model1, opt1.validloader, opt1.pred_loss_func, opt1)\n",
    "X_tsne1 = compute_tsne(out1['r_enc_list'], model1, TSNE_LIMIT=6000)\n",
    "\n",
    "res_labels = opt1.dict_map_events.keys()\n",
    "\n",
    "df1 = build_df(out1,opt1, X_tsne1['full'],TSNE_LIMIT=100000)\n",
    "fig_tsne1 = plot_tsne(df1,title=run1.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = build_df(out1,opt1, X_tsne1['full'])\n",
    "fig_temp = plot_tsne(df_temp,title=run1.name)\n",
    "fig_temp\n",
    "\n",
    "\n",
    "df_temp = build_df(out1,opt1, X_tsne1['dam'])\n",
    "fig_temp = plot_tsne(df_temp,title=run1.name)\n",
    "fig_temp\n",
    "\n",
    "\n",
    "df_temp = build_df(out1,opt1, X_tsne1['tee'])\n",
    "fig_temp = plot_tsne(df_temp,title=run1.name)\n",
    "fig_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = build_df(out1,opt1, X_tsne1['full'])\n",
    "fig_temp = plot_tsne(df_temp,title=run1.name)\n",
    "fig_temp\n",
    "\n",
    "\n",
    "df_temp = build_df(out1,opt1, X_tsne1['dam'])\n",
    "fig_temp = plot_tsne(df_temp,title=run1.name)\n",
    "fig_temp\n",
    "\n",
    "df_temp = build_df(out1,opt1, X_tsne1['tee'])\n",
    "fig_temp = plot_tsne(df_temp,title=run1.name)\n",
    "fig_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding common patterns\n",
    "common_patterns,fig_clustering = compute_common_patterns(out1,opt1,N_CLUSTERS=10)\n",
    "# common_patterns = common_patterns[np.argsort(common_patterns.sum(1)),:]\n",
    "\n",
    "# fig_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:3321/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM:3803\n",
      "NUM:3057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1207/753296189.py:62: DeprecationWarning:\n",
      "\n",
      "getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM:1846\n",
      "NUM:1322\n",
      "NUM:267\n",
      "NUM:1929\n",
      "NUM:1198\n",
      "NUM:2263\n",
      "NUM:410\n"
     ]
    }
   ],
   "source": [
    "df1 = build_df(out1,opt1, X_tsne1['full'])\n",
    "fig_tsne1 = plot_tsne(df1,title=run1.name)\n",
    "\n",
    "run_dash_app(fig_tsne1,out1,opt1,df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1207/753296189.py:62: DeprecationWarning:\n",
      "\n",
      "getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num= 100\n",
    "i_b = int(df1.iloc[num]['i_b'])\n",
    "i = int(df1.iloc[num]['i'])\n",
    "im_url, temp = cool_image(out1,i_b,i,opt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m point_ids \u001b[39m=\u001b[39m  [\u001b[39m26\u001b[39m, \u001b[39m49\u001b[39m, \u001b[39m126\u001b[39m, \u001b[39m133\u001b[39m, \u001b[39m148\u001b[39m, \u001b[39m156\u001b[39m, \u001b[39m160\u001b[39m, \u001b[39m197\u001b[39m, \u001b[39m202\u001b[39m, \u001b[39m227\u001b[39m, \u001b[39m230\u001b[39m, \u001b[39m371\u001b[39m, \u001b[39m397\u001b[39m, \u001b[39m404\u001b[39m, \u001b[39m405\u001b[39m, \u001b[39m422\u001b[39m, \u001b[39m467\u001b[39m, \u001b[39m485\u001b[39m, \u001b[39m524\u001b[39m, \u001b[39m558\u001b[39m, \u001b[39m560\u001b[39m, \u001b[39m562\u001b[39m, \u001b[39m586\u001b[39m, \u001b[39m644\u001b[39m, \u001b[39m654\u001b[39m, \u001b[39m668\u001b[39m, \u001b[39m691\u001b[39m, \u001b[39m730\u001b[39m, \u001b[39m751\u001b[39m]\n\u001b[1;32m      3\u001b[0m point_ids\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m fig_agg_mat, fig_freqs, fig_patterns \u001b[39m=\u001b[39m plot_heatmap_att_agg(df1,out1,point_ids,event_labels\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(opt1\u001b[39m.\u001b[39;49mdict_map_events\u001b[39m.\u001b[39;49mkeys()))\n",
      "Cell \u001b[0;32mIn[65], line 134\u001b[0m, in \u001b[0;36mplot_heatmap_att_agg\u001b[0;34m(df, out, point_ids, fig, event_labels)\u001b[0m\n\u001b[1;32m    131\u001b[0m i_b \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(df\u001b[39m.\u001b[39miloc[pid][\u001b[39m'\u001b[39m\u001b[39mi_b\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    132\u001b[0m i \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(df\u001b[39m.\u001b[39miloc[pid][\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 134\u001b[0m tee_mapped, norm \u001b[39m=\u001b[39m att_map(out,i_b,i,common_patterns)\n\u001b[1;32m    135\u001b[0m att_maps\u001b[39m.\u001b[39mappend(tee_mapped  )\n\u001b[1;32m    136\u001b[0m att_norms\u001b[39m.\u001b[39mappend(norm  )\n",
      "Cell \u001b[0;32mIn[65], line 39\u001b[0m, in \u001b[0;36matt_map\u001b[0;34m(out, i_b, i, common_patterns)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m tee_mapped, norm\n\u001b[1;32m     38\u001b[0m ev\u001b[39m=\u001b[39mev[:m]\n\u001b[0;32m---> 39\u001b[0m tee \u001b[39m=\u001b[39m out[\u001b[39m'\u001b[39;49m\u001b[39mlist_TE_att\u001b[39;49m\u001b[39m'\u001b[39;49m][i_b][i][:\u001b[39m1\u001b[39m,:m,:m]\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy() \n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mA1\u001b[39m\u001b[39m'\u001b[39m,tee)\n\u001b[1;32m     42\u001b[0m \u001b[39m# tee(i,j): influence of j-th event on (i+1)-th event    \u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "point_ids =  [26, 49, 126, 133, 148, 156, 160, 197, 202, 227, 230, 371, 397, 404, 405, 422, 467, 485, 524, 558, 560, 562, 586, 644, 654, 668, 691, 730, 751]\n",
    "\n",
    "point_ids=[1]\n",
    "\n",
    "fig_agg_mat, fig_freqs, fig_patterns = plot_heatmap_att_agg(df1,out1,point_ids,event_labels=list(opt1.dict_map_events.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x= df1.loc[df1.color_true=='Positive Samples','color_integral_']\n",
    "y= df1.loc[df1.color_true=='Negative Samples','color_integral_']\n",
    "\n",
    "x.mean(), x.std()\n",
    "y.mean(), y.std()\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "t_statistic, p_value = ttest_ind(x, y, alternative='two-sided')\n",
    "# Print the results\n",
    "print('t-statistic =', t_statistic)\n",
    "print('p-value =', p_value)\n",
    "\n",
    "df1['color_integral_'].corr(df1['color_t_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1966/753296189.py:62: DeprecationWarning:\n",
      "\n",
      "getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1 tensor([[0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
      "         0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
      "         0.0152, 0.0152, 0.0152, 0.0152, 0.0152],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8724, 0.1276, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8315, 0.0761, 0.0923, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8495, 0.0377, 0.0542, 0.0586, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8705, 0.0224, 0.0329, 0.0365, 0.0378, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8556, 0.0194, 0.0284, 0.0315, 0.0329, 0.0322, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6519, 0.1068, 0.0554, 0.0502, 0.0467, 0.0429, 0.0459, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7339, 0.0666, 0.0343, 0.0334, 0.0325, 0.0314, 0.0364, 0.0315, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7024, 0.0632, 0.0323, 0.0311, 0.0301, 0.0290, 0.0333, 0.0289, 0.0497,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6531, 0.0696, 0.0356, 0.0333, 0.0311, 0.0294, 0.0322, 0.0292, 0.0431,\n",
      "         0.0434, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5228, 0.0589, 0.0503, 0.0482, 0.0462, 0.0414, 0.0453, 0.0284, 0.0549,\n",
      "         0.0546, 0.0492, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6258, 0.0341, 0.0249, 0.0237, 0.0252, 0.0246, 0.0296, 0.0196, 0.0555,\n",
      "         0.0561, 0.0486, 0.0325, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5913, 0.0635, 0.0352, 0.0324, 0.0294, 0.0275, 0.0288, 0.0270, 0.0327,\n",
      "         0.0326, 0.0286, 0.0399, 0.0311, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5871, 0.0326, 0.0240, 0.0225, 0.0233, 0.0226, 0.0263, 0.0181, 0.0454,\n",
      "         0.0456, 0.0396, 0.0288, 0.0446, 0.0395, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5779, 0.0333, 0.0240, 0.0223, 0.0226, 0.0218, 0.0248, 0.0176, 0.0405,\n",
      "         0.0406, 0.0352, 0.0275, 0.0379, 0.0349, 0.0390, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5646, 0.0225, 0.0184, 0.0173, 0.0186, 0.0182, 0.0217, 0.0138, 0.0412,\n",
      "         0.0414, 0.0355, 0.0241, 0.0417, 0.0355, 0.0436, 0.0420, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5547, 0.0194, 0.0165, 0.0156, 0.0168, 0.0165, 0.0199, 0.0124, 0.0387,\n",
      "         0.0389, 0.0332, 0.0226, 0.0395, 0.0331, 0.0414, 0.0400, 0.0407, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5590, 0.0542, 0.0293, 0.0267, 0.0239, 0.0222, 0.0229, 0.0220, 0.0259,\n",
      "         0.0257, 0.0226, 0.0323, 0.0229, 0.0222, 0.0221, 0.0206, 0.0226, 0.0228,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5537, 0.0305, 0.0211, 0.0194, 0.0191, 0.0183, 0.0203, 0.0149, 0.0310,\n",
      "         0.0309, 0.0267, 0.0233, 0.0277, 0.0264, 0.0282, 0.0268, 0.0276, 0.0275,\n",
      "         0.0265, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5492, 0.0287, 0.0200, 0.0184, 0.0182, 0.0173, 0.0194, 0.0141, 0.0297,\n",
      "         0.0297, 0.0256, 0.0223, 0.0264, 0.0253, 0.0270, 0.0257, 0.0263, 0.0262,\n",
      "         0.0254, 0.0252, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4855, 0.0232, 0.0266, 0.0245, 0.0235, 0.0208, 0.0219, 0.0131, 0.0331,\n",
      "         0.0315, 0.0273, 0.0157, 0.0277, 0.0278, 0.0303, 0.0300, 0.0265, 0.0257,\n",
      "         0.0263, 0.0298, 0.0293, 0.0000, 0.0000],\n",
      "        [0.5247, 0.0244, 0.0176, 0.0162, 0.0162, 0.0155, 0.0174, 0.0122, 0.0278,\n",
      "         0.0277, 0.0238, 0.0195, 0.0247, 0.0236, 0.0253, 0.0241, 0.0245, 0.0244,\n",
      "         0.0237, 0.0238, 0.0237, 0.0392, 0.0000]])\n",
      "aaaa tensor([[0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
      "         0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
      "         0.0152, 0.0152, 0.0152, 0.0152, 0.0152],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8724, 0.1276, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8315, 0.0761, 0.0923, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8495, 0.0377, 0.0542, 0.0586, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8705, 0.0224, 0.0329, 0.0365, 0.0378, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8556, 0.0194, 0.0284, 0.0315, 0.0329, 0.0322, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6519, 0.1068, 0.0554, 0.0502, 0.0467, 0.0429, 0.0459, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7339, 0.0666, 0.0343, 0.0334, 0.0325, 0.0314, 0.0364, 0.0315, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7024, 0.0632, 0.0323, 0.0311, 0.0301, 0.0290, 0.0333, 0.0289, 0.0497,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6531, 0.0696, 0.0356, 0.0333, 0.0311, 0.0294, 0.0322, 0.0292, 0.0431,\n",
      "         0.0434, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5228, 0.0589, 0.0503, 0.0482, 0.0462, 0.0414, 0.0453, 0.0284, 0.0549,\n",
      "         0.0546, 0.0492, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6258, 0.0341, 0.0249, 0.0237, 0.0252, 0.0246, 0.0296, 0.0196, 0.0555,\n",
      "         0.0561, 0.0486, 0.0325, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5913, 0.0635, 0.0352, 0.0324, 0.0294, 0.0275, 0.0288, 0.0270, 0.0327,\n",
      "         0.0326, 0.0286, 0.0399, 0.0311, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5871, 0.0326, 0.0240, 0.0225, 0.0233, 0.0226, 0.0263, 0.0181, 0.0454,\n",
      "         0.0456, 0.0396, 0.0288, 0.0446, 0.0395, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5779, 0.0333, 0.0240, 0.0223, 0.0226, 0.0218, 0.0248, 0.0176, 0.0405,\n",
      "         0.0406, 0.0352, 0.0275, 0.0379, 0.0349, 0.0390, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5646, 0.0225, 0.0184, 0.0173, 0.0186, 0.0182, 0.0217, 0.0138, 0.0412,\n",
      "         0.0414, 0.0355, 0.0241, 0.0417, 0.0355, 0.0436, 0.0420, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5547, 0.0194, 0.0165, 0.0156, 0.0168, 0.0165, 0.0199, 0.0124, 0.0387,\n",
      "         0.0389, 0.0332, 0.0226, 0.0395, 0.0331, 0.0414, 0.0400, 0.0407, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5590, 0.0542, 0.0293, 0.0267, 0.0239, 0.0222, 0.0229, 0.0220, 0.0259,\n",
      "         0.0257, 0.0226, 0.0323, 0.0229, 0.0222, 0.0221, 0.0206, 0.0226, 0.0228,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5537, 0.0305, 0.0211, 0.0194, 0.0191, 0.0183, 0.0203, 0.0149, 0.0310,\n",
      "         0.0309, 0.0267, 0.0233, 0.0277, 0.0264, 0.0282, 0.0268, 0.0276, 0.0275,\n",
      "         0.0265, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5492, 0.0287, 0.0200, 0.0184, 0.0182, 0.0173, 0.0194, 0.0141, 0.0297,\n",
      "         0.0297, 0.0256, 0.0223, 0.0264, 0.0253, 0.0270, 0.0257, 0.0263, 0.0262,\n",
      "         0.0254, 0.0252, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4855, 0.0232, 0.0266, 0.0245, 0.0235, 0.0208, 0.0219, 0.0131, 0.0331,\n",
      "         0.0315, 0.0273, 0.0157, 0.0277, 0.0278, 0.0303, 0.0300, 0.0265, 0.0257,\n",
      "         0.0263, 0.0298, 0.0293, 0.0000, 0.0000],\n",
      "        [0.5247, 0.0244, 0.0176, 0.0162, 0.0162, 0.0155, 0.0174, 0.0122, 0.0278,\n",
      "         0.0277, 0.0238, 0.0195, 0.0247, 0.0236, 0.0253, 0.0241, 0.0245, 0.0244,\n",
      "         0.0237, 0.0238, 0.0237, 0.0392, 0.0000]])\n",
      "tensor([[ 1],\n",
      "        [ 2],\n",
      "        [ 3],\n",
      "        [ 4],\n",
      "        [ 5],\n",
      "        [ 6],\n",
      "        [ 7],\n",
      "        [ 8],\n",
      "        [ 9],\n",
      "        [10],\n",
      "        [11],\n",
      "        [12],\n",
      "        [13],\n",
      "        [14],\n",
      "        [15],\n",
      "        [16],\n",
      "        [17],\n",
      "        [18],\n",
      "        [19],\n",
      "        [20],\n",
      "        [21],\n",
      "        [22],\n",
      "        [23]])\n",
      "A2 tensor([[ 0.0152,  0.0152,  0.0152,  0.0152,  0.0152,  0.0152,  0.0152,  0.0152,\n",
      "          0.0152,  0.0152,  0.0152,  0.0152,  0.0152,  0.0152,  0.0152,  0.0152,\n",
      "          0.0152,  0.0152,  0.0152,  0.0152,  0.0152,  0.0152,  0.0152],\n",
      "        [ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 2.6173,  0.3827,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 3.3262,  0.3046,  0.3692,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 4.2475,  0.1887,  0.2709,  0.2928,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 5.2233,  0.1341,  0.1973,  0.2188,  0.2265,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 5.9890,  0.1361,  0.1985,  0.2205,  0.2301,  0.2257,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 5.2153,  0.8547,  0.4433,  0.4020,  0.3740,  0.3433,  0.3674,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 6.6055,  0.5993,  0.3088,  0.3003,  0.2928,  0.2825,  0.3272,  0.2837,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 7.0242,  0.6324,  0.3226,  0.3107,  0.3015,  0.2896,  0.3333,  0.2891,\n",
      "          0.4967,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 7.1843,  0.7653,  0.3918,  0.3660,  0.3416,  0.3234,  0.3544,  0.3217,\n",
      "          0.4736,  0.4779,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 6.2731,  0.7063,  0.6036,  0.5780,  0.5540,  0.4963,  0.5434,  0.3407,\n",
      "          0.6586,  0.6557,  0.5903,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 8.1355,  0.4430,  0.3233,  0.3075,  0.3275,  0.3203,  0.3845,  0.2544,\n",
      "          0.7212,  0.7291,  0.6318,  0.4219,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 8.2784,  0.8893,  0.4922,  0.4543,  0.4112,  0.3846,  0.4037,  0.3783,\n",
      "          0.4573,  0.4568,  0.3997,  0.5586,  0.4357,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 8.8067,  0.4895,  0.3594,  0.3370,  0.3495,  0.3394,  0.3947,  0.2712,\n",
      "          0.6806,  0.6841,  0.5935,  0.4327,  0.6687,  0.5929,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 9.2461,  0.5327,  0.3845,  0.3571,  0.3620,  0.3490,  0.3975,  0.2809,\n",
      "          0.6482,  0.6495,  0.5626,  0.4406,  0.6071,  0.5582,  0.6240,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 9.5982,  0.3827,  0.3123,  0.2944,  0.3155,  0.3086,  0.3690,  0.2344,\n",
      "          0.7007,  0.7039,  0.6028,  0.4103,  0.7085,  0.6028,  0.7411,  0.7148,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 9.9846,  0.3498,  0.2962,  0.2804,  0.3032,  0.2971,  0.3587,  0.2230,\n",
      "          0.6970,  0.7003,  0.5974,  0.4066,  0.7110,  0.5964,  0.7457,  0.7197,\n",
      "          0.7329,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [10.6217,  1.0296,  0.5575,  0.5069,  0.4534,  0.4214,  0.4355,  0.4171,\n",
      "          0.4919,  0.4892,  0.4297,  0.6138,  0.4357,  0.4225,  0.4205,  0.3916,\n",
      "          0.4287,  0.4334,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [11.0734,  0.6093,  0.4218,  0.3880,  0.3827,  0.3650,  0.4069,  0.2982,\n",
      "          0.6197,  0.6188,  0.5348,  0.4668,  0.5532,  0.5289,  0.5641,  0.5367,\n",
      "          0.5512,  0.5499,  0.5306,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [11.5340,  0.6028,  0.4199,  0.3861,  0.3815,  0.3639,  0.4064,  0.2954,\n",
      "          0.6240,  0.6230,  0.5374,  0.4676,  0.5550,  0.5314,  0.5667,  0.5392,\n",
      "          0.5525,  0.5508,  0.5331,  0.5293,  0.0000,  0.0000,  0.0000],\n",
      "        [10.6799,  0.5111,  0.5855,  0.5390,  0.5165,  0.4570,  0.4819,  0.2880,\n",
      "          0.7274,  0.6939,  0.6000,  0.3461,  0.6096,  0.6126,  0.6655,  0.6594,\n",
      "          0.5827,  0.5654,  0.5797,  0.6548,  0.6440,  0.0000,  0.0000],\n",
      "        [12.0680,  0.5620,  0.4059,  0.3726,  0.3731,  0.3562,  0.4009,  0.2804,\n",
      "          0.6387,  0.6370,  0.5468,  0.4493,  0.5672,  0.5429,  0.5822,  0.5553,\n",
      "          0.5634,  0.5605,  0.5449,  0.5466,  0.5441,  0.9019,  0.0000]])\n",
      "<class 'torch.Tensor'>\n",
      "dafas tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 2.6173,  0.3827,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 3.3262,  0.3046,  0.3692,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 4.2475,  0.1887,  0.2709,  0.2928,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 5.2233,  0.1341,  0.1973,  0.2188,  0.2265,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 5.9890,  0.1361,  0.1985,  0.2205,  0.2301,  0.2257,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 5.2153,  0.8547,  0.4433,  0.4020,  0.3740,  0.3433,  0.3674,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 6.6055,  0.5993,  0.3088,  0.3003,  0.2928,  0.2825,  0.3272,  0.2837,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 7.0242,  0.6324,  0.3226,  0.3107,  0.3015,  0.2896,  0.3333,  0.2891,\n",
      "          0.4967,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 7.1843,  0.7653,  0.3918,  0.3660,  0.3416,  0.3234,  0.3544,  0.3217,\n",
      "          0.4736,  0.4779,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 6.2731,  0.7063,  0.6036,  0.5780,  0.5540,  0.4963,  0.5434,  0.3407,\n",
      "          0.6586,  0.6557,  0.5903,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 8.1355,  0.4430,  0.3233,  0.3075,  0.3275,  0.3203,  0.3845,  0.2544,\n",
      "          0.7212,  0.7291,  0.6318,  0.4219,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 8.2784,  0.8893,  0.4922,  0.4543,  0.4112,  0.3846,  0.4037,  0.3783,\n",
      "          0.4573,  0.4568,  0.3997,  0.5586,  0.4357,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 8.8067,  0.4895,  0.3594,  0.3370,  0.3495,  0.3394,  0.3947,  0.2712,\n",
      "          0.6806,  0.6841,  0.5935,  0.4327,  0.6687,  0.5929,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 9.2461,  0.5327,  0.3845,  0.3571,  0.3620,  0.3490,  0.3975,  0.2809,\n",
      "          0.6482,  0.6495,  0.5626,  0.4406,  0.6071,  0.5582,  0.6240,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 9.5982,  0.3827,  0.3123,  0.2944,  0.3155,  0.3086,  0.3690,  0.2344,\n",
      "          0.7007,  0.7039,  0.6028,  0.4103,  0.7085,  0.6028,  0.7411,  0.7148,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 9.9846,  0.3498,  0.2962,  0.2804,  0.3032,  0.2971,  0.3587,  0.2230,\n",
      "          0.6970,  0.7003,  0.5974,  0.4066,  0.7110,  0.5964,  0.7457,  0.7197,\n",
      "          0.7329,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [10.6217,  1.0296,  0.5575,  0.5069,  0.4534,  0.4214,  0.4355,  0.4171,\n",
      "          0.4919,  0.4892,  0.4297,  0.6138,  0.4357,  0.4225,  0.4205,  0.3916,\n",
      "          0.4287,  0.4334,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [11.0734,  0.6093,  0.4218,  0.3880,  0.3827,  0.3650,  0.4069,  0.2982,\n",
      "          0.6197,  0.6188,  0.5348,  0.4668,  0.5532,  0.5289,  0.5641,  0.5367,\n",
      "          0.5512,  0.5499,  0.5306,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [11.5340,  0.6028,  0.4199,  0.3861,  0.3815,  0.3639,  0.4064,  0.2954,\n",
      "          0.6240,  0.6230,  0.5374,  0.4676,  0.5550,  0.5314,  0.5667,  0.5392,\n",
      "          0.5525,  0.5508,  0.5331,  0.5293,  0.0000,  0.0000,  0.0000],\n",
      "        [10.6799,  0.5111,  0.5855,  0.5390,  0.5165,  0.4570,  0.4819,  0.2880,\n",
      "          0.7274,  0.6939,  0.6000,  0.3461,  0.6096,  0.6126,  0.6655,  0.6594,\n",
      "          0.5827,  0.5654,  0.5797,  0.6548,  0.6440,  0.0000,  0.0000],\n",
      "        [12.0680,  0.5620,  0.4059,  0.3726,  0.3731,  0.3562,  0.4009,  0.2804,\n",
      "          0.6387,  0.6370,  0.5468,  0.4493,  0.5672,  0.5429,  0.5822,  0.5553,\n",
      "          0.5634,  0.5605,  0.5449,  0.5466,  0.5441,  0.9019,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "num=148\n",
    "i_b = int(df1.iloc[num]['i_b'])\n",
    "i = int(df1.iloc[num]['i'])\n",
    "\n",
    "merged_img,(att_mat_img,img_ev),tempp = cool_image(out1,i_b,i,opt1,CROP=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_ids = [3, 42, 56, 68, 133, 140, 209, 222, 265, 293, 302, 307, 336, 362, 368, 369, 430, 494, 549, 582, 597, 648, 650, 674, 730, 3, 42, 56, 68, 133, 140, 209, 222, 265, 293, 302, 307, 336, 362, 368, 369, 430, 494, 549, 582, 597, 648, 650, 674, 730, 3, 42, 56, 68, 133, 140, 209, 222, 265, 293, 302, 307, 336, 362, 368, 369, 430, 494, 549, 582, 597, 648, 650, 674, 730]\n",
    "\n",
    "fig_agg_mat, fig_freqs, fig_patterns = plot_heatmap_att_agg(df1,out1,point_ids,event_labels=list(opt1.dict_map_events.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataVis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## CUDA True\n",
      "[Info] INPUT DAM --> none-mark-label\n",
      "[Info] parameters: Namespace(data='/mlodata1/hokarami/tedam/p19-raindrop/split0/', data_label='multilabel', cuda=1, wandb=True, wandb_project='TEEDAM_supervised', log_freq=1, prof=False, per=100, balanced_batch=True, transfer_learning='', freeze='', ES_pat=100, setting='raindrop', test_center='', split='0', log='log.txt', user_prefix='[RAIN2--DA__base-concat]', pos_alpha=1.0, epoch=50, batch_size=128, lr=0.01, smooth=0.0, weight_decay=0.1, diag_offset=0, event_enc=0, time_enc='concat', te_d_mark=32, te_d_time=16, te_d_rnn=256, te_d_inner=128, te_d_k=32, te_d_v=32, te_n_head=4, te_n_layers=4, te_dropout=0.1, state=True, demo=True, num_states=1, noise=False, mod='none', int_dec='sahp', w_event=1.0, next_mark=1, w_class=False, w_pos=False, mark_detach=1, w_time=1.0, sample_label=True, w_pos_label=0.5, w_sample_label=100.0, hparams2write={'data': '/mlodata1/hokarami/tedam/p19/', 'data_label': 'multilabel', 'cuda': 1, 'wandb': True, 'wandb_project': 'TEEDAM_supervised', 'log_freq': 1, 'prof': False, 'per': 100, 'balanced_batch': True, 'transfer_learning': '', 'freeze': '', 'ES_pat': 100, 'setting': 'raindrop', 'test_center': '', 'split': '0', 'log': 'log.txt', 'user_prefix': '[RAIN2--DA__base-concat]', 'pos_alpha': 1.0, 'epoch': 50, 'batch_size': 128, 'lr': 0.01, 'smooth': 0.0, 'weight_decay': 0.1, 'diag_offset': 0, 'event_enc': 0, 'time_enc': 'concat', 'te_d_mark': 32, 'te_d_time': 16, 'te_d_rnn': 256, 'te_d_inner': 128, 'te_d_k': 32, 'te_d_v': 32, 'te_n_head': 4, 'te_n_layers': 4, 'te_dropout': 0.1, 'state': True, 'demo': True, 'num_states': 1, 'noise': False, 'mod': 'none', 'int_dec': 'sahp', 'w_event': 1.0, 'next_mark': 1, 'w_class': False, 'w_pos': False, 'mark_detach': 1, 'w_time': 1.0, 'sample_label': 1, 'w_pos_label': 0.5, 'w_sample_label': 100.0}, date='10-05-23--10-30-47', run_id='2358036', dataset='P19', str_config='-raindrop/split0', run_name='[RAIN2--DA__base-concat]2358036', run_path='/mlodata1/hokarami/tedam/p19-raindrop/split0/[RAIN2--DA__base-concat]2358036/', device=device(type='cuda'), INPUT='DAM', OUTPUT='none-mark-label')\n",
      "[Info] Loading train data...\n",
      "[Info] Loading dev data...\n",
      "[Info] Loading test data...\n",
      "[info] 100% of data will be considered\n",
      "[Info] Loading train STATE...\n",
      "[Info] Loading dev STATE...\n",
      "[Info] Loading test STATE...\n",
      "[info] 100% of data will be considered\n",
      "[info] True/total = 0.0406\n",
      "[info] balanced mini batches\n",
      "[info] STATE will be considered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "run_path=\"hokarami/TEEDAM_supervised/buhg78q3\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/buhg78q3/files?workspace=user-g-hojatkarami\n",
    "# run_path = \"hokarami/TEEDAM_unsupervised/00gzdjvg\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/7732nkn7/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "model1, opt1, run1 = read_from_wandb(run_path,consider_sample_labels=True)\n",
    "dict_metrics1, out1 = Main.valid_epoch_tsne(model1, opt1.validloader, opt1.pred_loss_func, opt1)\n",
    "X_tsne1 = compute_tsne(out1['r_enc_list'], model1, TSNE_LIMIT=100000)\n",
    "\n",
    "res_labels = opt1.dict_map_events.keys()\n",
    "\n",
    "df1 = build_df(out1,opt1, X_tsne1['full'],TSNE_LIMIT=100000)\n",
    "fig_tsne1 = plot_tsne(df1,title=run1.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28.909"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(list2save[0]['x'],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3840"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[{'x': -28.90871810913086,\n",
       "  'y': -4.1779069900512695,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 36.160892486572266, 'y': -2.510448455810547, 'color': 'True Negatives'},\n",
       " {'x': 35.06202697753906, 'y': -3.6897640228271484, 'color': 'True Negatives'},\n",
       " {'x': -23.869653701782227,\n",
       "  'y': -29.310422897338867,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 14.40906047821045, 'y': 27.157209396362305, 'color': 'True Negatives'},\n",
       " {'x': 2.313821792602539, 'y': -24.062231063842773, 'color': 'True Negatives'},\n",
       " {'x': -16.441301345825195,\n",
       "  'y': 24.689428329467773,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -36.50613784790039,\n",
       "  'y': -20.697689056396484,\n",
       "  'color': 'True Positives'},\n",
       " {'x': 2.802126407623291, 'y': 21.42722511291504, 'color': 'True Negatives'},\n",
       " {'x': -10.80672550201416, 'y': 3.1959879398345947, 'color': 'True Negatives'},\n",
       " {'x': -4.0367255210876465, 'y': 33.63460922241211, 'color': 'True Negatives'},\n",
       " {'x': -10.796721458435059,\n",
       "  'y': -9.677922248840332,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -18.742456436157227,\n",
       "  'y': 16.871286392211914,\n",
       "  'color': 'False Negatives'},\n",
       " {'x': -15.099262237548828,\n",
       "  'y': -27.76582145690918,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -13.833540916442871,\n",
       "  'y': -34.28448486328125,\n",
       "  'color': 'False Negatives'},\n",
       " {'x': -27.32939338684082, 'y': 13.538086891174316, 'color': 'True Negatives'},\n",
       " {'x': 2.9313290119171143,\n",
       "  'y': -26.268905639648438,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -35.12620162963867, 'y': 6.3832783699035645, 'color': 'True Negatives'},\n",
       " {'x': 13.064417839050293, 'y': 19.455015182495117, 'color': 'True Negatives'},\n",
       " {'x': 7.2472124099731445, 'y': 32.88309097290039, 'color': 'True Negatives'},\n",
       " {'x': -10.187586784362793, 'y': 8.282395362854004, 'color': 'True Negatives'},\n",
       " {'x': 36.927528381347656, 'y': 5.908114910125732, 'color': 'True Negatives'},\n",
       " {'x': -2.9878175258636475,\n",
       "  'y': 0.6195128560066223,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 2.38957142829895, 'y': 25.326854705810547, 'color': 'True Negatives'},\n",
       " {'x': -15.496500015258789,\n",
       "  'y': -12.586992263793945,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -2.1955065727233887,\n",
       "  'y': 12.023613929748535,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -22.19227409362793, 'y': 14.275991439819336, 'color': 'True Negatives'},\n",
       " {'x': 27.536779403686523,\n",
       "  'y': -11.349380493164062,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 25.562231063842773, 'y': -5.290048122406006, 'color': 'True Negatives'},\n",
       " {'x': -22.430265426635742,\n",
       "  'y': 24.700851440429688,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -23.44839859008789,\n",
       "  'y': 15.053317070007324,\n",
       "  'color': 'False Negatives'},\n",
       " {'x': 1.279999852180481, 'y': -11.660737037658691, 'color': 'True Negatives'},\n",
       " {'x': -11.13798999786377, 'y': -25.01934051513672, 'color': 'True Negatives'},\n",
       " {'x': 0.5741594433784485,\n",
       "  'y': 0.43697473406791687,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -3.727276563644409, 'y': 19.92912483215332, 'color': 'True Negatives'},\n",
       " {'x': -11.887795448303223,\n",
       "  'y': 19.088842391967773,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 17.24710464477539, 'y': -33.65273666381836, 'color': 'True Negatives'},\n",
       " {'x': 3.3044674396514893, 'y': 21.878028869628906, 'color': 'True Negatives'},\n",
       " {'x': -5.167095184326172,\n",
       "  'y': -33.876609802246094,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 29.627052307128906, 'y': 3.236515760421753, 'color': 'True Negatives'},\n",
       " {'x': 33.13521194458008, 'y': 1.8452789783477783, 'color': 'True Negatives'},\n",
       " {'x': -35.26436233520508, 'y': 10.364800453186035, 'color': 'True Negatives'},\n",
       " {'x': 31.024585723876953, 'y': 19.71866798400879, 'color': 'True Negatives'},\n",
       " {'x': 36.90279769897461, 'y': 5.742397308349609, 'color': 'True Negatives'},\n",
       " {'x': 18.213220596313477,\n",
       "  'y': -26.213577270507812,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 22.759920120239258, 'y': 12.173863410949707, 'color': 'True Negatives'},\n",
       " {'x': 7.932884693145752, 'y': 15.474822044372559, 'color': 'True Negatives'},\n",
       " {'x': 9.83256721496582, 'y': 11.332549095153809, 'color': 'True Negatives'},\n",
       " {'x': 20.475570678710938,\n",
       "  'y': -1.3168230056762695,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 23.321359634399414,\n",
       "  'y': -1.5823074579238892,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 28.914981842041016,\n",
       "  'y': -10.825940132141113,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -19.71727180480957,\n",
       "  'y': -23.617576599121094,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -6.99210262298584, 'y': 34.73807907104492, 'color': 'True Negatives'},\n",
       " {'x': 29.806859970092773, 'y': 2.171470880508423, 'color': 'True Negatives'},\n",
       " {'x': 40.24494934082031, 'y': 2.8247933387756348, 'color': 'True Negatives'},\n",
       " {'x': -5.672816276550293, 'y': 20.439594268798828, 'color': 'True Negatives'},\n",
       " {'x': -10.093311309814453,\n",
       "  'y': -5.975785255432129,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 5.2389020919799805,\n",
       "  'y': -25.186260223388672,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 28.989151000976562,\n",
       "  'y': -10.849125862121582,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 16.39346694946289, 'y': -0.8841321468353271, 'color': 'True Negatives'},\n",
       " {'x': -4.033728122711182, 'y': 19.408756256103516, 'color': 'True Negatives'},\n",
       " {'x': -4.073538780212402, 'y': -5.5924391746521, 'color': 'True Negatives'},\n",
       " {'x': -31.777708053588867,\n",
       "  'y': 17.334341049194336,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -21.786630630493164, 'y': 4.086021423339844, 'color': 'True Negatives'},\n",
       " {'x': -0.8533119559288025,\n",
       "  'y': 25.130151748657227,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -16.599958419799805,\n",
       "  'y': 22.915712356567383,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 28.098636627197266, 'y': 17.29375648498535, 'color': 'True Negatives'},\n",
       " {'x': 19.109346389770508, 'y': 26.177215576171875, 'color': 'True Negatives'},\n",
       " {'x': -5.612908840179443, 'y': 33.10040283203125, 'color': 'True Negatives'},\n",
       " {'x': 0.6959490776062012, 'y': 4.29282808303833, 'color': 'True Positives'},\n",
       " {'x': 33.58868408203125, 'y': 2.176015853881836, 'color': 'True Negatives'},\n",
       " {'x': -27.69268035888672, 'y': 3.8283820152282715, 'color': 'True Negatives'},\n",
       " {'x': -22.794912338256836,\n",
       "  'y': 2.8153603076934814,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -8.014616012573242,\n",
       "  'y': -23.414533615112305,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -0.1319221705198288, 'y': 7.599009037017822, 'color': 'True Negatives'},\n",
       " {'x': 13.0726318359375, 'y': 0.06961940228939056, 'color': 'True Negatives'},\n",
       " {'x': 40.27338790893555,\n",
       "  'y': -0.10270388424396515,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 8.411676406860352, 'y': -21.42320442199707, 'color': 'True Negatives'},\n",
       " {'x': -25.920190811157227,\n",
       "  'y': 14.131875991821289,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 11.490702629089355,\n",
       "  'y': -13.641555786132812,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -13.99447250366211,\n",
       "  'y': -25.609474182128906,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 14.795083999633789,\n",
       "  'y': -3.223688840866089,\n",
       "  'color': 'False Negatives'},\n",
       " {'x': -4.412581920623779, 'y': 20.481237411499023, 'color': 'True Negatives'},\n",
       " {'x': -35.909053802490234,\n",
       "  'y': -19.450899124145508,\n",
       "  'color': 'True Positives'},\n",
       " {'x': 21.672189712524414, 'y': 16.918582916259766, 'color': 'True Negatives'},\n",
       " {'x': 16.810043334960938, 'y': 10.92539119720459, 'color': 'True Negatives'},\n",
       " {'x': 27.96306800842285, 'y': 16.614151000976562, 'color': 'True Negatives'},\n",
       " {'x': 6.847324371337891, 'y': 5.379268169403076, 'color': 'True Negatives'},\n",
       " {'x': 29.85500717163086, 'y': 12.068970680236816, 'color': 'True Negatives'},\n",
       " {'x': 11.731130599975586, 'y': 6.28446102142334, 'color': 'True Negatives'},\n",
       " {'x': -16.524641036987305,\n",
       "  'y': -22.856441497802734,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -18.109569549560547,\n",
       "  'y': 14.094405174255371,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -1.7151978015899658,\n",
       "  'y': -27.640884399414062,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 30.893184661865234, 'y': 18.390005111694336, 'color': 'True Negatives'},\n",
       " {'x': -36.51047897338867, 'y': 10.102056503295898, 'color': 'True Negatives'},\n",
       " {'x': -1.2522779703140259, 'y': 28.83639144897461, 'color': 'True Negatives'},\n",
       " {'x': -32.848201751708984,\n",
       "  'y': -1.5309791564941406,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -20.070697784423828,\n",
       "  'y': 25.558719635009766,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 22.540815353393555, 'y': 3.2677128314971924, 'color': 'True Negatives'},\n",
       " {'x': -7.7136406898498535,\n",
       "  'y': -33.718292236328125,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -26.403322219848633,\n",
       "  'y': 16.405061721801758,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -30.841094970703125,\n",
       "  'y': -12.593396186828613,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 33.21327209472656, 'y': 14.89687442779541, 'color': 'True Negatives'},\n",
       " {'x': 16.93071937561035, 'y': -32.58887481689453, 'color': 'True Negatives'},\n",
       " {'x': -16.98092269897461, 'y': -24.50391387939453, 'color': 'True Negatives'},\n",
       " {'x': -17.518163681030273,\n",
       "  'y': 24.639848709106445,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -8.954208374023438, 'y': -26.09212303161621, 'color': 'True Negatives'},\n",
       " {'x': 15.652410507202148, 'y': 13.196309089660645, 'color': 'True Negatives'},\n",
       " {'x': 41.237762451171875, 'y': 4.73317813873291, 'color': 'True Negatives'},\n",
       " {'x': -37.55732345581055,\n",
       "  'y': -2.1295669078826904,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 25.230459213256836, 'y': -6.259962558746338, 'color': 'True Negatives'},\n",
       " {'x': -10.325523376464844, 'y': 8.306467056274414, 'color': 'True Negatives'},\n",
       " {'x': -15.765997886657715,\n",
       "  'y': -26.821496963500977,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 17.338111877441406, 'y': -3.012646198272705, 'color': 'True Negatives'},\n",
       " {'x': 40.255035400390625, 'y': 1.988775610923767, 'color': 'True Negatives'},\n",
       " {'x': -11.782054901123047,\n",
       "  'y': -16.49617576599121,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 10.380443572998047, 'y': -9.798243522644043, 'color': 'True Negatives'},\n",
       " {'x': -23.30378532409668, 'y': 16.461071014404297, 'color': 'True Negatives'},\n",
       " {'x': -22.96941566467285, 'y': 25.111774444580078, 'color': 'True Negatives'},\n",
       " {'x': 30.562458038330078, 'y': 18.70453453063965, 'color': 'True Negatives'},\n",
       " {'x': -24.478891372680664,\n",
       "  'y': -11.343870162963867,\n",
       "  'color': 'False Negatives'},\n",
       " {'x': -6.89323091506958, 'y': -5.164215087890625, 'color': 'True Negatives'},\n",
       " {'x': 5.318615913391113, 'y': 26.002851486206055, 'color': 'True Negatives'},\n",
       " {'x': -1.708680272102356, 'y': -5.439001560211182, 'color': 'True Negatives'},\n",
       " {'x': 1.3005844354629517,\n",
       "  'y': -22.325220108032227,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 16.207164764404297, 'y': 29.928789138793945, 'color': 'True Negatives'},\n",
       " {'x': 4.697194576263428, 'y': -33.3200569152832, 'color': 'True Negatives'},\n",
       " {'x': -18.284379959106445,\n",
       "  'y': -26.580047607421875,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 29.10879135131836, 'y': 0.9568109512329102, 'color': 'True Negatives'},\n",
       " {'x': -0.7623358964920044,\n",
       "  'y': 18.366533279418945,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -11.867375373840332,\n",
       "  'y': -29.572603225708008,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -22.242265701293945,\n",
       "  'y': -24.488967895507812,\n",
       "  'color': 'False Positives'},\n",
       " {'x': -6.800243377685547, 'y': 27.06736183166504, 'color': 'True Negatives'},\n",
       " {'x': -22.88160514831543,\n",
       "  'y': -28.337997436523438,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 33.567134857177734, 'y': 15.238314628601074, 'color': 'True Negatives'},\n",
       " {'x': 29.60176658630371, 'y': 11.3682861328125, 'color': 'True Negatives'},\n",
       " {'x': 31.79360008239746, 'y': 17.421951293945312, 'color': 'True Negatives'},\n",
       " {'x': -3.8100569248199463, 'y': 33.12745666503906, 'color': 'True Negatives'},\n",
       " {'x': -2.1850688457489014,\n",
       "  'y': -31.681291580200195,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -23.093509674072266,\n",
       "  'y': -28.514801025390625,\n",
       "  'color': 'True Positives'},\n",
       " {'x': -38.503021240234375,\n",
       "  'y': -1.6363263130187988,\n",
       "  'color': 'False Positives'},\n",
       " {'x': -34.780120849609375, 'y': 8.347500801086426, 'color': 'True Negatives'},\n",
       " {'x': -17.76387596130371,\n",
       "  'y': -26.489870071411133,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 10.515311241149902,\n",
       "  'y': -30.498571395874023,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -27.94516372680664,\n",
       "  'y': -5.2430949211120605,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -0.2557469606399536,\n",
       "  'y': 27.734638214111328,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -23.573627471923828,\n",
       "  'y': 14.820281028747559,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -10.965054512023926,\n",
       "  'y': -5.155389785766602,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 28.620227813720703, 'y': 18.207090377807617, 'color': 'True Negatives'},\n",
       " {'x': -12.953983306884766,\n",
       "  'y': -4.799567699432373,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -20.21305274963379, 'y': 9.904537200927734, 'color': 'True Negatives'},\n",
       " {'x': -15.184797286987305,\n",
       "  'y': -28.106122970581055,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -32.03575134277344, 'y': 5.224952697753906, 'color': 'False Negatives'},\n",
       " {'x': -24.64198875427246,\n",
       "  'y': -10.323752403259277,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 33.730262756347656,\n",
       "  'y': -3.7338082790374756,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 4.932524681091309, 'y': 15.984115600585938, 'color': 'True Negatives'},\n",
       " {'x': 5.38709020614624, 'y': 26.244157791137695, 'color': 'True Negatives'},\n",
       " {'x': -35.34376907348633,\n",
       "  'y': -19.076400756835938,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 3.930842161178589, 'y': 17.698476791381836, 'color': 'True Negatives'},\n",
       " {'x': 17.55225372314453, 'y': -12.897743225097656, 'color': 'True Negatives'},\n",
       " {'x': 9.578981399536133, 'y': 10.567875862121582, 'color': 'True Negatives'},\n",
       " {'x': 12.943772315979004,\n",
       "  'y': -29.957914352416992,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 19.79597282409668, 'y': -26.638986587524414, 'color': 'True Negatives'},\n",
       " {'x': -37.058834075927734,\n",
       "  'y': -17.053443908691406,\n",
       "  'color': 'True Positives'},\n",
       " {'x': -33.768978118896484,\n",
       "  'y': -1.3296186923980713,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -0.2627224922180176,\n",
       "  'y': 2.7643017768859863,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 33.19621276855469, 'y': -2.7940080165863037, 'color': 'True Negatives'},\n",
       " {'x': -38.79692840576172,\n",
       "  'y': -1.245448350906372,\n",
       "  'color': 'False Positives'},\n",
       " {'x': -0.8495153188705444,\n",
       "  'y': 30.504064559936523,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -26.12458610534668, 'y': 17.676965713500977, 'color': 'True Negatives'},\n",
       " {'x': 7.615540027618408, 'y': 15.638740539550781, 'color': 'True Negatives'},\n",
       " {'x': 20.266965866088867,\n",
       "  'y': -4.3739213943481445,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 20.34988784790039, 'y': -4.3429341316223145, 'color': 'True Negatives'},\n",
       " {'x': -26.21164894104004, 'y': 22.295211791992188, 'color': 'True Negatives'},\n",
       " {'x': 4.702584266662598, 'y': -24.385412216186523, 'color': 'True Negatives'},\n",
       " {'x': 10.501665115356445,\n",
       "  'y': -13.364696502685547,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -36.01435470581055, 'y': 11.82805347442627, 'color': 'True Negatives'},\n",
       " {'x': -37.73216247558594,\n",
       "  'y': -2.0335373878479004,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 11.426679611206055,\n",
       "  'y': -30.245269775390625,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -37.71355056762695, 'y': 7.498411655426025, 'color': 'True Negatives'},\n",
       " {'x': 4.539112091064453, 'y': 6.887508392333984, 'color': 'True Negatives'},\n",
       " {'x': -5.256892681121826,\n",
       "  'y': -29.742191314697266,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 4.351566314697266, 'y': -11.784847259521484, 'color': 'True Negatives'},\n",
       " {'x': 11.337467193603516, 'y': 22.115184783935547, 'color': 'True Negatives'},\n",
       " {'x': 31.706514358520508, 'y': 12.367691040039062, 'color': 'True Negatives'},\n",
       " {'x': -0.8958452343940735, 'y': 8.292476654052734, 'color': 'True Negatives'},\n",
       " {'x': 27.786087036132812, 'y': -3.040886878967285, 'color': 'True Negatives'},\n",
       " {'x': -10.087506294250488, 'y': 8.019582748413086, 'color': 'True Negatives'},\n",
       " {'x': -12.882251739501953,\n",
       "  'y': -11.278491973876953,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 24.506315231323242, 'y': -27.03632354736328, 'color': 'True Negatives'},\n",
       " {'x': -21.5894832611084, 'y': -9.448698043823242, 'color': 'True Negatives'},\n",
       " {'x': 1.3231122493743896,\n",
       "  'y': -24.417814254760742,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -29.42555809020996, 'y': 17.484447479248047, 'color': 'True Negatives'},\n",
       " {'x': 24.88756561279297, 'y': -29.02660369873047, 'color': 'True Negatives'},\n",
       " {'x': -0.3406495153903961, 'y': 6.63055419921875, 'color': 'True Negatives'},\n",
       " {'x': -32.97281265258789, 'y': 17.9175968170166, 'color': 'True Negatives'},\n",
       " {'x': -5.686202049255371, 'y': -5.045215129852295, 'color': 'True Negatives'},\n",
       " {'x': -33.31653594970703, 'y': 4.7055277824401855, 'color': 'True Negatives'},\n",
       " {'x': 2.137653112411499, 'y': -0.9232003092765808, 'color': 'True Negatives'},\n",
       " {'x': -5.216663837432861, 'y': 1.7813221216201782, 'color': 'True Negatives'},\n",
       " {'x': 10.167295455932617,\n",
       "  'y': -30.477027893066406,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -28.68773651123047, 'y': 3.9267654418945312, 'color': 'True Negatives'},\n",
       " {'x': 0.10581332445144653,\n",
       "  'y': 25.470544815063477,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -5.853806018829346, 'y': 33.708187103271484, 'color': 'True Negatives'},\n",
       " {'x': 20.04825210571289, 'y': 18.732229232788086, 'color': 'True Negatives'},\n",
       " {'x': -4.686312675476074, 'y': -33.63724899291992, 'color': 'True Negatives'},\n",
       " {'x': 28.358600616455078,\n",
       "  'y': -2.8505067825317383,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -23.744836807250977, 'y': 6.827741622924805, 'color': 'True Negatives'},\n",
       " {'x': -11.586923599243164,\n",
       "  'y': 3.0628628730773926,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -3.1652514934539795, 'y': -11.6686429977417, 'color': 'True Negatives'},\n",
       " {'x': -18.05141258239746,\n",
       "  'y': -24.615238189697266,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 4.073530197143555, 'y': 24.82416343688965, 'color': 'True Negatives'},\n",
       " {'x': 14.183198928833008, 'y': 10.83181095123291, 'color': 'True Negatives'},\n",
       " {'x': -33.496150970458984, 'y': 18.11127281188965, 'color': 'True Negatives'},\n",
       " {'x': -30.709081649780273,\n",
       "  'y': 19.666404724121094,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -30.871723175048828,\n",
       "  'y': 12.140820503234863,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -28.032672882080078, 'y': 3.963705539703369, 'color': 'True Negatives'},\n",
       " {'x': 0.36745724081993103,\n",
       "  'y': -11.762160301208496,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 6.9105072021484375, 'y': -3.963068723678589, 'color': 'True Negatives'},\n",
       " {'x': -5.598038196563721, 'y': -30.06899642944336, 'color': 'True Negatives'},\n",
       " {'x': -10.648031234741211,\n",
       "  'y': 20.886699676513672,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -25.274856567382812,\n",
       "  'y': 15.131698608398438,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 19.34727668762207, 'y': -26.511138916015625, 'color': 'True Negatives'},\n",
       " {'x': 42.06536102294922, 'y': 1.0028457641601562, 'color': 'True Negatives'},\n",
       " {'x': -21.552709579467773, 'y': 4.013945579528809, 'color': 'True Negatives'},\n",
       " {'x': -29.646669387817383,\n",
       "  'y': -9.019079208374023,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 31.618499755859375, 'y': 11.354613304138184, 'color': 'True Negatives'},\n",
       " {'x': 29.33652114868164, 'y': -10.857425689697266, 'color': 'True Negatives'},\n",
       " {'x': -21.977453231811523, 'y': 25.36023712158203, 'color': 'True Negatives'},\n",
       " {'x': 13.836875915527344,\n",
       "  'y': -21.202627182006836,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -26.697669982910156,\n",
       "  'y': -0.7234294414520264,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -20.326452255249023,\n",
       "  'y': -3.814844846725464,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -19.530597686767578,\n",
       "  'y': -3.081970691680908,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 3.5719377994537354, 'y': 24.535005569458008, 'color': 'True Negatives'},\n",
       " {'x': -2.701742172241211, 'y': 26.891908645629883, 'color': 'True Negatives'},\n",
       " {'x': -11.011775970458984,\n",
       "  'y': -32.656768798828125,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 22.686725616455078, 'y': 12.084124565124512, 'color': 'True Negatives'},\n",
       " {'x': -29.628408432006836,\n",
       "  'y': -9.850723266601562,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 6.471375465393066, 'y': 29.929004669189453, 'color': 'True Negatives'},\n",
       " {'x': -35.760475158691406,\n",
       "  'y': 10.352354049682617,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -19.803451538085938, 'y': 15.90813159942627, 'color': 'True Negatives'},\n",
       " {'x': -24.350139617919922,\n",
       "  'y': 1.2236281633377075,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 8.627896308898926, 'y': 30.601810455322266, 'color': 'True Negatives'},\n",
       " {'x': -7.57359504699707, 'y': 11.956537246704102, 'color': 'True Negatives'},\n",
       " {'x': -29.72713279724121, 'y': -9.32662296295166, 'color': 'False Positives'},\n",
       " {'x': 16.543148040771484, 'y': -7.963711261749268, 'color': 'True Negatives'},\n",
       " {'x': 32.93798065185547, 'y': -3.099648952484131, 'color': 'True Negatives'},\n",
       " {'x': 35.74422073364258, 'y': -0.8292281627655029, 'color': 'True Negatives'},\n",
       " {'x': -14.267535209655762,\n",
       "  'y': -13.537217140197754,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 10.241097450256348,\n",
       "  'y': -30.464752197265625,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -24.03438377380371, 'y': 1.2911767959594727, 'color': 'True Negatives'},\n",
       " {'x': 24.707141876220703, 'y': -27.3874568939209, 'color': 'True Negatives'},\n",
       " {'x': 4.3312764167785645, 'y': 24.69378089904785, 'color': 'True Negatives'},\n",
       " {'x': -27.523487091064453,\n",
       "  'y': -2.3160667419433594,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 7.397751331329346, 'y': -28.39314079284668, 'color': 'True Negatives'},\n",
       " {'x': 20.280563354492188,\n",
       "  'y': -12.875207901000977,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 6.882519245147705, 'y': 26.599437713623047, 'color': 'True Negatives'},\n",
       " {'x': -30.018991470336914,\n",
       "  'y': 11.577864646911621,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 45.999290466308594, 'y': 2.791433572769165, 'color': 'True Negatives'},\n",
       " {'x': -11.061745643615723,\n",
       "  'y': 20.447208404541016,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 6.195457935333252, 'y': 29.456418991088867, 'color': 'True Negatives'},\n",
       " {'x': 34.4766731262207, 'y': 0.5375451445579529, 'color': 'True Negatives'},\n",
       " {'x': 11.423042297363281, 'y': 11.065319061279297, 'color': 'True Negatives'},\n",
       " {'x': 11.018580436706543,\n",
       "  'y': -25.591047286987305,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 35.91264724731445, 'y': 5.555496692657471, 'color': 'True Negatives'},\n",
       " {'x': 8.161896705627441, 'y': 0.4594654440879822, 'color': 'False Negatives'},\n",
       " {'x': -4.271015167236328, 'y': -33.63583755493164, 'color': 'True Negatives'},\n",
       " {'x': 13.679059982299805, 'y': 31.61757469177246, 'color': 'True Negatives'},\n",
       " {'x': -19.5948486328125, 'y': 25.802169799804688, 'color': 'True Negatives'},\n",
       " {'x': 28.697040557861328, 'y': 11.034180641174316, 'color': 'True Negatives'},\n",
       " {'x': -12.038222312927246,\n",
       "  'y': 18.308334350585938,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 22.334598541259766, 'y': 4.73341178894043, 'color': 'True Negatives'},\n",
       " {'x': 2.0231945514678955,\n",
       "  'y': -23.867238998413086,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 16.71119499206543, 'y': 22.38519859313965, 'color': 'True Negatives'},\n",
       " {'x': -16.15766143798828, 'y': 10.77193832397461, 'color': 'True Negatives'},\n",
       " {'x': 10.493097305297852, 'y': 11.43742561340332, 'color': 'True Negatives'},\n",
       " {'x': 8.208609580993652, 'y': -31.919044494628906, 'color': 'True Negatives'},\n",
       " {'x': 12.82213020324707, 'y': -30.85497283935547, 'color': 'True Negatives'},\n",
       " {'x': -0.3389931917190552,\n",
       "  'y': -24.121042251586914,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 25.651472091674805, 'y': 10.01462459564209, 'color': 'True Negatives'},\n",
       " {'x': 18.467313766479492, 'y': 12.268200874328613, 'color': 'True Negatives'},\n",
       " {'x': 6.145755290985107, 'y': 0.18020901083946228, 'color': 'True Negatives'},\n",
       " {'x': 16.492158889770508,\n",
       "  'y': -29.055988311767578,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 15.545119285583496, 'y': 0.4963633120059967, 'color': 'True Negatives'},\n",
       " {'x': -17.256189346313477,\n",
       "  'y': -0.9557100534439087,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -2.660513401031494, 'y': 34.24248123168945, 'color': 'True Negatives'},\n",
       " {'x': -18.80145835876465, 'y': -5.336537837982178, 'color': 'True Negatives'},\n",
       " {'x': -35.77206039428711,\n",
       "  'y': -17.517410278320312,\n",
       "  'color': 'True Positives'},\n",
       " {'x': -36.10662078857422, 'y': 11.998863220214844, 'color': 'True Negatives'},\n",
       " {'x': -11.8654146194458, 'y': 5.760638236999512, 'color': 'True Negatives'},\n",
       " {'x': 17.068073272705078, 'y': 28.813634872436523, 'color': 'True Negatives'},\n",
       " {'x': -12.804215431213379,\n",
       "  'y': 15.818964958190918,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 25.518720626831055, 'y': -12.53911304473877, 'color': 'True Negatives'},\n",
       " {'x': 32.264713287353516, 'y': 16.646562576293945, 'color': 'True Negatives'},\n",
       " {'x': 8.62735366821289, 'y': -12.732086181640625, 'color': 'True Negatives'},\n",
       " {'x': 15.588385581970215, 'y': 31.52141761779785, 'color': 'True Negatives'},\n",
       " {'x': 28.035423278808594, 'y': 9.778952598571777, 'color': 'True Negatives'},\n",
       " {'x': 28.57726287841797, 'y': -7.101796627044678, 'color': 'True Negatives'},\n",
       " {'x': 27.114383697509766, 'y': 11.324687957763672, 'color': 'True Negatives'},\n",
       " {'x': -36.30317306518555, 'y': 10.79701042175293, 'color': 'True Negatives'},\n",
       " {'x': -12.0418062210083, 'y': 18.346925735473633, 'color': 'True Negatives'},\n",
       " {'x': 9.787848472595215, 'y': 26.1290283203125, 'color': 'True Negatives'},\n",
       " {'x': 4.772369861602783, 'y': -7.959325313568115, 'color': 'True Negatives'},\n",
       " {'x': -15.81765365600586,\n",
       "  'y': -3.9477932453155518,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 20.68287467956543, 'y': -0.6297832727432251, 'color': 'True Negatives'},\n",
       " {'x': -7.494175434112549, 'y': 2.902250289916992, 'color': 'True Negatives'},\n",
       " {'x': -0.600098729133606,\n",
       "  'y': -21.176883697509766,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 0.1524219512939453, 'y': 10.188920974731445, 'color': 'True Negatives'},\n",
       " {'x': 12.596378326416016, 'y': 16.31882095336914, 'color': 'True Negatives'},\n",
       " {'x': 32.12406921386719, 'y': 6.268425464630127, 'color': 'True Negatives'},\n",
       " {'x': 7.141224384307861, 'y': -5.1189069747924805, 'color': 'True Negatives'},\n",
       " {'x': 28.83041000366211, 'y': 17.99183464050293, 'color': 'True Negatives'},\n",
       " {'x': 31.031206130981445,\n",
       "  'y': -2.2978310585021973,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -14.978182792663574,\n",
       "  'y': 11.611224174499512,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 21.711729049682617, 'y': 20.569997787475586, 'color': 'True Negatives'},\n",
       " {'x': -26.00957489013672, 'y': -1.759650468826294, 'color': 'True Negatives'},\n",
       " {'x': 18.558717727661133,\n",
       "  'y': -14.412664413452148,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -20.181285858154297,\n",
       "  'y': -4.4186625480651855,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -32.55277633666992,\n",
       "  'y': 17.309778213500977,\n",
       "  'color': 'False Negatives'},\n",
       " {'x': -21.83547019958496, 'y': 17.38444709777832, 'color': 'True Negatives'},\n",
       " {'x': 15.05479621887207, 'y': 29.870405197143555, 'color': 'True Negatives'},\n",
       " {'x': 32.483306884765625, 'y': 3.779203176498413, 'color': 'True Negatives'},\n",
       " {'x': -1.3563796281814575, 'y': -7.01193380355835, 'color': 'True Negatives'},\n",
       " {'x': -25.999589920043945,\n",
       "  'y': -0.18870913982391357,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -34.85943603515625,\n",
       "  'y': -19.341833114624023,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 8.16135025024414, 'y': 27.53451156616211, 'color': 'True Negatives'},\n",
       " {'x': -34.071414947509766,\n",
       "  'y': -18.242267608642578,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 5.11185359954834, 'y': -15.148909568786621, 'color': 'True Negatives'},\n",
       " {'x': 28.4514217376709, 'y': 17.995386123657227, 'color': 'True Negatives'},\n",
       " {'x': 5.426463603973389, 'y': 31.047998428344727, 'color': 'True Negatives'},\n",
       " {'x': -29.774946212768555,\n",
       "  'y': -9.747166633605957,\n",
       "  'color': 'False Positives'},\n",
       " {'x': -24.658952713012695,\n",
       "  'y': -2.7054693698883057,\n",
       "  'color': 'False Negatives'},\n",
       " {'x': 17.088510513305664,\n",
       "  'y': -30.203275680541992,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 40.52255630493164, 'y': 2.0502946376800537, 'color': 'True Negatives'},\n",
       " {'x': -1.7021162509918213,\n",
       "  'y': -32.09675979614258,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -15.103761672973633,\n",
       "  'y': -5.077260494232178,\n",
       "  'color': 'False Negatives'},\n",
       " {'x': -15.177369117736816,\n",
       "  'y': 21.495634078979492,\n",
       "  'color': 'False Negatives'},\n",
       " {'x': 29.454837799072266, 'y': 16.697790145874023, 'color': 'True Negatives'},\n",
       " {'x': 42.66122055053711, 'y': 2.4900567531585693, 'color': 'True Negatives'},\n",
       " {'x': -16.311771392822266,\n",
       "  'y': 22.620946884155273,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 21.788593292236328, 'y': 29.256223678588867, 'color': 'True Negatives'},\n",
       " {'x': -25.40171241760254, 'y': -4.331759929656982, 'color': 'True Negatives'},\n",
       " {'x': -35.32365417480469,\n",
       "  'y': -20.806976318359375,\n",
       "  'color': 'True Positives'},\n",
       " {'x': 15.836731910705566,\n",
       "  'y': -13.801946640014648,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 21.47276496887207, 'y': 21.100177764892578, 'color': 'True Negatives'},\n",
       " {'x': 0.6051446199417114,\n",
       "  'y': -21.583799362182617,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -12.294499397277832,\n",
       "  'y': -33.98135757446289,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 23.438888549804688,\n",
       "  'y': -1.1676691770553589,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -26.40785026550293, 'y': 8.283623695373535, 'color': 'True Negatives'},\n",
       " {'x': -4.769812107086182, 'y': 5.3268866539001465, 'color': 'True Negatives'},\n",
       " {'x': 22.12005615234375, 'y': -1.737384557723999, 'color': 'True Negatives'},\n",
       " {'x': 11.21859359741211, 'y': 22.598655700683594, 'color': 'True Negatives'},\n",
       " {'x': -34.61679458618164, 'y': 6.277934551239014, 'color': 'True Negatives'},\n",
       " {'x': -0.21552206575870514,\n",
       "  'y': 26.78817367553711,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -33.2735710144043, 'y': 3.4375171661376953, 'color': 'True Negatives'},\n",
       " {'x': -6.200038909912109, 'y': 29.708480834960938, 'color': 'True Negatives'},\n",
       " {'x': 4.209994792938232, 'y': 16.608335494995117, 'color': 'True Negatives'},\n",
       " {'x': -36.23563003540039,\n",
       "  'y': -20.675865173339844,\n",
       "  'color': 'False Positives'},\n",
       " {'x': -19.11836814880371, 'y': 16.599300384521484, 'color': 'True Negatives'},\n",
       " {'x': 37.60711669921875, 'y': -0.514178454875946, 'color': 'True Negatives'},\n",
       " {'x': 13.327500343322754,\n",
       "  'y': -23.155128479003906,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 15.316329002380371, 'y': 25.905719757080078, 'color': 'True Negatives'},\n",
       " {'x': -16.2076358795166, 'y': 10.949915885925293, 'color': 'True Negatives'},\n",
       " {'x': 8.806921005249023, 'y': -21.769567489624023, 'color': 'True Negatives'},\n",
       " {'x': 4.530473232269287, 'y': 1.389418363571167, 'color': 'True Negatives'},\n",
       " {'x': 26.65204429626465, 'y': 8.255605697631836, 'color': 'True Negatives'},\n",
       " {'x': 16.766468048095703,\n",
       "  'y': -2.6802289485931396,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 10.425395965576172, 'y': -9.982378005981445, 'color': 'True Negatives'},\n",
       " {'x': -30.613868713378906,\n",
       "  'y': -11.643227577209473,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 14.26373291015625, 'y': 1.8776260614395142, 'color': 'True Negatives'},\n",
       " {'x': -34.717777252197266,\n",
       "  'y': -1.1967509984970093,\n",
       "  'color': 'False Positives'},\n",
       " {'x': -0.08348330110311508,\n",
       "  'y': 13.586129188537598,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 7.985623836517334, 'y': 5.364238262176514, 'color': 'True Negatives'},\n",
       " {'x': -15.802315711975098,\n",
       "  'y': -2.5504260063171387,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -33.76766586303711,\n",
       "  'y': -17.843223571777344,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 33.56080627441406, 'y': 4.4344024658203125, 'color': 'True Negatives'},\n",
       " {'x': 1.1796915531158447,\n",
       "  'y': -1.5191000699996948,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 31.7830753326416, 'y': -8.464221000671387, 'color': 'True Negatives'},\n",
       " {'x': -8.928215026855469, 'y': 3.022202491760254, 'color': 'True Negatives'},\n",
       " {'x': 8.048423767089844, 'y': 30.863277435302734, 'color': 'True Negatives'},\n",
       " {'x': 39.93090057373047, 'y': 2.558387517929077, 'color': 'True Negatives'},\n",
       " {'x': -15.307211875915527,\n",
       "  'y': -22.181089401245117,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 6.453766345977783, 'y': 5.348464488983154, 'color': 'True Negatives'},\n",
       " {'x': -20.38070297241211, 'y': 16.914709091186523, 'color': 'True Negatives'},\n",
       " {'x': -14.44917106628418,\n",
       "  'y': -25.618154525756836,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -8.60526180267334, 'y': -4.7881293296813965, 'color': 'True Negatives'},\n",
       " {'x': 24.894800186157227, 'y': 5.135221481323242, 'color': 'True Negatives'},\n",
       " {'x': 11.54883861541748, 'y': -23.719951629638672, 'color': 'True Negatives'},\n",
       " {'x': 18.252534866333008,\n",
       "  'y': 27.896833419799805,\n",
       "  'color': 'False Negatives'},\n",
       " {'x': -4.526423454284668,\n",
       "  'y': -10.001336097717285,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 22.049148559570312,\n",
       "  'y': -26.950626373291016,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 0.6012998819351196, 'y': 2.3046658039093018, 'color': 'True Negatives'},\n",
       " {'x': -10.356979370117188,\n",
       "  'y': 3.4781084060668945,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 6.210629463195801, 'y': -35.05149841308594, 'color': 'True Negatives'},\n",
       " {'x': 16.764503479003906, 'y': -2.981964111328125, 'color': 'True Negatives'},\n",
       " {'x': -35.63079071044922, 'y': -16.14255142211914, 'color': 'True Positives'},\n",
       " {'x': -29.05345916748047, 'y': 10.60578727722168, 'color': 'True Negatives'},\n",
       " {'x': -14.952818870544434,\n",
       "  'y': -30.43839454650879,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -10.969533920288086,\n",
       "  'y': 7.2646284103393555,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 23.703853607177734,\n",
       "  'y': -10.894368171691895,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -5.032881259918213,\n",
       "  'y': -11.674086570739746,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 19.542158126831055, 'y': 11.606091499328613, 'color': 'True Negatives'},\n",
       " {'x': -37.53499984741211, 'y': 0.3834227919578552, 'color': 'True Positives'},\n",
       " {'x': -29.089479446411133,\n",
       "  'y': 11.554390907287598,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -29.8859920501709, 'y': 20.432090759277344, 'color': 'True Negatives'},\n",
       " {'x': -11.210919380187988,\n",
       "  'y': 20.311527252197266,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 2.316335439682007, 'y': 9.324819564819336, 'color': 'True Negatives'},\n",
       " {'x': 24.469083786010742,\n",
       "  'y': -11.814460754394531,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 3.89398193359375, 'y': 15.673603057861328, 'color': 'True Negatives'},\n",
       " {'x': -29.173248291015625, 'y': -7.44634485244751, 'color': 'True Negatives'},\n",
       " {'x': 11.085326194763184,\n",
       "  'y': -13.409493446350098,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -16.186952590942383,\n",
       "  'y': -0.869221568107605,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -9.434882164001465, 'y': 11.137674331665039, 'color': 'True Negatives'},\n",
       " {'x': 12.09947395324707, 'y': -12.687875747680664, 'color': 'True Negatives'},\n",
       " {'x': -2.1336331367492676,\n",
       "  'y': -24.76927375793457,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 32.22152328491211, 'y': -8.066882133483887, 'color': 'True Negatives'},\n",
       " {'x': 21.934661865234375, 'y': -28.46479034423828, 'color': 'True Negatives'},\n",
       " {'x': 7.412326812744141, 'y': 0.3306477963924408, 'color': 'True Negatives'},\n",
       " {'x': -22.0511531829834,\n",
       "  'y': -25.066598892211914,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 4.5276689529418945, 'y': -26.40786361694336, 'color': 'True Negatives'},\n",
       " {'x': 0.05144887417554855,\n",
       "  'y': -12.250717163085938,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -7.106003284454346, 'y': -9.666593551635742, 'color': 'True Negatives'},\n",
       " {'x': 15.809242248535156, 'y': -28.13369369506836, 'color': 'True Negatives'},\n",
       " {'x': 3.5557565689086914, 'y': 17.1689510345459, 'color': 'True Negatives'},\n",
       " {'x': -37.91765213012695, 'y': 2.9151108264923096, 'color': 'True Negatives'},\n",
       " {'x': -16.10572624206543, 'y': -8.284201622009277, 'color': 'True Negatives'},\n",
       " {'x': 20.09456443786621, 'y': 10.87869930267334, 'color': 'True Negatives'},\n",
       " {'x': 0.11723501235246658,\n",
       "  'y': -20.61825180053711,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -32.34944534301758, 'y': 8.026053428649902, 'color': 'True Negatives'},\n",
       " {'x': 5.651525497436523,\n",
       "  'y': -0.06786077469587326,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -8.110278129577637, 'y': 14.826091766357422, 'color': 'True Negatives'},\n",
       " {'x': -32.46727752685547,\n",
       "  'y': -15.755711555480957,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 8.401145935058594, 'y': -9.568046569824219, 'color': 'True Negatives'},\n",
       " {'x': -8.480912208557129, 'y': 14.200505256652832, 'color': 'True Negatives'},\n",
       " {'x': -5.946795463562012,\n",
       "  'y': -30.520828247070312,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -37.75883865356445, 'y': 6.427626132965088, 'color': 'True Negatives'},\n",
       " {'x': -32.06283950805664, 'y': 11.923803329467773, 'color': 'True Negatives'},\n",
       " {'x': -25.029619216918945,\n",
       "  'y': -2.016880750656128,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -28.510496139526367,\n",
       "  'y': -14.731728553771973,\n",
       "  'color': 'False Positives'},\n",
       " {'x': -32.35130310058594, 'y': 4.440080642700195, 'color': 'True Negatives'},\n",
       " {'x': -37.829551696777344, 'y': 4.667117118835449, 'color': 'True Negatives'},\n",
       " {'x': -2.4625203609466553,\n",
       "  'y': -22.53250503540039,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -32.847171783447266,\n",
       "  'y': 3.9278175830841064,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 11.932557106018066, 'y': -29.08767318725586, 'color': 'True Negatives'},\n",
       " {'x': -29.075645446777344,\n",
       "  'y': -6.919973373413086,\n",
       "  'color': 'False Positives'},\n",
       " {'x': -10.466443061828613,\n",
       "  'y': -25.50482177734375,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -36.38353729248047,\n",
       "  'y': -19.971315383911133,\n",
       "  'color': 'True Positives'},\n",
       " {'x': -7.307398796081543, 'y': -8.9345121383667, 'color': 'True Negatives'},\n",
       " {'x': -0.7277206778526306,\n",
       "  'y': 25.289182662963867,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -28.386926651000977,\n",
       "  'y': 3.7512893676757812,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -5.199997425079346,\n",
       "  'y': -13.606762886047363,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -15.0996675491333, 'y': -0.704422116279602, 'color': 'True Negatives'},\n",
       " {'x': 21.42664909362793, 'y': -3.769390106201172, 'color': 'True Negatives'},\n",
       " {'x': -20.47825050354004,\n",
       "  'y': -2.3363945484161377,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 32.158199310302734, 'y': 6.650988578796387, 'color': 'True Negatives'},\n",
       " {'x': 10.100895881652832, 'y': -9.084465026855469, 'color': 'True Negatives'},\n",
       " {'x': 18.7853946685791, 'y': -14.401322364807129, 'color': 'True Negatives'},\n",
       " {'x': -14.5606107711792, 'y': 11.559484481811523, 'color': 'True Negatives'},\n",
       " {'x': 19.838973999023438,\n",
       "  'y': -0.5760286450386047,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -14.807934761047363, 'y': 15.49531364440918, 'color': 'True Negatives'},\n",
       " {'x': 34.165321350097656, 'y': -5.526458263397217, 'color': 'True Negatives'},\n",
       " {'x': 11.936610221862793, 'y': -23.23942756652832, 'color': 'True Negatives'},\n",
       " {'x': -33.27085494995117,\n",
       "  'y': -16.853557586669922,\n",
       "  'color': 'True Positives'},\n",
       " {'x': -27.019399642944336, 'y': 9.39583683013916, 'color': 'True Negatives'},\n",
       " {'x': -35.437503814697266,\n",
       "  'y': -19.841157913208008,\n",
       "  'color': 'True Positives'},\n",
       " {'x': 20.6798095703125, 'y': -14.38475513458252, 'color': 'True Negatives'},\n",
       " {'x': 46.13856506347656, 'y': 2.734426975250244, 'color': 'True Negatives'},\n",
       " {'x': -25.688129425048828,\n",
       "  'y': 0.45419174432754517,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -12.658843040466309,\n",
       "  'y': -2.456284523010254,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -31.598926544189453,\n",
       "  'y': -15.45878791809082,\n",
       "  'color': 'False Positives'},\n",
       " {'x': 16.08449935913086, 'y': -6.670697212219238, 'color': 'True Negatives'},\n",
       " {'x': 9.289968490600586, 'y': -13.997740745544434, 'color': 'True Negatives'},\n",
       " {'x': 20.99970817565918, 'y': 21.447216033935547, 'color': 'True Negatives'},\n",
       " {'x': -18.325109481811523,\n",
       "  'y': 0.7791081666946411,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 26.80091667175293, 'y': -10.321154594421387, 'color': 'True Negatives'},\n",
       " {'x': -13.47872543334961,\n",
       "  'y': -11.443487167358398,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 6.748708248138428, 'y': 2.8490076065063477, 'color': 'True Negatives'},\n",
       " {'x': 8.538392066955566, 'y': -34.95039367675781, 'color': 'True Negatives'},\n",
       " {'x': 28.318645477294922, 'y': 0.8770963549613953, 'color': 'True Negatives'},\n",
       " {'x': 1.114203929901123, 'y': -1.4106042385101318, 'color': 'True Negatives'},\n",
       " {'x': -13.101225852966309,\n",
       "  'y': 15.569899559020996,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 2.471083641052246, 'y': -23.118467330932617, 'color': 'True Negatives'},\n",
       " {'x': -14.214693069458008,\n",
       "  'y': -2.9899725914001465,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 14.096161842346191,\n",
       "  'y': -0.1489046961069107,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -9.838462829589844, 'y': -9.520757675170898, 'color': 'True Negatives'},\n",
       " {'x': 26.970027923583984,\n",
       "  'y': -10.407151222229004,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 5.629745960235596, 'y': 0.8908444046974182, 'color': 'True Negatives'},\n",
       " {'x': 10.031932830810547,\n",
       "  'y': -12.485885620117188,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -7.450929641723633,\n",
       "  'y': -14.248173713684082,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 18.26891326904297,\n",
       "  'y': -0.37334147095680237,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 21.874692916870117,\n",
       "  'y': -26.229223251342773,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': 29.677610397338867, 'y': 0.8106353878974915, 'color': 'True Negatives'},\n",
       " {'x': -19.785079956054688, 'y': 9.098518371582031, 'color': 'True Negatives'},\n",
       " {'x': -28.953800201416016,\n",
       "  'y': -6.312048435211182,\n",
       "  'color': 'True Negatives'},\n",
       " {'x': -23.563060760498047,\n",
       "  'y': -4.648802757263184,\n",
       "  'color': 'False Negatives'},\n",
       " {'x': 30.667020797729492, 'y': 0.8670955300331116, 'color': 'True Negatives'},\n",
       " {'x': -9.147920608520508, 'y': 26.008878707885742, 'color': 'True Negatives'},\n",
       " {'x': -24.37887954711914, 'y': -11.36745548248291, 'color': 'True Negatives'},\n",
       " {'x': 18.065288543701172, 'y': -26.49997901916504, 'color': 'True Negatives'},\n",
       " {'x': -28.847002029418945,\n",
       "  'y': -2.6890599727630615,\n",
       "  'color': 'True Negatives'}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list2save)\n",
    "df1.iloc[:500][['x','y','color']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['full', 'dam'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3840, 2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "39580"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "X_tsne1.keys()\n",
    "X_tsne1['full'].shape\n",
    "type(X_tsne1['full'])\n",
    "list2save = df1.iloc[:500][['x','y','color']].to_dict('records')\n",
    "\n",
    "# save list to json\n",
    "with open('tsne_datavis.json','w') as f:\n",
    "    f.write(json.dumps(list2save))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path =  \"hokarami/TEEDAM_supervised/lzvf7erg\" # [Q20-DA__base-concat]1586915\n",
    "\n",
    "\n",
    "\n",
    "run_path = \"hokarami/TEEDAM_unsupervised/su87zi5z\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/su87zi5z/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "# h0s0\n",
    "run_path = \"hokarami/TEEDAM_supervised/bxawmk71\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/bxawmk71/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "# seft\n",
    "run_path = \"hokarami/TEEDAM_supervised/qvdve9d5\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/qvdve9d5?workspace=user-g-hojatkarami\n",
    "\n",
    "# seft\n",
    "run_path = \"hokarami/TEEDAM_supervised/qvdve9d5\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/qvdve9d5?workspace=user-g-hojatkarami\n",
    "\n",
    "\n",
    "# so\n",
    "# run_path = \"hokarami/TEEDAM_unsupervised_timeCat/22xvwu9y\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/su87zi5z/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "\n",
    "# p19\n",
    "run_path =  \"hokarami/TEEDAM_supervised/yftm62nl\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/yftm62nl?workspace=user-g-hojatkarami\n",
    "\n",
    "\n",
    "\n",
    "model2, opt2, run2 = read_from_wandb(run_path)\n",
    "dict_metrics2, out2 = Main.valid_epoch_tsne(model2, opt2.validloader, opt2.pred_loss_func, opt2)\n",
    "X_tsne2, X_tsne_split2 = compute_tsne(out2['r_enc_list'], model2)\n",
    "\n",
    "# res_labels = opt2.dict_map_events.keys()\n",
    "\n",
    "df2 = build_df(out2,opt2, X_tsne2)\n",
    "fig_tsne2 = plot_tsne(df2,title=run2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = build_df(out2,opt2, X_tsne2)\n",
    "fig_temp = plot_tsne(df_temp,title=run2.name)\n",
    "fig_temp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "run_dash_app(fig_tsne2,out2,opt2,df2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path = \"hokarami/TEEDAM_unsupervised_timeCat/ecw38vpt\" # https://wandb.ai/hokarami/TEEDAM_unsupervised_timeCat/runs/ecw38vpt/overview?workspace=user-g-hojatkarami\n",
    "run_path = \"hokarami/TEEDAM_unsupervised_timeCat/2vxzsnyv\" # https://wandb.ai/hokarami/TEEDAM_unsupervised_timeCat/runs/ol0mjnyt/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "model3, opt3, run3 = read_from_wandb(run_path)\n",
    "\n",
    "temp = [\"Nice Question\",\"Good Anser\",\"Guru\",\"Popular Question\",\"Famou Question\",\"Nice Answer\",\"Good Question\", \"Caucus\",\"Notable Question\",\"Nercromancer\",\"Promoter\",\"Yearling\",\"Revival\",\"Enlightened\",\"Greateanswer\",\"Populist\",\"Great Question\",\"Constituent\",\"Announcer\",\"Stellar Question\",\"Booster\",\"Publicist\"]\n",
    "opt3.dict_map_events = {i:x for i,x in enumerate(temp)}\n",
    "\n",
    "\n",
    "\n",
    "dict_metrics3, out3 = Main.valid_epoch_tsne(model3, opt3.validloader, opt3.pred_loss_func, opt3)\n",
    "X_tsne3, X_tsne_split3 = compute_tsne(out3['r_enc_list'], model3)\n",
    "\n",
    "if hasattr(opt3,'dict_map_events'):\n",
    "    res_labels = opt3.dict_map_events.keys()\n",
    "\n",
    "df3 = build_df(out3,opt3, X_tsne3)\n",
    "fig_tsne3 = plot_tsne(df3,title=run3.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=out3\n",
    "i_b=0\n",
    "\n",
    "out.keys()\n",
    "\n",
    "y = out['y_true'][:]\n",
    "y_p = out['y_pred'][:]\n",
    "e = out['event_type_list'][i_b][0,1:]\n",
    "p = out['next_event_type_list'][i_b][0,1:]\n",
    "e_next = out['event_type_list'][i_b][0,:-1]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "metrics.roc_auc_score(y,y_p,average='weighted')\n",
    "metrics.roc_auc_score(y, y_p, labels= torch.arange(3) ,average=None)\n",
    "metrics.accuracy_score(y, y_p, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape\n",
    "\n",
    "# y[:100]\n",
    "\n",
    "px.imshow(y[100:200].transpose())\n",
    "px.imshow(y_p[100:200].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y, y_p)[:,:9]\n",
    "\n",
    "np.unique(y,return_counts=True)\n",
    "metrics.f1_score(y, y_p, labels= torch.arange(22) ,average=None, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding common patterns\n",
    "common_patterns,fig_clustering = compute_common_patterns(out3,opt3)\n",
    "# fig_clustering\n",
    "\n",
    "df3 = build_df(out3,opt3, X_tsne3)\n",
    "fig_tsne3 = plot_tsne(df3,title=run3.name)\n",
    "\n",
    "run_dash_app(fig_tsne3,out3,opt3,df3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid=90\n",
    "i_b = int(df1.iloc[pid]['i_b'])\n",
    "i = int(df1.iloc[pid]['i'])\n",
    "i_b,i\n",
    "_,temp = cool_image(out1,i_b,i,opt1)\n",
    "temp[0]\n",
    "\n",
    "\n",
    "fig_cif = plot_cif(out1,i_b,i)\n",
    "fig_cif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.keys()\n",
    "out1['next_event_type_list'][10][94].shape\n",
    "out1['event_type_list'][10][94].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(2)\n",
    "df2.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn-ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_knn=5\n",
    "pid_all = list(df1.index)\n",
    "\n",
    "pid_positive = list(df1[df1.color_true=='Positive Samples'].index)\n",
    "\n",
    "list_sim_score1 = []\n",
    "list_sim_score2 = []\n",
    "\n",
    "X1 = np.concatenate(out1['r_enc_list'],axis=0)[:,:model1.d_out_te]\n",
    "X2 = np.concatenate(out2['r_enc_list'],axis=0)[:,:]\n",
    "\n",
    "for pid in tqdm( pid_positive ):\n",
    "    id_origin = pid\n",
    "\n",
    "    knn_pids = find_knn_pids (X1, id_origin, n_knn=n_knn)\n",
    "    sim_score1, list_summary = cal_similarity(df1, out1, pid, knn_pids)    \n",
    "\n",
    "\n",
    "    knn_pids = find_knn_pids (X2, id_origin, n_knn=n_knn)\n",
    "    sim_score2, list_summary = cal_similarity(df2, out2, pid, knn_pids)    \n",
    "    \n",
    "    if ( not np.isnan(sim_score1) ) and ( not np.isnan(sim_score2) ):\n",
    "        list_sim_score1.append(sim_score1)\n",
    "        list_sim_score2.append(sim_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list_sim_score1), np.std(list_sim_score1)\n",
    "np.mean(list_sim_score2), np.std(list_sim_score2)\n",
    "\n",
    "diff = np.array(list_sim_score1) - np.array(list_sim_score2)\n",
    "diff.sum()/len(diff)\n",
    "diff.argsort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "group1 = [x-0.0 for x in list_sim_score1]\n",
    "group2 = list_sim_score2\n",
    "t_statistic, p_value = ttest_ind(group1, group2, alternative='greater')\n",
    "\n",
    "# Print the results\n",
    "print('t-statistic =', t_statistic)\n",
    "print('p-value =', p_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save selected to local\n",
    "n_knn=5\n",
    "id_origin = 90\n",
    "\n",
    "len_CROP = len(opt1.dict_map_states.keys()) - len(opt1.dict_map_events.keys())\n",
    "len_CROP\n",
    "\n",
    "X1 = np.concatenate(out1['r_enc_list'],axis=0)[:,:model1.d_out_te]\n",
    "X2 = np.concatenate(out2['r_enc_list'],axis=0)[:,:]\n",
    "\n",
    "\n",
    "            # plot and save id_origin\n",
    "i_b = df1.iloc[id_origin]['i_b']\n",
    "i = df1.iloc[id_origin]['i']\n",
    "im_url, temp = cool_image(out1,i_b,i,opt1)\n",
    "im_url.save(f\"./local/images/Origin_{id_origin}.png\")\n",
    "\n",
    "            # save event attentions\n",
    "_,temp = cool_image(out1,i_b,i,opt1)\n",
    "temp[0].save(f\"./local/images/Origin_{id_origin}_att.png\")\n",
    "\n",
    "            # save cif plot\n",
    "fig_cif = plot_cif(out1,i_b,i)\n",
    "fig_cif.write_image(f\"./local/images/Origin_{id_origin}_CIF.svg\")\n",
    "\n",
    "\n",
    "i_g=0\n",
    "for out,opt,df,X in zip([out1,out2],[opt1,opt2],[df1,df2],[X1,X2]):\n",
    "    knn_pids = find_knn_pids (X, id_origin, n_knn=n_knn)\n",
    "    knn_pids\n",
    "    for pid in knn_pids:\n",
    "        # NEW\n",
    "        i_b = df.iloc[pid]['i_b']\n",
    "        i = df.iloc[pid]['i']\n",
    "        im_url, temp = cool_image(out,i_b,i,opt,CROP=len_CROP)\n",
    "        im_url.save(f\"./local/images/C{i_g}_{pid}.png\")\n",
    "\n",
    "\n",
    "\n",
    "    i_g+=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cif diff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$log(\\lambda)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pid_positive = list(df1[df1.color_true=='Positive Samples'].index)\n",
    "pid_negative = list(df1[df1.color_true=='Negative Samples'].index)\n",
    "\n",
    "list_sim_score1 = []\n",
    "list_sim_score2 = []\n",
    "\n",
    "lens = np.concatenate([x.sum(1).cpu().detach() for x in out1['non_pad_mask_list']])\n",
    "X = np.concatenate(out1['list_log_sum'],axis=0) # [n_samples]\n",
    "Y = np.concatenate(out1['list_integral_'],axis=0)/lens # [n_samples]\n",
    "\n",
    "\n",
    "X[pid_positive].mean(), X[pid_positive].std()\n",
    "\n",
    "X[pid_negative].mean(), X[pid_negative].std()\n",
    "\n",
    "\n",
    "\n",
    "Y[pid_positive].mean(), Y[pid_positive].std()\n",
    "\n",
    "Y[pid_negative].mean(), Y[pid_negative].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "group1 = X[pid_positive]\n",
    "group2 = X[pid_negative]\n",
    "t_statistic, p_value = ttest_ind(group1, group2, alternative='greater')\n",
    "\n",
    "# Print the results\n",
    "print('t-statistic =', t_statistic)\n",
    "print('p-value =', p_value)\n",
    "\n",
    "\n",
    "group1 = Y[pid_positive]\n",
    "group2 = Y[pid_negative]\n",
    "t_statistic, p_value = ttest_ind(group1, group2, alternative='greater')\n",
    "\n",
    "# Print the results\n",
    "print('t-statistic =', t_statistic)\n",
    "print('p-value =', p_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clusters = [\n",
    "\n",
    "[1715, 1212, 957, 834, 1401, 1532, 33, 514, 559, 1493],\n",
    "[1715, 1542, 808, 957, 1541, 1296, 252, 978, 34, 1204]\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "df = df1\n",
    "out = out1\n",
    "\n",
    "fig2 = go.Figure()\n",
    "for i_c, cluster in enumerate(clusters):\n",
    "    list_summary = []\n",
    "    for pid in cluster:\n",
    "        \n",
    "\n",
    "        i_b = df.iloc[pid]['i_b']\n",
    "        i = df.iloc[pid]['i']\n",
    "\n",
    "        ev = out['event_type_list'][i_b][i]\n",
    "        t = out['event_time_list'][i_b][i]\n",
    "        st=out['state_time_list'][i_b][i]\n",
    "        \n",
    "        P = st.int().max().item() + 1\n",
    "\n",
    "        M = event2mat(ev,t,P)\n",
    "\n",
    "        \n",
    "\n",
    "        vector = M.sum(1)/M.shape[1]*24\n",
    "\n",
    "        \n",
    "        list_summary.append(vector)\n",
    "        \n",
    "\n",
    "    dotp = [ np.dot(j,list_summary[0])/(np.linalg.norm(j) * np.linalg.norm(list_summary[0])) for  j in list_summary]\n",
    "    np.mean( dotp )\n",
    "\n",
    "    vec_mean = np.mean(list_summary,axis=0)\n",
    "    vec_std = np.std(list_summary,axis=0)\n",
    "\n",
    "    # sum(vec_std)\n",
    "\n",
    "    \n",
    "    _ = fig2.add_trace(go.Bar(\n",
    "        name=f'Cluster {i_c}',\n",
    "        x=list(res_labels), y=vec_mean,\n",
    "        error_y=dict(type='data', array=vec_std)\n",
    "    ))\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_tsne1.write_image(\"local/images/fig1.svg\")\n",
    "fig_tsne2.write_image(\"local/images/fig2.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save selected to local\n",
    "n_knn=5\n",
    "id_origin = 420\n",
    "\n",
    "X1 = np.concatenate(out1['r_enc_list'],axis=0)[:,:model1.d_out_te]\n",
    "X2 = np.concatenate(out2['r_enc_list'],axis=0)[:,:]\n",
    "\n",
    "i_g=0\n",
    "for out,opt,df,X in zip([out1,out2],[opt1,opt2],[df1,df2],[X1,X2]):\n",
    "    \n",
    "    knn_pids = find_knn_pids (X, id_origin, n_knn=n_knn)\n",
    "    knn_pids\n",
    "    # for pid in knn_pids:\n",
    "    #     # NEW\n",
    "    #     i_b = df.iloc[pid]['i_b']\n",
    "    #     i = df.iloc[pid]['i']\n",
    "    #     im_url, temp = cool_image(out,i_b,i,opt,CROP=11)\n",
    "    #     # im_url.save(f\"./local/images/C{i_g}_{pid}.png\")\n",
    "    i_g+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fig =  go.Figure()\n",
    "# new_fig = go.Figure(data=fig_prev['data'],layout=fig_prev['layout'])\n",
    "\n",
    "\n",
    "# new_fig = bar_summary(df,out, point_ids,res_labels, fig=new_fig)\n",
    "\n",
    "n_knn=5\n",
    "id_origin = 583\n",
    "\n",
    "X1 = np.concatenate(out1['r_enc_list'],axis=0)[:,:model1.d_out_te]\n",
    "X2 = np.concatenate(out2['r_enc_list'],axis=0)[:,:]\n",
    "\n",
    "new_fig = bar_summary(df,out, [id_origin],res_labels, fig=new_fig)\n",
    "\n",
    "for out,opt,df,X in zip([out1,out2],[opt1,opt2],[df1,df2],[X1,X2]):\n",
    "    knn_pids = find_knn_pids (X, id_origin, n_knn=n_knn)\n",
    "    \n",
    "    new_fig = bar_summary(df,out, knn_pids,res_labels, fig=new_fig)\n",
    "\n",
    "new_fig\n",
    "new_fig.write_image(\"local/images/bar.svg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIF vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df1\n",
    "out=out1\n",
    "pid_positive = list(df1[df1.color_true=='Positive Samples'].index)\n",
    "pid_negative = list(df1[df1.color_true=='Negative Samples'].index)\n",
    "\n",
    "\n",
    "\n",
    "fig=go.Figure()\n",
    "\n",
    "for pid in pid_positive[:50]:\n",
    "\n",
    "    i_b = int(df.iloc[pid]['i_b'])\n",
    "    i = int(df.iloc[pid]['i'])\n",
    "    taus,cifs_int = cif_cum(out1,i_b,i, fig=None)\n",
    "    _ = fig.add_trace(go.Scatter(x=taus,y=cifs_int,marker=dict(\n",
    "                    # size=2,\n",
    "                    color='red',\n",
    "                )))\n",
    "    \n",
    "for pid in pid_negative[:50]:\n",
    "\n",
    "    i_b = int(df.iloc[pid]['i_b'])\n",
    "    i = int(df.iloc[pid]['i'])\n",
    "    taus,cifs_int = cif_cum(out1,i_b,i, fig=None)\n",
    "    _ = fig.add_trace(go.Scatter(x=taus,y=cifs_int,marker=dict(\n",
    "                    # size=2,\n",
    "                    color='blue',\n",
    "                )))\n",
    "\n",
    "\n",
    "_=fig.update_layout(\n",
    "        # autosize=False,\n",
    "        # title=title,\n",
    "        width=600,\n",
    "        height=600,\n",
    "        showlegend=False,\n",
    "\n",
    "    )\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1.dict_map_events.keys()\n",
    "\n",
    "\n",
    "for i,k in enumerate(opt1.dict_map_events.keys()):\n",
    "    print(f'{i}:{k}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=out1\n",
    "opt=opt1\n",
    "out.keys()\n",
    "\n",
    "\n",
    "all_events=np.concatenate(out['event_type_list'],axis=1) # [B,LLLLLL,K] or [B,LLLL]\n",
    "\n",
    "if opt.data_label=='multiclass':\n",
    "    identity = np.eye(opt.num_marks+1,dtype=np.int8)\n",
    "    all_events = identity.take(all_events, axis=0)[:,:,1:] #  [B,LLLL] -> [B,LLLLLL,K]\n",
    "\n",
    "\n",
    "\n",
    "all_events.shape\n",
    "masks=all_events.sum(-1)>0\n",
    "\n",
    "\n",
    "\n",
    "TSNE_LIMIT=16000\n",
    "X = all_events[masks][:TSNE_LIMIT]\n",
    "\n",
    "\n",
    "X_str = [''.join(str(cell) for cell in row) for row in X]\n",
    "\n",
    "X.shape\n",
    "len(X_str)\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['val']=list(X)\n",
    "df['str']=X_str\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=60, learning_rate=10,n_jobs=4)\n",
    "X_tsne = tsne.fit_transform(X[:TSNE_LIMIT,:])\n",
    "\n",
    "df['x']=X_tsne[:,0]\n",
    "df['y']=X_tsne[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['str'].unique())\n",
    "\n",
    "s = df['str'].value_counts()/df['str'].value_counts().sum()\n",
    "s[s>0.001].head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "N_CLUSTERS=22\n",
    "\n",
    "# Generate sample binary data\n",
    "data = X\n",
    "\n",
    "# Initialize KMeans object with 2 clusters\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS)\n",
    "\n",
    "# Fit the data to the KMeans object\n",
    "_ = kmeans.fit(data)\n",
    "\n",
    "# Get the labels and cluster centers\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "labels\n",
    "\n",
    "\n",
    "\n",
    "df['cluster_kmeans']=labels\n",
    "\n",
    "fig=px.scatter(df,x='x',y='y',color='cluster_kmeans',hover_data=[\"str\"])\n",
    "fig\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initialize DBSCAN object\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "\n",
    "# Fit the data to the DBSCAN object\n",
    "dbscan.fit(data)\n",
    "\n",
    "# Get the labels\n",
    "labels = dbscan.labels_\n",
    "\n",
    "df['cluster_dbscan']=labels\n",
    "\n",
    "fig=px.scatter(df,x='x',y='y',color='cluster_dbscan',hover_data=[\"str\"])\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = 'cluster_kmeans'\n",
    "\n",
    "common_patterns = []\n",
    "\n",
    "\n",
    "if opt.data_label=='multiclass':\n",
    "    common_patterns=np.eye(opt.num_marks,dtype=np.int8)\n",
    "\n",
    "else:\n",
    "    for i in range(N_CLUSTERS):\n",
    "        temp = df[df[METHOD]==i]['val'].values\n",
    "        temp = (temp.sum()/temp.shape[0]*100).astype(int)\n",
    "\n",
    "        temp[temp<30]=0\n",
    "        temp[temp>=30]=1\n",
    "        print(temp,'\\n')\n",
    "        common_patterns.append(temp)\n",
    "\n",
    "    common_patterns = np.array(common_patterns)\n",
    "common_patterns.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt3.diag_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_b=3\n",
    "i=3\n",
    "\n",
    "tee_mapped, norm = att_map(out3,i_b,i,common_patterns)\n",
    "\n",
    "tee_mapped[:5,:5]\n",
    "norm[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df3\n",
    "out=out3\n",
    "\n",
    "point_ids = [2, 7, 15, 23, 34, 36, 37, 44, 49, 63, 74, 79, 83, 86, 87, 89, 93, 109, 115, 116, 117, 123, 128, 135, 137, 140, 153, 154, 159, 160, 168, 172, 174, 179, 202, 217, 226, 236, 245, 247, 257, 258, 259, 260, 261, 278, 281, 286, 287, 311, 319, 334, 336, 347, 348, 351, 364, 379, 386, 420, 426, 431, 443, 448, 455, 460, 466, 467, 469, 480, 487, 491, 500, 505, 517, 518, 523, 2, 7, 15, 23, 34, 36, 37, 44, 49, 63, 74, 79, 83, 86, 87, 89, 93, 109, 115, 116, 117, 123, 128, 135, 137, 140, 153, 154, 159, 160, 168, 172, 174, 179, 202, 217, 226, 236, 245, 247, 257, 258, 259, 260, 261, 278, 281, 286, 287, 311, 319, 334, 336, 347, 348, 351, 364, 379, 386, 420, 426, 431, 443, 448, 455, 460, 466, 467, 469, 480, 487, 491, 500, 505, 517, 518, 523]\n",
    "\n",
    "att_maps=[]\n",
    "att_norms=[]\n",
    "for pid in point_ids:\n",
    "\n",
    "    i_b = int(df.iloc[pid]['i_b'])\n",
    "    i = int(df.iloc[pid]['i'])\n",
    "\n",
    "    tee_mapped, norm = att_map(out,i_b,i,common_patterns)\n",
    "    att_maps.append(tee_mapped  )\n",
    "    att_norms.append(norm  )\n",
    "\n",
    "norms_matrix = sum(att_norms)\n",
    "norms_matrix[norms_matrix==0]=1\n",
    "agg_matrix = sum(att_maps)/norms_matrix\n",
    "# agg_matrix[agg_matrix<0.5]=0\n",
    "agg_matrix[norms_matrix<20]=0\n",
    "\n",
    "norms_matrix[6,16]\n",
    "agg_matrix[6,16]\n",
    "\n",
    "if len(out['event_type_list'][0].shape)==2: # if multi_class\n",
    "    patt_str = [f'C {i}' for i in range(common_patterns.shape[0])]\n",
    "else:\n",
    "    patt_str = [''.join(str(cell) for cell in row) for row in common_patterns]\n",
    "\n",
    "patt_str = [f'C {i}' for i in range(common_patterns.shape[0])]\n",
    "\n",
    "fig_agg_mat = px.imshow(agg_matrix, x=patt_str, y=patt_str)\n",
    "fig_agg_mat\n",
    "\n",
    "fig_freqs = px.imshow(norms_matrix, x=patt_str, y=patt_str)\n",
    "fig_freqs\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TL (tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\DATA\\Tasks\\tedam2\\vis_attn.ipynb Cell 112\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/DATA/Tasks/tedam2/vis_attn.ipynb#Y216sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m api \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39mApi()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/DATA/Tasks/tedam2/vis_attn.ipynb#Y216sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m runs \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mruns(\u001b[39m\"\u001b[39m\u001b[39mhokarami/TEEDAM_supervised\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/DATA/Tasks/tedam2/vis_attn.ipynb#Y216sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_filt \u001b[39m=\u001b[39m dl_runs(runs, selected_tag\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdatavis\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "runs = api.runs(\"hokarami/TEEDAM_supervised\")\n",
    "df_filt = dl_runs(runs, selected_tag='datavis')\n",
    "len(df_filt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_config = pd.DataFrame(   [{k:v for k,v in x.items()} for x in df_filt.config]    )\n",
    "df_summary = pd.DataFrame(   [{k:v for k,v in x.items()} for x in df_filt.summary]    )\n",
    "df_path = df_filt.path.apply(lambda x:'/'.join(x))\n",
    "df_con = pd.concat([df_config, df_summary, df_path],axis=1)\n",
    "len(df_con)\n",
    "\n",
    "df_con['transfer_learning'].unique()\n",
    "\n",
    "if 'knn-ps-mean' in df_con:\n",
    "    q = (  (df_con['transfer_learning']=='DO') & (df_con['knn-ps-mean'].isnull())  )  | ((df_con['INPUT']=='DAM') & (df_con['knn-ps-mean'].isnull())  )\n",
    "else:\n",
    "    q = df_con['transfer_learning'].astype(bool)+True\n",
    "\n",
    "  \n",
    "df_con = df_con[q]\n",
    "len(df_con)\n",
    "run_paths = df_con.path.tolist()\n",
    "\n",
    "\n",
    "# run_paths = [\"hokarami/TEEDAM_supervised/jjolnx6e\",\n",
    "#              \"hokarami/TEEDAM_supervised/mfn0tkkj\"  \n",
    "#              ]\n",
    "\n",
    "# # TEDA freeze TE\n",
    "# run_path = \"hokarami/TEEDAM_supervised/mfn0tkkj\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/347dttsz/overview?workspace=user-g-hojatkarami\n",
    "# # TEDA no freeze\n",
    "# # run_path = \"hokarami/TEEDAM_supervised/5ft25axi\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/347dttsz/overview?workspace=user-g-hojatkarami\n",
    "# # DAM baseline\n",
    "# run_path = \"hokarami/TEEDAM_supervised/jjolnx6e\" # https://wandb.ai/hokarami/TEEDAM_unsupervised/runs/347dttsz/overview?workspace=user-g-hojatkarami\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_path = \"hokarami/TEEDAM_supervised/3drow7po\" # https://wandb.ai/hokarami/TEEDAM_supervised/runs/3drow7po/overview?workspace=user-g-hojatkarami\n",
    "\n",
    "# run_paths = [\"hokarami/TEEDAM_supervised/3drow7po\"]\n",
    "n_knn=10\n",
    "temp = list()\n",
    "bad_runs=list()\n",
    "for run_path in tqdm(run_paths[:], leave=False):\n",
    "\n",
    "    try:\n",
    "        model1, opt1, run1 = read_from_wandb(run_path,consider_sample_labels=True)\n",
    "    except:\n",
    "        bad_runs.append(run_path)\n",
    "        continue\n",
    "    dict_metrics1, out1 = Main.valid_epoch_tsne(model1, opt1.validloader, opt1.pred_loss_func, opt1)\n",
    "    # X_tsne1, X_tsne_split1 = compute_tsne(out1['r_enc_list'], model1)\n",
    "    res_labels = opt1.dict_map_events.keys()\n",
    "    df1 = build_df(out1,opt1, X_tsne=None)\n",
    "\n",
    "    pid_all = list(df1.index)\n",
    "\n",
    "    pid_positive = list(df1[df1.color_true=='Positive Samples'].index)\n",
    "\n",
    "    list_sim_score1 = []\n",
    "    # X1 = np.concatenate(out1['r_enc_list'],axis=0)[:,:]\n",
    "    \n",
    "    # only DAM embeddings\n",
    "    X1 = np.concatenate(out1['r_enc_list'],axis=0)[:,model1.d_out_te:model1.d_out_te+model1.d_out_dam]\n",
    "\n",
    "    # only TEE embeddings\n",
    "    X1 = np.concatenate(out1['r_enc_list'],axis=0)[:,:model1.d_out_te]\n",
    "\n",
    "    # TEE+DAM embeddings\n",
    "    X1 = np.concatenate(out1['r_enc_list'],axis=0)[:,:model1.d_out_te+model1.d_out_dam]\n",
    "\n",
    "    for pid in tqdm( pid_positive ):\n",
    "        id_origin = pid\n",
    "\n",
    "        knn_pids = find_knn_pids (X1, id_origin, n_knn=n_knn)\n",
    "        sim_score1, list_summary = cal_similarity(df1, out1, pid, knn_pids)    \n",
    "\n",
    "        \n",
    "        if ( not np.isnan(sim_score1) ) :\n",
    "            list_sim_score1.append(sim_score1)\n",
    "\n",
    "    # np.mean(list_sim_score1), np.std(list_sim_score1)\n",
    "\n",
    "    api = wandb.Api()\n",
    "    run = api.run(run_path)\n",
    "\n",
    "    run.summary['knn-ps-mean'] = np.mean(list_sim_score1)\n",
    "    run.summary['knn-ps-std'] = np.std(list_sim_score1)\n",
    "    run.summary.update()\n",
    "\n",
    "    temptemp=dict()\n",
    "    temptemp['path']=run_path\n",
    "    temptemp['knn-ps-mean']=np.mean(list_sim_score1)\n",
    "    temptemp['knn-ps-std']=np.std(list_sim_score1)\n",
    "    temp.append(temptemp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_knn=5\n",
    "pid_all = list(df1.index)\n",
    "\n",
    "pid_positive = list(df1[df1.color_true=='Positive Samples'].index)\n",
    "\n",
    "list_sim_score1 = []\n",
    "X1 = np.concatenate(out1['r_enc_list'],axis=0)[:,:model1.d_out_te]\n",
    "\n",
    "\n",
    "for pid in tqdm( pid_positive ):\n",
    "    id_origin = pid\n",
    "\n",
    "    knn_pids = find_knn_pids (X1, id_origin, n_knn=n_knn)\n",
    "    sim_score1, list_summary = cal_similarity(df1, out1, pid, knn_pids)    \n",
    "\n",
    "    \n",
    "    if ( not np.isnan(sim_score1) ) :\n",
    "        list_sim_score1.append(sim_score1)\n",
    "\n",
    "np.mean(list_sim_score1), np.std(list_sim_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "run = api.run(run_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.summary['knn-ps-mean'] = np.mean(list_sim_score1)\n",
    "run.summary['knn-ps-std'] = np.std(list_sim_score1)\n",
    "run.summary.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare\n",
    "\n",
    "P19-sc-H0-s3\n",
    "\n",
    "[H70HHG--DA__base-concat]2094041\n",
    "hokarami/TEEDAM_supervised/gl1y64zk\n",
    "\n",
    "[H70HHG--TEDA__none-concat]2096437\n",
    "hokarami/TEEDAM_supervised/b1u14qh0\n",
    "\n",
    "[H70HHG--TEDA__none-concat]2096437 no pre training\n",
    "hokarami/TEEDAM_supervised/mkue3rz4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path = \"hokarami/TEEDAM_supervised/gl1y64zk\"\n",
    "\n",
    "model1, opt1, run1 = read_from_wandb(run_path,consider_sample_labels=True)\n",
    "dict_metrics1, out1 = Main.valid_epoch_tsne(model1, opt1.validloader, opt1.pred_loss_func, opt1)\n",
    "X_tsne1 = compute_tsne(out1['r_enc_list'], model1, TSNE_LIMIT=6000)\n",
    "\n",
    "res_labels = opt1.dict_map_events.keys()\n",
    "\n",
    "df1 = build_df(out1,opt1, X_tsne1['full'],TSNE_LIMIT=100000)\n",
    "fig_tsne1 = plot_tsne(df1,title=run1.name)\n",
    "\n",
    "\n",
    "print('Baseline: DAM model')\n",
    "fig_tsne1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path = \"hokarami/TEEDAM_supervised/mkue3rz4\"\n",
    "\n",
    "model2, opt2, run2 = read_from_wandb(run_path,consider_sample_labels=True)\n",
    "dict_metrics2, out2 = Main.valid_epoch_tsne(model2, opt2.validloader, opt2.pred_loss_func, opt2)\n",
    "X_tsne2 = compute_tsne(out2['r_enc_list'], model2, TSNE_LIMIT=6000)\n",
    "\n",
    "res_labels = opt2.dict_map_events.keys()\n",
    "\n",
    "df2 = build_df(out2,opt2, X_tsne2['full'],TSNE_LIMIT=100000)\n",
    "fig_tsne2 = plot_tsne(df2,title=run2.name)\n",
    "\n",
    "\n",
    "print('Baseline: TEDAM model')\n",
    "fig_tsne2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = build_df(out2,opt2, X_tsne2['full'],TSNE_LIMIT=100000)\n",
    "fig_tsne2 = plot_tsne(df2,title=run2.name)\n",
    "\n",
    "\n",
    "print('Baseline: TEDAM model > full')\n",
    "fig_tsne2\n",
    "\n",
    "df2 = build_df(out2,opt2, X_tsne2['tee'],TSNE_LIMIT=100000)\n",
    "fig_tsne2 = plot_tsne(df2,title=run2.name)\n",
    "\n",
    "\n",
    "print('Baseline: TEDAM model > tee')\n",
    "fig_tsne2\n",
    "\n",
    "df2 = build_df(out2,opt2, X_tsne2['dam'],TSNE_LIMIT=100000)\n",
    "fig_tsne2 = plot_tsne(df2,title=run2.name)\n",
    "\n",
    "\n",
    "print('Baseline: TEDAM model > dam')\n",
    "fig_tsne2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo conda install -c conda-forge transformers datasets tokenizers --name paper2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['non_pad_mask_list', 'event_type_list', 'event_time_list', 'next_event_type_list', 'list_intens_at_samples', 'list_taus', 'list_true_intens_at_evs', 'list_log_sum', 'list_integral_', 'state_mod_list', 'state_time_list', 'state_value_list', 'r_enc_list', 'list_TE_att', 'list_DAM_att', 'y_pred', 'y_true', 'y_score', 'y_true_list', 'masks', 'y_state_pred', 'y_state_true', 'y_state_score'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 82, 24])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out1['event_type_list'])\n",
    "out1['event_type_list'][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 82, 24])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=out1['event_type_list'][0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 41)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  2.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0., 11.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out=out1\n",
    "df=df1\n",
    "i_b=0\n",
    "i=0\n",
    "\n",
    "ev = out['event_type_list'][i_b][i]\n",
    "t = out['event_time_list'][i_b][i]\n",
    "P = t.int().max().item() + 1\n",
    "m = (ev.sum(1)>0).sum() # False are masked\n",
    "\n",
    "ev = event2mat(ev,t,P)\n",
    "ev.shape\n",
    "ev.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ev=[]\n",
    "\n",
    "for pid in df['id']:\n",
    "    i_b = int(df.iloc[pid]['i_b'])\n",
    "    i = int(df.iloc[pid]['i'])\n",
    "\n",
    "    ev = out['event_type_list'][i_b][i]\n",
    "    t = out['event_time_list'][i_b][i]\n",
    "    P = t.int().max().item() + 1\n",
    "    m = (ev.sum(1)>0).sum() # False are masked\n",
    "\n",
    "    ev = event2mat(ev,t,P) # [M,L]\n",
    "    list_ev.append(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 49856)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.concatenate(list_ev,axis=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 977)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(977,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([63.55, 11.42,  4.  ,  1.96,  1.7 ,  1.59,  1.26,  1.14,  0.86,\n",
       "        0.73])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 1., 1., 0., 0., 1., 1.],\n",
       "       [0., 0., 1., 0., 1., 1., 0., 0., 1., 1.],\n",
       "       [0., 0., 1., 0., 1., 1., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_cols, unique_counts = np.unique(x, axis=1, return_counts=True)\n",
    "unique_cols.shape\n",
    "unique_counts.shape\n",
    "\n",
    "idx = np.argsort(-unique_counts)\n",
    "unique_counts = unique_counts[idx]\n",
    "unique_cols = unique_cols[:,idx]\n",
    "\n",
    "\n",
    "unique_counts = np.round(unique_counts/unique_counts.sum()*100,2)\n",
    "\n",
    "unique_counts[:10]\n",
    "unique_cols[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BUN',\n",
       " 'Creatinine',\n",
       " 'Glucose',\n",
       " 'HCO3',\n",
       " 'Na',\n",
       " 'K',\n",
       " 'Mg',\n",
       " 'HCT',\n",
       " 'Platelets',\n",
       " 'WBC',\n",
       " 'FiO2',\n",
       " 'PaCO2',\n",
       " 'PaO2',\n",
       " 'pH',\n",
       " 'SaO2',\n",
       " 'ALP',\n",
       " 'ALT',\n",
       " 'AST',\n",
       " 'Albumin',\n",
       " 'Bilirubin',\n",
       " 'Lactate',\n",
       " 'Cholesterol',\n",
       " 'TroponinI',\n",
       " 'TroponinT']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = list(opt1.dict_map_events.keys())\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 41)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . BUN Creatinine Glucose HCO3 Na K Mg HCT Platelets WBC . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . Nothing . BUN Creatinine Glucose HCO3 Na K Mg .'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ev[0].shape\n",
    "list_text = []\n",
    "for ev in list_ev[:]:\n",
    "    text = [] #[\"<CLS>\"]\n",
    "    for e in ev.transpose():\n",
    "        # e\n",
    "        measured = np.where(e==1.0)[0]\n",
    "        if np.any(measured):\n",
    "            temp=[d[i] for i in measured] + [\".\"]\n",
    "            text.extend( temp )\n",
    "        else:\n",
    "            text.extend([\"Nothing\",\".\"])\n",
    "    \n",
    "\n",
    "    list_text.append(\" \".join(text))\n",
    "    # ev.shape\n",
    "    # text = '<CLS> '\n",
    "list_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True)\n",
    "print(inputs)\n",
    "\n",
    "# raw_inputs =list_text\n",
    "# inputs = tokenizer(raw_inputs, padding=True, truncation=True)\n",
    "# len(inputs.input_ids)\n",
    "# len(inputs.input_ids[0])\n",
    "# inputs.input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer(\"Using a Transformer network is simple\")\n",
    "\n",
    "tokenizer.tokenize(\"Using a Transformer network is simple\")\n",
    "\n",
    "tokens = tokenizer.tokenize(list_text[0])\n",
    "tokens\n",
    "\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids\n",
    "\n",
    "inputs = tokenizer(list_text[0])\n",
    "\n",
    "tokenizer.decode(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(list_text)\n",
    "inputs['input_ids'][0]\n",
    "\n",
    "tokenizer.decode(inputs['input_ids'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_tokens = tokenizer.tokenize(sequences[0])\n",
    " \n",
    "ex_ids = tokenizer.convert_tokens_to_ids(ex_tokens)\n",
    "ex_ids\n",
    "tokenizer.decode(ex_ids)\n",
    "\n",
    "tokens['input_ids'][0]\n",
    "tokenizer.decode(tokens['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(list_text)\n",
    "# inputs['input_ids'][0]\n",
    "\n",
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bun',\n",
       " 'creatinine',\n",
       " 'glucose',\n",
       " 'hco3',\n",
       " 'na',\n",
       " 'k',\n",
       " 'mg',\n",
       " 'hct',\n",
       " 'platelets',\n",
       " 'wbc',\n",
       " 'fio2',\n",
       " 'paco2',\n",
       " 'pao2',\n",
       " 'ph',\n",
       " 'sao2',\n",
       " 'alp',\n",
       " 'alt',\n",
       " 'ast',\n",
       " 'albumin',\n",
       " 'bilirubin',\n",
       " 'lactate',\n",
       " 'cholesterol',\n",
       " 'troponini',\n",
       " 'troponint',\n",
       " 'nothing',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df1\n",
    "out=out1\n",
    "opt=opt1\n",
    "\n",
    "vocab_list = list(opt.dict_map_events.keys())\n",
    "vocab_list = [x.lower() for x in vocab_list] + ['nothing','.']\n",
    "\n",
    "vocab_list\n",
    "\n",
    "# Open file in write mode\n",
    "with open('vocab_list.txt', 'w') as f:\n",
    "    # Convert list to a string and write to file\n",
    "    f.write('\\n'.join(map(str, vocab_list)))\n",
    "\n",
    "# Close file\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_df(out,opt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders=['trainloader','validloader','testloader']\n",
    "\n",
    "\n",
    "\n",
    "for loader in loaders:\n",
    "    _, out = Main.valid_epoch_tsne(model1, getattr(opt1,loader), opt1.pred_loss_func, opt1)\n",
    "    # X_tsne, X_tsne_split = compute_tsne(out1['r_enc_list'], model1)\n",
    "    len(out['event_type_list'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)\n",
    "\n",
    "len(out['event_type_list'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['non_pad_mask_list', 'event_type_list', 'event_time_list', 'next_event_type_list', 'list_intens_at_samples', 'list_taus', 'list_true_intens_at_evs', 'list_log_sum', 'list_integral_', 'state_mod_list', 'state_time_list', 'state_value_list', 'r_enc_list', 'list_TE_att', 'list_DAM_att', 'y_pred', 'y_true', 'y_score', 'y_true_list', 'masks', 'y_state_pred', 'y_state_true', 'y_state_score'])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['HR_N',\n",
       " 'NIDiasABP_N',\n",
       " 'NIMAP_H',\n",
       " 'NISysABP_H',\n",
       " 'RespRate_N',\n",
       " 'Temp_N',\n",
       " '.',\n",
       " 'NIDiasABP_N',\n",
       " 'NIMAP_H',\n",
       " 'NISysABP_H',\n",
       " 'RespRate_N',\n",
       " 'GCS_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'NIDiasABP_N',\n",
       " 'NIMAP_H',\n",
       " 'NISysABP_H',\n",
       " 'RespRate_N',\n",
       " 'Temp_N',\n",
       " 'GCS_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_L',\n",
       " 'NIDiasABP_N',\n",
       " 'NIMAP_N',\n",
       " 'NISysABP_N',\n",
       " 'RespRate_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'NIDiasABP_N',\n",
       " 'NIMAP_N',\n",
       " 'NISysABP_N',\n",
       " 'RespRate_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'NIDiasABP_N',\n",
       " 'NIMAP_N',\n",
       " 'NISysABP_H',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'NIDiasABP_N',\n",
       " 'NIMAP_H',\n",
       " 'NISysABP_H',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " 'BUN_N',\n",
       " 'Creatinine_N',\n",
       " 'Glucose_N',\n",
       " 'HCO3_N',\n",
       " 'Na_N',\n",
       " 'K_N',\n",
       " 'HCT_N',\n",
       " 'Platelets_N',\n",
       " 'WBC_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " '.',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'Temp_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_H',\n",
       " 'GCS_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'Urine_N',\n",
       " 'BUN_N',\n",
       " 'Creatinine_N',\n",
       " 'Glucose_N',\n",
       " 'K_N',\n",
       " 'TroponinI_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'Temp_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'GCS_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_H',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_H',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'Temp_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'GCS_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'BUN_N',\n",
       " 'Creatinine_N',\n",
       " 'Glucose_N',\n",
       " 'HCO3_N',\n",
       " 'Na_N',\n",
       " 'K_N',\n",
       " 'HCT_N',\n",
       " 'Platelets_N',\n",
       " 'WBC_N',\n",
       " 'TroponinI_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_H',\n",
       " 'SysABP_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'Temp_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'GCS_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'Temp_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_N',\n",
       " 'GCS_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'Temp_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_H',\n",
       " 'GCS_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'Temp_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " 'GCS_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'Temp_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'GCS_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'Temp_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'GCS_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'BUN_N',\n",
       " 'Creatinine_N',\n",
       " 'Glucose_N',\n",
       " 'HCO3_N',\n",
       " 'Na_N',\n",
       " 'K_N',\n",
       " 'Mg_N',\n",
       " 'HCT_N',\n",
       " 'Platelets_N',\n",
       " 'WBC_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'Temp_N',\n",
       " 'DiasABP_H',\n",
       " 'MAP_H',\n",
       " 'SysABP_H',\n",
       " 'GCS_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'MAP_N',\n",
       " 'SysABP_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'DiasABP_N',\n",
       " 'SysABP_N',\n",
       " 'Urine_N',\n",
       " '.',\n",
       " 'HR_N',\n",
       " 'RespRate_N',\n",
       " 'Temp_N',\n",
       " 'Urine_N']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()\n",
    "dict_temp = {v:k for k,v in opt.dict_map_states.items()}\n",
    "\n",
    "\n",
    "sv = out['state_value_list'][0][0]\n",
    "sm = out['state_mod_list'][0][0]\n",
    "st = out['state_time_list'][0][0]\n",
    "\n",
    "m = sm!=0\n",
    "\n",
    "q_H = sv[m]>2\n",
    "q_L = sv[m]<-2\n",
    "q = (q_H.int()-q_L.int()).tolist()\n",
    "\n",
    "dict_range = {-1:\"_L\", 0:\"_N\", 1:\"_H\"}\n",
    "q_range = [dict_range[x] for x in q]\n",
    "# q_range\n",
    "\n",
    "mods = (sm[m]-1).tolist()\n",
    "\n",
    "mods_final = [ dict_temp[int(mod)]+r for mod,r in zip(mods,q_range)]\n",
    "\n",
    "dot_indices = torch.nonzero( st[m].diff() ).flatten().tolist()\n",
    "elements_to_insert = [\".\" for i in dot_indices]\n",
    "\n",
    "k=1\n",
    "for i, e in zip(dot_indices, elements_to_insert):\n",
    "    \n",
    "    mods_final.insert(i+k, e)\n",
    "    k+=1\n",
    "\n",
    "mods_final\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'term' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m P \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mint()\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mitem() \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     28\u001b[0m m \u001b[39m=\u001b[39m (ev\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39msum() \u001b[39m# False are masked\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m term\n\u001b[1;32m     30\u001b[0m ev \u001b[39m=\u001b[39m event2mat(ev,t,P) \u001b[39m# [M,L]\u001b[39;00m\n\u001b[1;32m     31\u001b[0m myData\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mmat\u001b[39m\u001b[39m'\u001b[39m:ev})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'term' is not defined"
     ]
    }
   ],
   "source": [
    "loaders=['trainloader','validloader','testloader']\n",
    "\n",
    "\n",
    "\n",
    "for loader in loaders:\n",
    "    \n",
    "\n",
    "    _, out = Main.valid_epoch_tsne(model1, getattr(opt1,loader), opt1.pred_loss_func, opt1)\n",
    "    # X_tsne, X_tsne_split = compute_tsne(out1['r_enc_list'], model1)\n",
    "\n",
    "\n",
    "    # df = build_df(out,opt1)\n",
    "    # fig_tsne = plot_tsne(df,title=run1.name)\n",
    "\n",
    "\n",
    "\n",
    "    # a list of dictionaries {'mat'}\n",
    "    myData = []\n",
    "    N_patients = len(out['event_type_list']) * opt.batch_size\n",
    "    for pid in range(N_patients):\n",
    "        # i_b = int(df.iloc[pid]['i_b'])\n",
    "        # i = int(df.iloc[pid]['i'])\n",
    "        i_b, i = divmod(pid, opt.batch_size)\n",
    "\n",
    "        ev = out['event_type_list'][i_b][i]\n",
    "        t = out['event_time_list'][i_b][i]\n",
    "        P = t.int().max().item() + 1\n",
    "        m = (ev.sum(1)>0).sum() # False are masked\n",
    "        term\n",
    "        ev = event2mat(ev,t,P) # [M,L]\n",
    "        myData.append({'mat':ev})\n",
    "\n",
    "    len(myData)\n",
    "    myData[0]['mat'].shape\n",
    "\n",
    "\n",
    "\n",
    "    # add 'text' field to dict\n",
    "    # list_text = []\n",
    "    for sample in myData[:]:\n",
    "        text = [] #[\"<CLS>\"]\n",
    "        for e in sample['mat'].transpose(): # mat[L,K] e[K]\n",
    "            # e\n",
    "            measured = np.where(e==1.0)[0]\n",
    "            if np.any(measured):\n",
    "                temp=[vocab_list[i] for i in measured] #+ [\".\"]\n",
    "                temp[-1] += '.'\n",
    "                text.extend( temp )\n",
    "            else:\n",
    "                text.extend([\"nothing.\"])\n",
    "        \n",
    "        sample['text'] = \" \".join(text)\n",
    "        # list_text.append(\" \".join(text))\n",
    "        # ev.shape\n",
    "        # text = '<CLS> '\n",
    "\n",
    "    myData[0].keys()\n",
    "    myData[0]['text']\n",
    "\n",
    "\n",
    "\n",
    "    # saving\n",
    "\n",
    "    myData = [{'mat':sample['mat'].tolist(),'text':sample['text']} for sample in myData]\n",
    "\n",
    "    raw_data = \"\".join(  [x['text'] for x in myData]  )\n",
    "\n",
    "    import json\n",
    "\n",
    "    with open(f\"ehr_bert_data/{loader}.json\", \"w\") as fp:\n",
    "        json.dump(myData,fp) \n",
    "\n",
    "    # with open(f\"ehr_bert_data/{loader}.json\", \"w\") as fp:\n",
    "    #     json.dump(myData,fp) \n",
    "\n",
    "    with open(f\"ehr_bert_data/{loader}_raw.txt\", \"w\") as file:\n",
    "        file.write(raw_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['non_pad_mask_list', 'event_type_list', 'event_time_list', 'next_event_type_list', 'list_intens_at_samples', 'list_taus', 'list_true_intens_at_evs', 'list_log_sum', 'list_integral_', 'state_mod_list', 'state_time_list', 'state_value_list', 'r_enc_list', 'list_TE_att', 'list_DAM_att', 'y_pred', 'y_true', 'y_score', 'y_true_list', 'masks', 'y_state_pred', 'y_state_true', 'y_state_score'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31167305"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3709484"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3711729"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()\n",
    "dict_temp = {v:k for k,v in opt.dict_map_states.items()}\n",
    "dict_range = {-1:\"_L\", 0:\"_N\", 1:\"_H\"}\n",
    "\n",
    "loaders=['trainloader','validloader','testloader']\n",
    "\n",
    "\n",
    "\n",
    "for loader in loaders:\n",
    "    \n",
    "\n",
    "    _, out = Main.valid_epoch_tsne(model1, getattr(opt1,loader), opt1.pred_loss_func, opt1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # a list of dictionaries {'mat'}\n",
    "    myData = []\n",
    "    N_patients = len(out['event_type_list']) * opt.batch_size\n",
    "    for pid in range(N_patients):\n",
    "        # i_b = int(df.iloc[pid]['i_b'])\n",
    "        # i = int(df.iloc[pid]['i'])\n",
    "        i_b, i = divmod(pid, opt.batch_size)\n",
    "\n",
    "        sv = out['state_value_list'][i_b][i]\n",
    "        sm = out['state_mod_list'][i_b][i]\n",
    "        st = out['state_time_list'][i_b][i]\n",
    "        m = sm!=0\n",
    "\n",
    "        q_H = sv[m]>2\n",
    "        q_L = sv[m]<-2\n",
    "        q = (q_H.int()-q_L.int()).tolist()\n",
    "\n",
    "        q_range = [dict_range[x] for x in q]\n",
    "        # q_range\n",
    "\n",
    "        mods = (sm[m]-1).tolist()\n",
    "\n",
    "        mods_final = [ dict_temp[int(mod)]+r for mod,r in zip(mods,q_range)]\n",
    "\n",
    "        dot_indices = torch.nonzero( st[m].diff() ).flatten().tolist()\n",
    "        elements_to_insert = [\".\" for i in dot_indices]\n",
    "\n",
    "        k=1\n",
    "        for i, e in zip(dot_indices, elements_to_insert):\n",
    "            \n",
    "            mods_final.insert(i+k, e)\n",
    "            k+=1\n",
    "\n",
    "        sample_string = \" \".join(mods_final).replace(\" .\",\".\")+\". DISCHARGE.\"\n",
    "        myData.append({'text':sample_string})\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # saving\n",
    "\n",
    "    myData = [{'text':sample['text']} for sample in myData]\n",
    "\n",
    "    raw_data = \"\".join(  [x['text'] for x in myData]  )\n",
    "\n",
    "    import json\n",
    "\n",
    "    with open(f\"ehr_states_llm_data/{loader}.json\", \"w\") as file:\n",
    "        json.dump(myData,file) \n",
    "\n",
    "    # with open(f\"ehr_states_llm_data/{loader}.json\", \"w\") as fp:\n",
    "    #     json.dump(myData,fp) \n",
    "\n",
    "    with open(f\"ehr_states_llm_data/{loader}_raw.txt\", \"w\") as file:\n",
    "        file.write(raw_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HR_N NIDiasABP_N NIMAP_N NISysABP_N RespRate_N Temp_N Urine_N. Lactate_N. HR_N NIDiasABP_N NIMAP_N NISysABP_N RespRate_N GCS_N. HR_N NIDiasABP_L NIMAP_L NISysABP_L RespRate_N. HR_N NIDiasABP_N NIMAP_N NISysABP_N RespRate_N. HR_N NIDiasABP_N NIMAP_N NISysABP_N RespRate_N. RespRate_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N. HR_N NIDiasABP_N NIMAP_N NISysABP_N RespRate_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. Lactate_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N GCS_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N GCS_N Urine_N BUN_N Creatinine_N Glucose_N HCO3_N Na_N K_N Mg_N HCT_N Platelets_N WBC_N. Lactate_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N GCS_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. Lactate_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N GCS_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. Lactate_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N GCS_N Urine_N. BUN_N Creatinine_N Glucose_N HCO3_N Na_N K_N HCT_N Platelets_N WBC_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. Lactate_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N Urine_N. HCT_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N GCS_N Urine_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N GCS_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. BUN_N Creatinine_N Glucose_N HCO3_N Na_N K_N Mg_N HCT_N Platelets_N WBC_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N GCS_N Urine_N. Lactate_N. BUN_N Creatinine_N Glucose_N HCO3_N Na_N K_N Mg_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N GCS_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N GCS_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N Urine_N. HR_N RespRate_N Temp_N DiasABP_N MAP_N SysABP_N. HR_N RespRate_N DiasABP_N MAP_N SysABP_N GCS_N Urine_N BUN_N Creatinine_N Glucose_N HCO3_N Na_N K_N Mg_N Albumin_N. DISCHARGE.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(mods_final).replace(\" .\",\".\")+\". DISCHARGE.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ridi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['non_pad_mask_list', 'event_type_list', 'event_time_list', 'next_event_type_list', 'list_intens_at_samples', 'list_taus', 'list_true_intens_at_evs', 'list_log_sum', 'list_integral_', 'state_mod_list', 'state_time_list', 'state_value_list', 'r_enc_list', 'list_TE_att', 'list_DAM_att', 'y_pred', 'y_true', 'y_score', 'y_true_list', 'masks', 'y_state_pred', 'y_state_true', 'y_state_score'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31105821"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3709484"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3711729"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()\n",
    "dict_temp = {v:k for k,v in opt.dict_map_states.items()}\n",
    "dict_range = {-1:\"_L\", 0:\"_N\", 1:\"_H\"}\n",
    "\n",
    "loaders=['trainloader','validloader','testloader']\n",
    "\n",
    "\n",
    "\n",
    "for loader in loaders:\n",
    "    \n",
    "\n",
    "    _, out = Main.valid_epoch_tsne(model1, getattr(opt1,loader), opt1.pred_loss_func, opt1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # a list of dictionaries {'mat'}\n",
    "    myData = []\n",
    "    N_patients = len(out['event_type_list']) * opt.batch_size\n",
    "    for pid in range(N_patients):\n",
    "        # i_b = int(df.iloc[pid]['i_b'])\n",
    "        # i = int(df.iloc[pid]['i'])\n",
    "        i_b, i = divmod(pid, opt.batch_size)\n",
    "\n",
    "        sv = out['state_value_list'][i_b][i]\n",
    "        sm = out['state_mod_list'][i_b][i]\n",
    "        st = out['state_time_list'][i_b][i]\n",
    "        m = sm!=0\n",
    "\n",
    "        q_H = sv[m]>2\n",
    "        q_L = sv[m]<-2\n",
    "        q = (q_H.int()-q_L.int()).tolist()\n",
    "\n",
    "        q_range = [dict_range[x] for x in q]\n",
    "        # q_range\n",
    "\n",
    "        mods = (sm[m]-1).tolist()\n",
    "\n",
    "        mods_final = [ dict_temp[int(mod)]+r for mod,r in zip(mods,q_range)]\n",
    "\n",
    "        dot_indices = torch.nonzero( st[m].diff() ).flatten().tolist()\n",
    "        elements_to_insert = [\".\" for i in dot_indices]\n",
    "\n",
    "        k=1\n",
    "        for i, e in zip(dot_indices, elements_to_insert):\n",
    "            \n",
    "            mods_final.insert(i+k, e)\n",
    "            k+=1\n",
    "\n",
    "        sample_string = \" \".join(mods_final).replace(\" .\",\".\")+\". DISCHARGE.\"\n",
    "\n",
    "        \n",
    "        # added part\n",
    "        temp = sample_string.split(\". \")\n",
    "        # temp[0]\n",
    "        temp = [\"_\".join(x.split(\" \")) for x in temp]\n",
    "        # temp[0]\n",
    "\n",
    "        sample_string = \". \".join(temp)\n",
    "\n",
    "\n",
    "        myData.append({'text':sample_string})\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # saving\n",
    "\n",
    "    myData = [{'text':sample['text']} for sample in myData]\n",
    "\n",
    "    raw_data = \"\".join(  [x['text'] for x in myData]  )\n",
    "\n",
    "    import json\n",
    "\n",
    "    with open(f\"ehr_states_llm_data/{loader}_ridi.json\", \"w\") as file:\n",
    "        json.dump(myData,file) \n",
    "\n",
    "    # with open(f\"ehr_states_llm_data/{loader}_ridi.json\", \"w\") as fp:\n",
    "    #     json.dump(myData,fp) \n",
    "\n",
    "    with open(f\"ehr_states_llm_data/{loader}_ridi_raw.txt\", \"w\") as file:\n",
    "        file.write(raw_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HR_H NIDiasABP_N NIMAP_N NISysABP_N Temp_N GCS_N Urine_N'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_Temp_N_GCS_N_Urine_N'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_Temp_N_GCS_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N. BUN_N_Creatinine_N_Glucose_N_HCO3_N_Na_N_K_N_Mg_N_HCT_H_Platelets_N_WBC_N_Albumin_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_GCS_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Temp_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Temp_N_Urine_N. HR_N_NIDiasABP_N_NIMAP_N_NISysABP_N. HR_N. HR_N_NIDiasABP_N_NIMAP_N_NISysABP_N. HR_N_NIDiasABP_N_NIMAP_N_NISysABP_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N. Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Temp_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Temp_N_GCS_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Temp_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Temp_N_GCS_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Temp_N_Urine_N. BUN_N_Creatinine_N_Glucose_N_HCO3_N_Na_N_K_N_Mg_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_DiasABP_N_MAP_N_SysABP_N. HR_H_RespRate_N_Temp_N_DiasABP_N_MAP_N_SysABP_N_GCS_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_DiasABP_N_MAP_N_SysABP_N. Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_DiasABP_N_MAP_N_SysABP_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_DiasABP_N_MAP_N_SysABP_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_RespRate_N_Temp_N_DiasABP_N_MAP_N_SysABP_N_Urine_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_L_DiasABP_N_MAP_N_SysABP_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_DiasABP_N_MAP_N_SysABP_N. HR_H_NIDiasABP_N_NIMAP_N_NISysABP_N_Temp_N_DiasABP_N_MAP_N_SysABP_N_Urine_N. HR_H_DiasABP_N_MAP_N_SysABP_N_Urine_N. HR_N_DiasABP_N_MAP_N_SysABP_N. HR_H_RespRate_N_DiasABP_N_MAP_N_SysABP_N. HR_H_DiasABP_N_MAP_N_SysABP_N. HR_H_Temp_N_DiasABP_N_MAP_N_SysABP_N_Urine_N. HCT_N_Platelets_H_WBC_N. HR_H_DiasABP_N_MAP_N_SysABP_N. HR_H_DiasABP_N_MAP_N_SysABP_N. HR_H_DiasABP_N_MAP_N_SysABP_N. HR_H_DiasABP_N_MAP_N_SysABP_N. HR_H_DiasABP_N_MAP_N_SysABP_N_Urine_N. HR_N_Temp_N_DiasABP_N_MAP_N_SysABP_N_GCS_N. DISCHARGE.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = sample_string.split(\". \")\n",
    "temp[0]\n",
    "temp = [\"_\".join(x.split(\" \")) for x in temp]\n",
    "temp[0]\n",
    "\n",
    "\". \".join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/hokarami/.cache/huggingface/datasets/json/default-e507e5a890310311/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 3/3 [00:00<00:00, 989.38it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 83.61it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/hokarami/.cache/huggingface/datasets/json/default-e507e5a890310311/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 569.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "my_dataset = load_dataset(\"json\",\n",
    "    data_files={\n",
    "            \"train\": \"ehr_bert_data/trainloader.json\",\n",
    "            \"validation\": \"ehr_bert_data/validloader.json\",\n",
    "            \"test\": \"ehr_bert_data/testloader.json\",\n",
    "        })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TKZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\DATA\\Tasks\\tedam2\\vis_attn.ipynb Cell 167\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/DATA/Tasks/tedam2/vis_attn.ipynb#Y325sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtokenizers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainers\u001b[39;00m \u001b[39mimport\u001b[39;00m WordPieceTrainer, WordLevelTrainer\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/DATA/Tasks/tedam2/vis_attn.ipynb#Y325sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# trainer = WordPieceTrainer(vocab_size=len(vocab_list), special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/DATA/Tasks/tedam2/vis_attn.ipynb#Y325sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m trainer \u001b[39m=\u001b[39m WordLevelTrainer(vocab_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(vocab_list), special_tokens\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m[UNK]\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m[CLS]\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m[SEP]\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m[MASK]\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/DATA/Tasks/tedam2/vis_attn.ipynb#Y325sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# files = [f\"data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/DATA/Tasks/tedam2/vis_attn.ipynb#Y325sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m files \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mmyData_raw.txt\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_list' is not defined"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/tokenizers/pipeline\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece, WordLevel\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "bert_tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")\n",
    "\n",
    "from tokenizers.trainers import WordPieceTrainer, WordLevelTrainer\n",
    "# trainer = WordPieceTrainer(vocab_size=len(vocab_list), special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "trainer = WordLevelTrainer(vocab_size=len(vocab_list), special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "\n",
    "# files = [f\"data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "\n",
    "files = [\"myData_raw.txt\"]\n",
    "bert_tokenizer.train(files, trainer)\n",
    "bert_tokenizer.save(\"bert-ehr-tkz.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bert_tokenizer.get_vocab())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate line execution\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# my_dataset = load_dataset(\"json\", data_files=\"myData.json\", split=\"train\")\n",
    "\n",
    "\n",
    "# my_dataset = load_dataset(\"json\",\n",
    "#     data_files={\n",
    "#             \"train\": \"ehr_bert_data/trainloader.json\",\n",
    "#             \"validation\": \"ehr_bert_data/validloader.json\",\n",
    "#             \"test\": \"ehr_bert_data/testloader.json\",\n",
    "#         })\n",
    "\n",
    "\n",
    "data_files={\n",
    "            \"train\": \"ehr_bert_data/trainloader.json\",\n",
    "            \"validation\": \"ehr_bert_data/validloader.json\",\n",
    "            \"test\": \"ehr_bert_data/testloader.json\",\n",
    "        }\n",
    "my_dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "my_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = my_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "# sample = my_dataset.shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> text: {row['text']}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_list.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    vocab_list = content.split()\n",
    "    print(vocab_list)\n",
    "\n",
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer \n",
    "\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model_checkpoint = \"prajjwal1/bert-tiny\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "len(tokenizer.get_vocab())\n",
    "\n",
    "tokenizer.add_tokens(vocab_list)\n",
    "# tokenizer = PreTrainedTokenizerFast(\n",
    "#     # tokenizer_object=bert_tokenizer,\n",
    "#     tokenizer_file=\"bert-ehr-tkz.json\", # You can load from the tokenizer file, alternatively\n",
    "#     unk_token=\"[UNK]\",\n",
    "#     pad_token=\"[PAD]\",\n",
    "#     cls_token=\"[CLS]\",\n",
    "#     sep_token=\"[SEP]\",\n",
    "#     mask_token=\"[MASK]\",\n",
    "# )\n",
    "\n",
    "len(tokenizer.get_vocab())\n",
    "# tokenizer.get_vocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(my_dataset['train'][0]['text'])\n",
    "# wrapped_tokenizer.tokenize(my_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel, AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initializing a BERT bert-base-uncased style configuration\n",
    "configuration = BertConfig(vocab_size = len(tokenizer.get_vocab()), hidden_size=32, num_hidden_layers=4, num_attention_heads=4, intermediate_size=128,  )\n",
    "\n",
    "\n",
    "# configuration = BertConfig()\n",
    "\n",
    "\n",
    "\n",
    "# Initializing a model (with random weights) from the bert-base-uncased style configuration\n",
    "model = AutoModelForMaskedLM.from_config(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model_checkpoint = \"prajjwal1/bert-tiny\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = my_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=['text', \"mat\"]\n",
    ")\n",
    "tokenized_datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128\n",
    "\n",
    "# Slicing produces a list of lists for each feature\n",
    "# tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "tokenized_samples = tokenized_datasets['train'][:3]\n",
    "\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Example {idx} length: {len(sample)}'\")\n",
    "\n",
    "tokenized_samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")\n",
    "\n",
    "\n",
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets['train'][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "\n",
    "for sample in samples:\n",
    "    \n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = 0.8\n",
    "# test_size = 0.2\n",
    "# # test_size = int(0.1 * train_size)\n",
    "\n",
    "# downsampled_dataset = lm_datasets.train_test_split(\n",
    "#     train_size=train_size, test_size=test_size, seed=42\n",
    "# )\n",
    "# downsampled_dataset\n",
    "\n",
    "downsampled_dataset=lm_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = 'EHRpattern'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-3,\n",
    "    weight_decay=0.1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "\n",
    "    dataloader_num_workers=0,\n",
    "    # debug='underflow_overflow',\n",
    "    \n",
    "    # use_multiprocessing=False,\n",
    "    # use_multiprocessing_for_evaluation=False,\n",
    ")\n",
    "\n",
    "training_args = training_args.set_optimizer(name=\"adamw_torch\", beta1=0.8)\n",
    "\n",
    "# torch.optim.AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "device\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "# curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "# sudo apt-get install git-lfs\n",
    "\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./saved_models/ehr-distilbert-base-uncased\"\n",
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", model=model, tokenizer=tokenizer,device=model.device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = my_dataset['train'][0]['text']\n",
    "text\n",
    "text = 'nothing. nothing. bun creatinine glucose hco3 na k mg [MASK] platelets wbc.'\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\\t\\t\\t{pred['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = my_dataset['train'][0]['text']\n",
    "text\n",
    "seq = text.replace(\".\", \" .\").split(\" \")\n",
    "\n",
    "for i in range(1,len(seq)):\n",
    "    if seq[i]=='.':\n",
    "        continue\n",
    "    temp = seq[:i+1].copy()\n",
    "    temp[i]= '[MASK]'\n",
    "    temp = ' '.join(temp).replace(\" .\", \".\")\n",
    "\n",
    "    # temp\n",
    "    # mask_filler(temp)\n",
    "    print(mask_filler(temp)[1]['token_str'], ' ### ', seq[i], '###', mask_filler(temp)[0]['score']) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/course/chapter7/3?fw=pt\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
    "print(f\"'>>> BERT number of parameters: 110M'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "my_dataset = load_dataset(\"json\", data_files=\"myData.json\", split=\"train\")\n",
    "# my_dataset = load_dataset(\"imdb\")['train'].rename_column('label','mat')\n",
    "# my_dataset = my_dataset.remove_columns(['mat'])\n",
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = my_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "sample = my_dataset.shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> text: {row['text']}'\")\n",
    "    # print(f\"'>>> Label: {row['label']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.model_max_length\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = my_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=['text', 'mat']\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128\n",
    "\n",
    "# Slicing produces a list of lists for each feature\n",
    "# tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "tokenized_samples = tokenized_datasets[:3]\n",
    "\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Example {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])\n",
    "tokenizer.decode(lm_datasets[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "samples = [lm_datasets[i] for i in range(2)]\n",
    "\n",
    "for sample in samples:\n",
    "    \n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "samples = [lm_datasets[i] for i in range(2)]\n",
    "\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "test_size = 0.2\n",
    "# test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets.train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "\n",
    "    dataloader_num_workers=0,\n",
    "    # debug='underflow_overflow',\n",
    "\n",
    "    # report_to=None\n",
    "    \n",
    "    # use_multiprocessing=False,\n",
    "    # use_multiprocessing_for_evaluation=False,\n",
    ")\n",
    "\n",
    "training_args = training_args.set_optimizer(name=\"adamw_torch\", beta1=0.8)\n",
    "\n",
    "# torch.optim.AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "device\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "# curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "# sudo apt-get install git-lfs\n",
    "\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    # optimizers=(torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9),\n",
    "    #                 None\n",
    "    #                 )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo conda create --name hf python=3.9\n",
    "\n",
    "sudo conda install -c anaconda ipykernel  --name hf\n",
    "\n",
    "sudo conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia --name hf\n",
    "\n",
    "sudo conda install -c conda-forge transformers datasets tokenizers --name hf\n",
    "\n",
    "\n",
    "sudo conda install -c conda-forge wandb --name hf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca5b395ee1d8a1cd2783c3fccc5aaaf2f1d95e614c8b133e284cded792af89cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
