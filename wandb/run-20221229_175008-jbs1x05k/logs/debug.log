2022-12-29 17:50:08,764 INFO    MainThread:16988 [wandb_setup.py:_flush():68] Configure stats pid to 16988
2022-12-29 17:50:08,764 INFO    MainThread:16988 [wandb_setup.py:_flush():68] Loading settings from C:\Users\hokarami\.config\wandb\settings
2022-12-29 17:50:08,764 INFO    MainThread:16988 [wandb_setup.py:_flush():68] Loading settings from C:\DATA\Tasks\220702\codes\thp_final\wandb\settings
2022-12-29 17:50:08,764 INFO    MainThread:16988 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True', 'api_key': '***REDACTED***'}
2022-12-29 17:50:08,765 INFO    MainThread:16988 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'Main.py', 'program': 'C:\\DATA\\Tasks\\220702\\codes\\thp_final\\Main.py'}
2022-12-29 17:50:08,776 INFO    MainThread:16988 [wandb_init.py:_log_setup():476] Logging user logs to C:\DATA\Tasks\220702\codes\thp_final\wandb\run-20221229_175008-jbs1x05k\logs\debug.log
2022-12-29 17:50:08,776 INFO    MainThread:16988 [wandb_init.py:_log_setup():477] Logging internal logs to C:\DATA\Tasks\220702\codes\thp_final\wandb\run-20221229_175008-jbs1x05k\logs\debug-internal.log
2022-12-29 17:50:08,777 INFO    MainThread:16988 [wandb_init.py:init():516] calling init triggers
2022-12-29 17:50:08,778 INFO    MainThread:16988 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
config: {'data': 'C:/DATA/data/processed/p12_full_seft/', 'data_label': 'multilabel', 'cuda': False, 'wandb': True, 'per': 50, 'balanced_batch': True, 'transfer_learning': '', 'freeze': '', 'log': 'log.txt', 'user_prefix': '[bal]TEDA__marklabel-', 'pos_alpha': 1.0, 'epoch': 30, 'batch_size': 8, 'lr': 0.00245, 'smooth': 0.0, 'event_enc': 1, 'time_enc': 'concat', 'te_d_mark': 8, 'te_d_time': 8, 'te_d_rnn': 256, 'te_d_inner': 16, 'te_d_k': 8, 'te_d_v': 8, 'te_n_head': 4, 'te_n_layers': 4, 'te_dropout': 0.1, 'state': True, 'demo': False, 'num_states': 35, 'w_sample_label': 100.0, 'mod': 'none', 'int_dec': 'sahp', 'w_event': 1, 'next_mark': 1, 'w_class': False, 'w_pos': True, 'mark_detach': 0, 'w_time': 1.0, 'sample_label': 1, 'hparams2write': {'data': 'C:/DATA/data/processed/p12_full_seft/', 'data_label': 'multilabel', 'cuda': False, 'wandb': True, 'per': 50, 'balanced_batch': True, 'transfer_learning': '', 'freeze': '', 'log': 'log.txt', 'user_prefix': '[bal]TEDA__marklabel-', 'pos_alpha': 1.0, 'epoch': 30, 'batch_size': 8, 'lr': 0.00245, 'smooth': 0.0, 'event_enc': 1, 'time_enc': 'concat', 'te_d_mark': 8, 'te_d_time': 8, 'te_d_rnn': 256, 'te_d_inner': 16, 'te_d_k': 8, 'te_d_v': 8, 'te_n_head': 4, 'te_n_layers': 4, 'te_dropout': 0.1, 'state': True, 'demo': False, 'num_states': 1, 'w_sample_label': 100.0, 'mod': 'none', 'int_dec': 'sahp', 'w_event': 1, 'next_mark': 1, 'w_class': False, 'w_pos': True, 'mark_detach': 0, 'w_time': 1.0, 'sample_label': 1}, 'date': '29-12-22--17-49-55', 'run_id': '1036419', 'str_config': '-none-state1-per50', 'run_name': '[bal]TEDA__marklabel-1036419-none-state1-per50', 'run_folder': 'C:/DATA/data/processed/p12_full_seft/[bal]TEDA__marklabel-1036419-none-state1-per50/', 'device': device(type='cpu'), 'trainloader': <torch.utils.data.dataloader.DataLoader object at 0x00000194111394F0>, 'testloader': <torch.utils.data.dataloader.DataLoader object at 0x000001940F43DFA0>, 'num_marks': 25, 'pos_weight': tensor([13.3299, 13.2649, 14.2023, 13.6398,  9.8851, 12.6716, 13.6233, 13.6036,
        13.0283,  0.4717, 14.3766,  5.2823,  7.6584,  7.6795, 23.9278,  7.2796,
        50.0000, 50.0000, 50.0000, 50.0000, 50.0000, 23.5461, 50.0000, 50.0000,
        50.0000], dtype=torch.float64), 'num_demos': 4, 'w': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1.]), 'type_loss': <function type_loss_BCE at 0x0000019406AAB670>, 'pred_loss_func': BCEWithLogitsLoss(), 'TE_config': {'n_marks': 25, 'd_type_emb': 8, 'time_enc': 'concat', 'd_time': 8, 'd_inner': 16, 'n_layers': 4, 'n_head': 4, 'd_k': 8, 'd_v': 8, 'dropout': 0.1}, 'DAM_config': {'output_activation': 'relu', 'output_dims': 4, 'n_phi_layers': 2, 'phi_width': 32, 'phi_dropout': 0.1, 'n_psi_layers': 2, 'psi_width': 32, 'psi_latent_width': 64, 'dot_prod_dim': 32, 'n_heads': 2, 'attn_dropout': 0.1, 'latent_width': 8, 'n_rho_layers': 2, 'rho_width': 32, 'rho_dropout': 0.1, 'max_timescale': 1000, 'n_positional_dims': 8, 'num_mods': 35, 'num_demos': 4, 'online': False}, 'demo_config': {}, 'CIF_config': {}, 'next_type_config': {'n_marks': 25}, 'next_time_config': True, 'label_config': 1}
2022-12-29 17:50:08,779 INFO    MainThread:16988 [wandb_init.py:init():569] starting backend
2022-12-29 17:50:08,779 INFO    MainThread:16988 [wandb_init.py:init():573] setting up manager
2022-12-29 17:50:08,788 INFO    MainThread:16988 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn, using: spawn
2022-12-29 17:50:08,797 INFO    MainThread:16988 [wandb_init.py:init():580] backend started and connected
2022-12-29 17:50:08,804 INFO    MainThread:16988 [wandb_init.py:init():658] updated telemetry
2022-12-29 17:50:08,849 INFO    MainThread:16988 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
2022-12-29 17:50:09,338 INFO    MainThread:16988 [wandb_run.py:_on_init():2006] communicating current version
2022-12-29 17:50:09,406 INFO    MainThread:16988 [wandb_run.py:_on_init():2010] got version response 
2022-12-29 17:50:09,406 INFO    MainThread:16988 [wandb_init.py:init():728] starting run threads in backend
2022-12-29 17:50:10,601 INFO    MainThread:16988 [wandb_run.py:_console_start():1986] atexit reg
2022-12-29 17:50:10,602 INFO    MainThread:16988 [wandb_run.py:_redirect():1844] redirect: SettingsConsole.WRAP_RAW
2022-12-29 17:50:10,602 INFO    MainThread:16988 [wandb_run.py:_redirect():1909] Wrapping output streams.
2022-12-29 17:50:10,602 INFO    MainThread:16988 [wandb_run.py:_redirect():1931] Redirects installed.
2022-12-29 17:50:10,602 INFO    MainThread:16988 [wandb_init.py:init():765] run started, returning control to user process
